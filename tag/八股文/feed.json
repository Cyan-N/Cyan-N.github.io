{
    "version": "https://jsonfeed.org/version/1",
    "title": "慕青の迷途 • All posts by \"八股文\" tag",
    "description": "时雨病重症患者",
    "home_page_url": "https://cecilia.cool",
    "items": [
        {
            "id": "https://cecilia.cool/2024/03/07/%E5%85%AB%E8%82%A1%E6%96%87/%E9%9D%A2%E7%BB%8F/",
            "url": "https://cecilia.cool/2024/03/07/%E5%85%AB%E8%82%A1%E6%96%87/%E9%9D%A2%E7%BB%8F/",
            "title": "面经",
            "date_published": "2024-03-07T02:14:42.000Z",
            "content_html": "<p>本文是我在网上搜集、朋友面试、自己面试的总结，是一个持续更新的系列，所以应该会很长</p>\n<h1 id=\"敖丙-读者面试\"><a class=\"anchor\" href=\"#敖丙-读者面试\">#</a> 敖丙 - 读者面试</h1>\n<p>资料来源：<span class=\"exturl\" data-url=\"aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL0JWMUNFNDExRTdTRy8/dmRfc291cmNlPTVhY2Y1YTdiMjNkMjhlNzYzM2U1YTliMzgxYzU3YzQy\">https://www.bilibili.com/video/BV1CE411E7SG/?vd_source=5acf5a7b23d28e7633e5a9b381c57c42</span></p>\n<p>问了好多问题，因为不是真实面试场景，所以一遇到不会的问题就换技术栈，所以涉及比较广，我选择有代表性的问题、我也不会的问题写上去。</p>\n<blockquote>\n<p>Q1： <code>SpringBean</code>  注入过程</p>\n</blockquote>\n<blockquote>\n<p>Q2：聊聊 Java8 新特性，了解过更高版本的新特性吗？</p>\n</blockquote>\n<blockquote>\n<p>Q3：线上 CPU100% 如何排查？</p>\n</blockquote>\n<blockquote>\n<p>Q4：InnoDB 为什么使用 B + 树而不用二叉树或者 B 树？</p>\n</blockquote>\n<blockquote>\n<p>Q5：MyISAM 和 InnoDB 的区别</p>\n</blockquote>\n<blockquote>\n<p>Q6：一张表最多能建多少个索引？</p>\n</blockquote>\n<blockquote>\n<p>Q7：Redis 如何保证数据高可用？</p>\n</blockquote>\n<p><code>Redis</code>  为了保证数据高可用，引入了持久化机制， 在早期版本，还有 VM，后来版本不推荐了。现在一般都是使用 <code>RDB</code> 、 <code>AOF</code>  或者混合持久化。</p>\n<ul>\n<li>\n<p><code>RDB</code>  通过对内存数据拍摄快照来持久化数据，触发机制是在一定时间内发生一定次数的修改操作。当然也可以使用 <code>save/bgsave</code>  主动拍摄快照，前者会阻塞线程，后者才会 <code>fork</code>  一个子线程进行快照拍摄。因为采用了压缩算法，实际占用空间很小。异步存储为了保证数据一致性，借助了操作系统的 <code>Copy on Write</code>  机制，主线程修改哪个页，就会先将这个页复制出来，在复制页进行修改。等快照拍摄结束，再将复制的页合并到原始内存中。</p>\n</li>\n<li>\n<p><code>AOF</code>  通过存储执行的命令到磁盘中保证数据的持久性，可以配置多种存储方式，比如执行一条命令就存储一条，或者每秒存储一次，或者看系统心情，什么时候有空什么时候就将缓冲区的命令存进去。 <code>AOF</code>  机制执行久了，就会导致文件保存了很多无效的命令，所以需要重写 <code>AOF</code>  文件 —— <code>bgrewriteaof</code> ，过程为：子线程遍历 <code>Redis</code>  内存生成一系列指令，然后将这些指令序列化到临时文件中，过程中的增量命令会追加到临时文件中，最后替换 <code>AOF</code>  文件。这里需要重点说一下，我们将数据写入到文件中时，其实是先写入到内核缓冲区，再到磁盘缓冲区，最后到磁盘，最后一个阶段我们是无法介入的，但是可以调用 <code>fsync()</code>  强制将数据刷新到磁盘缓冲区。 <code>redis</code>  默认是每秒调用一次。（有参数控制何时重写，比如文件大小超过多少，增量达到多少）</p>\n</li>\n<li>\n<p><code>混合持久化</code> ：4.0 版本后还出现了混合持久化，该机制必须打开 <code>AOF</code> ，隔一段时间拍摄快照，生成 <code>rdb</code>  数据，两次快照之间的记录使用 <code>AOF</code>  日志来记录，并追加到 <code>rdb</code>  数据后面。恢复数据时，先回复 <code>rbd</code>  数据，再执行 <code>AOF</code>  日志。这种机制既解决了 <code>rdb</code>  快照摄时突然断电导致整个快照丢失（因为还在临时文件中），也解决了 <code>AOF</code>  文件太大，不断重写的性能消耗。</p>\n</li>\n</ul>\n<p>参考链接：</p>\n<ul>\n<li><span class=\"exturl\" data-url=\"aHR0cHM6Ly9wZGFpLnRlY2gvbWQvZGIvbm9zcWwtcmVkaXMvZGItcmVkaXMteC1yZGItYW9mLmh0bWwjcmRiJUU1JTkyJThDYW9mJUU2JUI3JUI3JUU1JTkwJTg4JUU2JTk2JUI5JUU1JUJDJThGLTQtMCVFNyU4OSU4OCVFNiU5QyVBQw==\">Redis 进阶 - 持久化</span></li>\n<li><span class=\"exturl\" data-url=\"aHR0cHM6Ly9tcC53ZWl4aW4ucXEuY29tL3MvT19xRGNvNi1EYXN1M1JvbVdJS19JZw==\">https://mp.weixin.qq.com/s/O_qDco6-Dasu3RomWIK_Ig</span></li>\n</ul>\n<blockquote>\n<p>Q8：Redis 雪崩、穿透和击穿是什么？</p>\n</blockquote>\n<ul>\n<li><strong>缓存雪崩</strong>：是因为大量的 <code>Key</code>  同时过期，然后又来了海量的请求，导致全部打到后端数据库，造成数据库压力甚至宕机。在主从复制中，如果主节点和从节点时间相差很大，在主节点没有过期的 <code>Key</code>  可能从节点已经过期了，如果主节点宕机了恰好选举出这个从节点作为新的主节点，那么它可能就会面临一大批同时过期的 <code>Key</code> ，可能就会导致缓存雪崩。上述例子提示我们应该保证主从节点时间一致。同时，<strong>应对缓存雪崩可以在过期时间上加一个随机值</strong>。</li>\n<li><strong>缓存穿透</strong>：通俗来说就是大量<strong>不存在数据的请求</strong>直接打到后端数据库中，击穿了缓存。假设黑客攻击，请求的全是一些非法数据，比如 <code>id = -1、age = -1</code>  什么的，后端数据库都不存在，那么缓存更不可能存在，所以请求相当于无视缓存直接请求到后端数据库。应对方法可以<strong>对每个不存在数据的请求在缓存中加上 <code>unknown</code> ，这样下次相同的非法请求打过来缓存就可以拦截。<strong>但是这也有问题，毕竟非法、各不相同的请求有无数个，所以可以在接口层增加校验，不合法的参数直接 <code>return</code> 。 <code>Redis</code>  自带的</strong>布隆过滤器</strong>也能够很好的防止缓存穿透，它的原理就是利用高效的数据结构和算法快速判断这个 <code>key</code>  是否在数据库中。</li>\n<li><strong>缓存击穿</strong>：某个热点数据，很多请求都在访问它，但是如果该数据突然过期了，还是会导致很多请求打到后端数据库，所以可以<strong>设置热点数据不过期或者加上互斥锁</strong>。</li>\n</ul>\n<p>参考链接：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9tcC53ZWl4aW4ucXEuY29tL3Mva256LWotbThiVGc1R25LYzdvZVpMZw==\">https://mp.weixin.qq.com/s/knz-j-m8bTg5GnKc7oeZLg</span></p>\n<blockquote>\n<p>Q9：看你项目是涉及金融场景，是如何保证<strong>幂等</strong>的？</p>\n</blockquote>\n<h1 id=\"朋友-腾讯云智研发\"><a class=\"anchor\" href=\"#朋友-腾讯云智研发\">#</a> 朋友 - 腾讯云智研发</h1>\n<p>该部分面经是我朋友面的腾讯云智研发，他在面完之后只有一个体会：计网、os 这些真的很能考研一个科班是否基础扎实。因为他被问了大量的计网知识，面试过程也是疯狂道歉。希望大家能够巩固基础，下 <code>offer</code>  雨。</p>\n<blockquote>\n<p>Q1：TCP 与 UDP 区别</p>\n</blockquote>\n<p>TCP 是一对一建立连接的可靠传输控制协议，可以保证数据被全部有序的接收，同时还有流量控制、拥塞避免的功能。</p>\n<p>UDP 是一对多、一对一，多对一，多对多的不可靠无连接用户数据报协议，对应用层传过来的报文不合并也不拆分，只是添加 UDP 首部（TCP 会根据 MSS 拆分）。它是尽最大努力发送数据，但是不能保证数据一定会到达接收方，数据在传输过程中丢了也就丢了，不会重传，也不会根据网络拥塞情况做出优化让步。但是因为速度快，可以用于直播这种实时应用。</p>\n<blockquote>\n<p>Q2：GET/POST 区别</p>\n</blockquote>\n<p><code>GET</code>  主要用于传输表单，会在 <code>URL</code>  上携带参数，存在一定风险； <code>POST</code>  主要用于传输实体对象，不会修改 <code>URL</code> 。</p>\n<blockquote>\n<p>Q3：HTTP/HTTPS 区别</p>\n</blockquote>\n<p><code>http</code>  是明文传输，在数据传输的过程中容易被中间人窃取甚至替换，是不安全的。但是 <code>https</code>  是安全的，通过非对称加密传输客户端生成的<strong>随机密钥</strong>，再用该密钥进行对称加密传输数据。</p>\n<p>详细过程参考（我在脑海里走了一遍加密流程）：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9tcC53ZWl4aW4ucXEuY29tL3MvMjFKYVh3ZGZTakl0ajVTZ093aGFwZw==\">https://mp.weixin.qq.com/s/21JaXwdfSjItj5SgOwhapg</span></p>\n<blockquote>\n<p>Q4：SESSION/COOKIE 区别</p>\n</blockquote>\n<blockquote>\n<p>Q5：TOKEN 是啥？</p>\n</blockquote>\n<blockquote>\n<p>Q6：如何实现多个设备同时登陆 ？</p>\n</blockquote>\n<blockquote>\n<p>Q7：如何实现多个设备无法同时登陆？</p>\n</blockquote>\n<blockquote>\n<p>Q8：Redis 的 hash 如何存储？</p>\n</blockquote>\n<blockquote>\n<p>Q9：Redis 的 list</p>\n</blockquote>\n<blockquote>\n<p>Q10：Redis 持久化</p>\n</blockquote>\n<p><code>rdb</code> 、 <code>aof</code> 、混合持久化，在第一篇面试已经写了</p>\n<blockquote>\n<p>Q11：MySQL 常用数据库引擎有哪些？</p>\n</blockquote>\n<blockquote>\n<p>Q12：InnoDB 主键索引实现方式？</p>\n</blockquote>\n<blockquote>\n<p>Q13：B + 树和散列表有什么区别？</p>\n</blockquote>\n<blockquote>\n<p>Q14：什么时候会索引失效？</p>\n</blockquote>\n<blockquote>\n<p>Q15：事务有哪些特性？</p>\n</blockquote>\n<p><code>ACID</code> ：原子性、持久性、隔离性、一致性</p>\n<blockquote>\n<p>Q16：间隙锁</p>\n</blockquote>\n<blockquote>\n<p>Q17：事务有哪些隔离级别？</p>\n</blockquote>\n<blockquote>\n<p>Q18：MVCC</p>\n</blockquote>\n<p>笔试：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9kZXNpZ24tY2lyY3VsYXItcXVldWUv\">设计循环队列</span></p>\n",
            "tags": [
                "八股文"
            ]
        },
        {
            "id": "https://cecilia.cool/2023/03/11/%E5%85%AB%E8%82%A1%E6%96%87/MySQL/",
            "url": "https://cecilia.cool/2023/03/11/%E5%85%AB%E8%82%A1%E6%96%87/MySQL/",
            "title": "MySQL",
            "date_published": "2023-03-11T12:10:31.000Z",
            "content_html": "<p>本文参考网上各种博客以及《MySQL 是怎样运行的》一书，特别声明的是，我并没有看过任何关于 MySQL 的源码，我所见所学，皆是站在前人的基础。</p>\n<blockquote>\n<p>一条 <code>SQL</code>  查询语句在 <code>MySQL</code>  中如何执行的？</p>\n</blockquote>\n<p>算是一道经典面试题了，相当于计网的一条 <code>URL</code>  从输入到界面呈现发生了什么类似。</p>\n<ol>\n<li>先检查语句是否有权限，在 8.0 版本以前，有权限会先检查缓存，但是这并不是一个很好的特性，所以在 8.0 版本后删除了该操作。</li>\n<li>分析器进行词法分析，提取关键字，判断语法。</li>\n<li>执行 <code>sql</code> ：\n<ol>\n<li>预处理阶段：检查查询语句的表、字段是否存在，将 <code>*</code>  扩展为表上所有列。</li>\n<li>优化阶段：优化器生成执行方案</li>\n<li>执行阶段：执行方案交给执行器进行权限校验后执行，返回执行结果。</li>\n</ol>\n</li>\n</ol>\n<blockquote>\n<p><code>InnoDB</code>  和 <code>MyISAM</code>  的比较</p>\n</blockquote>\n<ul>\n<li>事务： <code>InnoDB</code>  是事务型的，可以使用 <code>Commit</code>  和 <code>Rollback</code> 。而 <code>MyISAM</code>  不支持事务。</li>\n<li>并发： <code>InnoDB</code>  支持行级锁，而 <code>MyISAM</code>  支持表级锁。</li>\n<li>外键： <code>InnoDB</code>  支持外键而 <code>MyISAM</code>  不支持</li>\n<li>崩溃恢复： <code>InnoDB</code>  崩溃恢复后发生损坏的概率比 <code>InnoDB</code>  高，而且恢复速度也慢。</li>\n<li><code>InnoDB</code>  更适合处理事务性应用程序，需要支持外键约束和具有高并发和可靠性； <code>MyISAM</code>  更适合处理只读或只写的应用程序，需要较好的性能和较少的系统资源。</li>\n</ul>\n<blockquote>\n<p>一般如何对 <code>SQL</code>  优化？</p>\n</blockquote>\n<p>这种问题把你能想到的都答上去，但是尽量要有条理。</p>\n<ol>\n<li>设计表方面：尽量减少字符串使用，多用数字类型；使用字符串尽量用 <code>varchar</code>  节省空间。当索引列的重复数据占大多数时，建议删掉索引，因为使用这种索引并不会节省太多时间，相反有时回表太多导致性能下降。</li>\n<li><code>SQL</code>  语句方面：少使用 <code>select *</code> ，尽量避免用 <code>or</code>  连接条件。</li>\n<li>索引方面：对要查询的条件的列， <code>order by</code>  的字段建立索引，不要建立过多的索引，尽量使用组合索引。</li>\n</ol>\n<blockquote>\n<p>怎么看执行计划，里面的字段是什么意思？</p>\n</blockquote>\n<p>用 <code>explain</code>  命令，后面跟查询语句： <code>explain select name from student;</code> 。里面的字段，记不清，但是看到了我知道它代表什么意思。</p>\n<ul>\n<li><code>id</code> ：每个 <code>select</code>  语句都会分配一个唯一 <code>id</code> 。</li>\n<li><code>select_type</code> ：对应的查询类型，有 <code>simple</code> 、 <code>union</code> 、 <code>primary</code>  等</li>\n<li><code>table</code> ：表名</li>\n<li><code>partitions</code> ：匹配的分区信息</li>\n<li><code>type</code> ：表示这个查询对表执行查询时的访问方法，如 <code>const</code> 、 <code>ref</code> 、 <code>eq_ref</code>  等。</li>\n<li><code>possible_key</code> ：表示这个查询可能用到的索引</li>\n<li><code>key</code> ：实际用到的索引</li>\n<li><code>key_len</code> ：索引中形成的扫描区间和边界条件的列的长度</li>\n<li><code>ref</code> ：与索引列进行等值匹配的是什么， <code>const</code>  表示常数， <code>func</code>  表示函数</li>\n<li><code>rows</code> ：表示查询的估计行数</li>\n<li><code>filtered</code> ：查询优化器预测扇出记录有多少条记录符合其余条件的百分比。</li>\n<li><code>extra</code> ：记录一些额外信息，能够更准确理解 <code>MySQL</code>  如何执行给定查询语句。</li>\n</ul>\n<blockquote>\n<p>为什么不用 B 树而是用 B + 树？</p>\n</blockquote>\n<p>B + 树是 B 树的一种优化，B + 树非叶子节点存储的不是数据记录，而是目录记录。因为不存储数据，所以可以存储更多的目录记录，对应的叶子节点就可以存储更多的数据记录，使得整棵树层数更小，一般来说，一棵树都不会超过 4 层，所以查找起来非常快。其次在于所有记录在叶子节点，叶子节点的页连成双向链表，用于做范围查询有天然优势，而 B 树就很难做到。</p>\n<p>个人还有个浅显的理解，在增删记录时，B + 树一般修改的只有非叶子节点，以删除一条记录为例，如果删除的记录是该页中键值大小在中间的记录，那么直接删了就行，并不会影响上面的目录记录。但是如果是 B 树，删除的记录在往往会影响下面节点的记录的位置。</p>\n<blockquote>\n<p>什么是最左前缀原则和最左匹配原则？</p>\n</blockquote>\n<ul>\n<li>最左前缀：创建多列索引（组合索引），应该把 <code>where</code>  使用最频繁的列放在索引的最左边。</li>\n<li>最左匹配：创建的组合索引 <code>(a,b,c)</code>  相当于创建了索引 <code>(a)、(a,b)、(a,b,c)</code>  三个索引。</li>\n</ul>\n<blockquote>\n<p>索引下推是什么？</p>\n</blockquote>\n<p><code>MySQL5.6</code>  默认开启的索引下推，在联合索引中，比如是 <code>(a,b)</code>  两个字段，再加上主键。假设有一条 <code>sql</code>  语句的过滤条件涉及到 a、b 两个，那么索引下推开启后，存储引擎先找到所有符合关于 a 条件的数据，再根据索引中已经存在的 b 进行过滤。找到符合条件的数据，最后在回表查询。</p>\n<p>参考链接：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9tcC53ZWl4aW4ucXEuY29tL3MvODdxc3JqLV9oRzU0dXhjT2xGcjM1UQ==\">https://mp.weixin.qq.com/s/87qsrj-_hG54uxcOlFr35Q</span></p>\n<blockquote>\n<p><code>MySQL</code>  调优你一般从哪些方面入手？</p>\n</blockquote>\n<ul>\n<li>查询 <code>explain</code>  执行计划</li>\n<li>索引的建立一定要合适，尽量使索引满足最左前缀原则，查询过程中尽量触发索引覆盖和索引下推。</li>\n<li>在读多写少的业务场景中，建立普通索引使用 <code>change buffer</code>  减少 IO 耗时。唯一索引并不能走 <code>change buffer</code> ，因为更新操作需要判断是否违反唯一性约束。</li>\n<li>如果遇到很长的字段做索引，如何优化：\n<ul>\n<li>可以建立前缀索引，但是这可能会影响索引的选择性，需要根据实际情况来测试和调整</li>\n<li>建立哈希索引，将字段值映射到哈希值，再用哈希值做索引，但是这种不支持范围查询，只能精准查询。</li>\n<li>删除重复部分，考虑使不同字符之间区分度变高，如反转一下。</li>\n</ul>\n</li>\n</ul>\n<blockquote>\n<p>像 <code>name like %xxx%</code>  这种怎么优化？</p>\n</blockquote>\n<p>看一下所有的查询条件，尽量将这种模糊匹配放到最后面，然后创建联合索引，多利用索引下推和索引覆盖。如果要查询的字段比较少，也可以建立联合索引，触发索引覆盖，因为二级索引的磁盘 IO 始终要少一些。</p>\n<p>还可以建立全文索引（但是准确度会有损失），对于只有前面有 <code>%</code>  的模糊查询，看可以通过生成列 + 反转匹配规则来优化。</p>\n<p>参考链接：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9tcC53ZWl4aW4ucXEuY29tL3MveWd2dVAzNUJfc0pBbEJIdXVFSmhmZw==\">https://mp.weixin.qq.com/s/ygvuP35B_sJAlBHuuEJhfg</span></p>\n<blockquote>\n<p><code>count(*)</code>  和 <code>count(1)</code>  的性能谁更好？</p>\n</blockquote>\n<p><code>count</code>  函数本质为：在 <code>server</code>  层维护一个 <code>count</code> ，然后存储引擎查到一条记录返回，判断 <code>count</code>  指定字段在记录里是否为 <code>NULL</code> ，不为 <code>NULL</code>  就 + 1。比如 <code>count(name)</code>  就要对返回的记录查询是否 <code>name</code>  为 <code>NULL</code> 。但是 <code>count(1)</code>  和 <code>count(*)</code>  其实都没啥区别，执行计划应该都是一样的，因为 1 和 * 都不是 <code>NULL</code> ，只要返回一条记录， <code>count</code>  直接 ++ 即可。</p>\n<p>拓展一下： <code>count(主键)</code>  和 <code>count(字段)</code>  的效率反而更低一些，server 会读取返回的记录判断字段是否为 <code>NULL</code> 。</p>\n<p>如果有二级索引的话，执行计划更愿意走二级索引，因为二级索引更小，花费的磁盘 IO 也更小。</p>\n<p>如果一个表会很大，又想频繁执行 <code>count(*)</code> ，优化方面可以取近似值（通过 <code>explain</code>  查看估计的 <code>row</code> ，如果精确度要求不高的话），还可以专门维护一张计数表。</p>\n<blockquote>\n<p>谈谈 <code>Buffer Pool</code></p>\n</blockquote>\n<p>为了缓存<strong>磁盘</strong>中的页，申请一片<strong>连续</strong>内存，叫做 <code>Buffer Pool</code> ，缓冲池的页与磁盘中的页一一对应。</p>\n<p>页与控制块为一对，控制块（保存页的表空间号，页号，在缓冲池中地址，链表节点等）在缓冲池前面，页在后面，向内占用内存，直到用完或者产生内存碎片。判断一个页是否已经被加载到缓存中，MySQL 使用的是哈希表，<strong>key</strong> 为表空间号 + 页号，<strong>value</strong> 为控制块。</p>\n<ul>\n<li><strong>Free 链表</strong>：该链表上的页都是空闲的，还没被使用（修改）</li>\n<li><strong>Flush 链表</strong>：该链表上的页都是脏页，要实现持久化需要刷脏</li>\n<li><strong>LRU 链表</strong>：last recently used 链表，当缓冲池内存不够时，需要把那些不常使用的页回收重新分配。</li>\n</ul>\n<p>链表里面的节点都是控制块。</p>\n<ul>\n<li>\n<p>最简单的 LRU 链表：页被访问，该页就会被放到 LRU 链表最前面。回收时，直接把链表尾部的页回收。但是 InnoDB 可以预读，会导致问题</p>\n<ul>\n<li>线性预读：顺序访问某个区的页超过某个值，就会将下个区的所有页都读到缓冲池中</li>\n<li>随机预读：一个区 13 个连续的页面都被加载到缓冲区中，该区所有的页面都会被加载到缓冲区</li>\n</ul>\n<p>这就导致可能根本用不到的页跑到链表前面，把常用的节点挤掉，降低 <code>Buffer Pool</code>  命中率。</p>\n<p>同时，全表扫描也会导致这种问题，因为访问的页实在太多了，而且很多页都是因为要全表扫描而用几次就不用了。总结就是：</p>\n<ul>\n<li>加载的页可能用不到</li>\n<li>加载太多页导致常用的页被挤掉</li>\n<li><strong>只要访问一次页，就要导致链表节点变动，开销太大</strong></li>\n</ul>\n</li>\n<li>\n<p>分区的 LRU 链表：把链表前部分设为 young 区域，存储使用频率非常高的页，也叫做<strong>热数据</strong>。后部分设为 old 区域，存储使用频率不是很高的页，又叫做冷数据。同时一个页在一定时间间隔内被连续访问，就加载到 old 区，因为一个页有很多条记录，全表扫描在扫描一个页时会间隔很短的访问很多次。 还有一点：为了避免频繁的节点移动，只有被访问的缓冲页位于 young 区域 1/4 后面才会被移动到头部。</p>\n</li>\n</ul>\n<p>刷脏：</p>\n<ol>\n<li>从 LRU 链表的冷数据中刷新一部分到磁盘中</li>\n<li>从 flush 链表中刷新一部分到磁盘中</li>\n</ol>\n<p>LRU 尾部的节点很容易被释放用于加载其他的页，在此之前，需要判断该页是否被修改，如果修改了，还需要先刷脏才行，这样其实是很慢的，而这种只刷新一个页面的操作叫做：<strong>BUF_FLUSH_SINGLE_PAGE</strong>。</p>\n<p><strong>其他</strong>：</p>\n<ul>\n<li>操作链表肯定是要上锁的，所以缓冲池可以被分为很多个 Buff Pool 实例，每个缓冲池实例维护各自的链表，提高并发量。</li>\n<li>5.7.5 之后，有了 Chunk 这个概念，表示一片连续内存的单位，Buff Pool 就是由若干个 Chunk 组成。一个 Chunk 包含了若干个页 + 控制块。所以服务器运行时就可以通过增删 Chunk 来改变缓冲池大小。</li>\n</ul>\n<blockquote>\n<p>事物的隔离级别有哪些？</p>\n</blockquote>\n<p><code>SQL</code>  标准定义的隔离级别有：</p>\n<ul>\n<li>读未提交：可以读到其他未提交事务修改的数据，也会导致脏读、不可重复读、幻读。</li>\n<li>读提交：可以读到其他事务提交后的数据，会导致不可重复读、幻读。</li>\n<li>可重复读：保证一个事务中同一个数据的一致性，会导致幻读</li>\n<li>串行化：性能最差，最安全。</li>\n</ul>\n<blockquote>\n<p><code>redo log</code>  的持久化策略</p>\n</blockquote>\n<p><code>redo</code>  日志的持久化策略有几种：</p>\n<ul>\n<li>后台每秒一次，将 <code>redo log buffer</code>  持久化到磁盘</li>\n<li><code>MySQL</code>  正常关闭时</li>\n<li>当 <code>redo</code>  日志大小超过 <code>redo log buffer</code>  大小一般时，就会持久化到磁盘中</li>\n<li>每次事务提交时，根据 <code>commit</code>  参数决定何时持久化到磁盘中。如果为 0，不会主动触发写入磁盘；如果为 1，主动持久化到磁盘；如果为 2，每次事务提交会写入 <code>redo log</code>  文件，本质上时写入 <code>Page Cache</code>  而不是写入磁盘。对于 2 而言， <code>MySQL</code>  崩了，但是只要操作系统不崩，数据也会从 <code>page cache</code>  写入到磁盘中。</li>\n</ul>\n<blockquote>\n<p><code>redo log</code>  的存储是怎样的？</p>\n</blockquote>\n<p><code>redo</code>  日志存储到文件中并使用循环存储，如果所有文件满了就会从头覆盖存储，覆盖是有条件的，如果不符合条件导致无法覆盖， <code>MySQL</code>  就会拒绝更新操作，从而阻塞。</p>\n<p>一个 <code>redo</code>  日志能否覆盖，取决于 <code>lsn</code>  值，它是该条 <code>redo</code>  日志的偏移量。关于 lsn 记录，每次一组 redo 日志写入时都会记录 lsn，同时缓冲池中的 flush 链表中每个页的控制块都记录了两个变量 old_lsn 和 new_lsn。前者是页第一次被修改时记录的 lsn，后者是最近一次修改时记录的 lsn。当刷脏时，会将 flush 尾节点先刷磁盘，假设这个尾节点的 old_lsn 为<strong> A</strong>，那么 redo 日志文件组中 <code>lsn&lt;A</code>  的日志都会失效。当一个页刷脏后，磁盘中页的 header 会记录下此时的 <code>new_lsn</code> ，当通过 redo 日志恢复时，如果关于某个页的 redo 日志的 <code>lsn&lt;new_lsn</code> ，也不用进行，直接跳过，加快速度。</p>\n<blockquote>\n<p><code>binlog</code>  和 <code>redo log</code>  的区别？</p>\n</blockquote>\n<ul>\n<li>适用对象不同： <code>binlog</code>  产生于 <code>server</code>  层，而 <code>redo log</code>  是 <code>InnoDB</code>  产生的</li>\n<li>文件格式不同： <code>redo log</code>  有专门的压缩，文件比较小； <code>binlog</code>  的文件格式比较多，有 <code>STATEMENT</code> 、 <code>ROW</code> 、 <code>MIXED</code>  三种。\n<ul>\n<li><code>STATEMENT</code> ：只记录命令，这种缺点就是如果命令中使用了一些动态函数，比如 <code>uuid</code> 、 <code>now</code>  之类的，主从数据库可能出现不一致。</li>\n<li><code>ROW</code> ：记录行记录被改变后的结果，改动多少行就记录多少行，有时 <code>binlog</code>  文件会因为一条命令变得很大</li>\n<li><code>MIXED</code> ：包含了前两种模式，会根据不同的情况自动使用 <code>STATEMENT</code>  和 <code>ROW</code>  模式。</li>\n</ul>\n</li>\n<li>写入方式不同： <code>binlog</code>  是追加， <code>redo log</code>  是循环写，文件大小是固定的。</li>\n<li>用途不同： <code>binlog</code>  用于备份恢复，主从复制； <code>redo log</code>  用于断电等故障恢复。</li>\n</ul>\n<blockquote>\n<p><code>binlog</code>  也能恢复数据，为什么还要 <code>redo log</code> ?</p>\n</blockquote>\n<p>这是 <code>MySQL</code>  发展原因， <code>InnoDB</code>  不是一开始就有的， <code>MySQL</code>  默认得存储引擎是 <code>MyISAM</code> ，而此时就已经有 <code>binlog</code>  了，所以设计 <code>binlog</code>  时一开始也没有设计恢复数据页能力和一些必要的功能。 <code>InnoDB</code>  出来后可以支持事务，所以需要 <code>redo log</code> ， <code>binlog</code>  在恢复事务方面并没有 <code>redo log</code>  那样好， <code>redo log</code>  不仅需要记录事务导致哪些页发生变化，还要记录 <code>undo log</code>  变化的页，还要考虑日志刷脏问题。</p>\n<p>而且 <code>redo log</code>  的出现，通过两阶段提交，保证了数据的完整：sql 更新操作将数据更新到内存，并且记录到 <code>redo log</code>  中，此时 <code>redo log</code>  处于 <code>prepare</code>  状态，执行器生成该操作的 <code>binlog</code>  并写入磁盘，此时 <code>redo log</code>  的状态才改为 <code>commit</code>  状态，完成更新。</p>\n<blockquote>\n<p>看你简历写了熟悉 <code>MVCC</code> ，能说说是什么原理吗？</p>\n</blockquote>\n<p>解释 <code>MVCC</code>  首先要回到事务隔离级别以及如何实现这种隔离级别：</p>\n<ul>\n<li><strong>读未提交</strong>是事务可以读到其他事务修改但未提交的数据，会造成脏读，这种隔离级别实现直接读最新的数据即可。</li>\n<li><strong>读提交</strong>是事务可以读到其他事务修改并提交后的数据，<strong>可重复读</strong>是连其他事务修改了的数据都不能读，这两者就是通过 <code>MVCC</code>  实现的。</li>\n<li><strong>串行化</strong>直接加锁即可，性能最低，但最安全。</li>\n</ul>\n<p>所以平时业务需要根据对数据一致性的不同要求设置不同的事务隔离。提到 <code>MVCC</code> ，是基于 <code>undo</code>  日志形成的版本链和 <code>ReadView</code>  实现的。</p>\n<ul>\n<li><strong>版本链</strong>：每条记录都有一个隐藏属性 <code>roll_pointer</code>  可以指向上一个记录的版本，这个版本实质上就是 <code>undo</code>  日志，而 <code>undo</code>  日志本身也是记录，也有 <code>roll_pointer</code> ，从而就形成了版本链。还需要提到的就是，记录还有一个隐藏属性 <code>trx_id</code> ，表示该记录版本的事务 <code>id</code> 。</li>\n<li><strong>ReadView</strong>：产生的时机待会再说，器中包括了几个重要的属性\n<ul>\n<li><code>m_ids</code> ：生成 <code>ReadView</code>  时，还在活跃的所有事务的 <code>id</code> ，活跃就是指还没有提交的事务。</li>\n<li><code>min_trx_id</code> ：生成 <code>ReadView</code>  时，还在活跃的最小的事务 <code>id</code> 。</li>\n<li><code>max_trx_id</code> ：生成 <code>ReadView</code>  时，下一个事务应该被分配的 <code>id</code> 。注意它并不是 <code>m_ids</code>  中的最大值。</li>\n<li><code>creator_trx_id</code> ：创建该 <code>ReadView</code>  的事务的 <code>id</code> 。</li>\n</ul>\n</li>\n</ul>\n<p>我们先看一下 <code>MVCC</code>  如何实现<strong>读提交</strong>的隔离性，这个级别的要求就是事务可以读到已提交的事务的修改。我们假设所有的事务都在对记录 A（此时 A 的 <code>trx_id</code>  为 80，该事务已经提交）进行修改和读取。当事务 100 修改了两次记录 A，那么关于记录 A 的版本链 <code>100-&gt;100-&gt;80</code> 。然后事务 90 要对记录 A 进行查询，此时就会响应 <code>select</code>  生成 <code>ReadView</code> ，那么这个 <code>ReadView</code>  的 <code>m_ids</code>  就包括了 100，然后先读到第一个 100，发现 100 已经在 <code>m_ids</code>  中，表示这个事务还没有提交，就不能读到它的修改的数据，就向下查找，直到找到 80 这个版本，发现已经提交了，可以读。</p>\n<p>后来假设事务 100 已经提交了，那么事务 90 再执行查询语句，又会新生成一个 <code>ReadView</code> ，此时 <code>m_ids</code>  就不会包含 100 了，也就可以读到 100 的记录版本了。</p>\n<p>再看一下如何实现<strong>可重复读</strong>的隔离级别，它要保证整个事务的数据都是一致的，其实对应的策略就是只生成一个 <code>ReadView</code> ，而不是每次查询都生成一个 <code>ReadView</code> 。</p>\n<p>参考：《MySQL 是怎样运行的》第 21 章，作者 —— 小孩子 4919</p>\n",
            "tags": [
                "八股文"
            ]
        },
        {
            "id": "https://cecilia.cool/2023/03/08/%E5%85%AB%E8%82%A1%E6%96%87/Redis/",
            "url": "https://cecilia.cool/2023/03/08/%E5%85%AB%E8%82%A1%E6%96%87/Redis/",
            "title": "Redis",
            "date_published": "2023-03-08T09:27:22.000Z",
            "content_html": "<p>本文积累了我在（准备）面试的关于 <code>Redis</code>  的问题，当然我不喜欢背八股文，所以就会强迫自己去系统性学习， <code>Redis</code>  之前我开了一个 <code>tag</code>  的，但是没更完就要准备面试了，Java 后端岗位太卷了！！</p>\n<p>大部分参考<span class=\"exturl\" data-url=\"aHR0cHM6Ly94aWFvbGluY29kaW5nLmNvbS8=\">小林 Coding</span> 以及敖丙的<span class=\"exturl\" data-url=\"aHR0cHM6Ly9naXRodWIuY29tL0FvYmluZ0phdmEvSmF2YUZhbWlseQ==\"> JavaFamily</span></p>\n<h1 id=\"通用篇\"><a class=\"anchor\" href=\"#通用篇\">#</a> 通用篇</h1>\n<blockquote>\n<p><code>Redis</code>  为什么这么快？</p>\n</blockquote>\n<p>有几个方面的原因：</p>\n<ul>\n<li>单线程执行，在 6.0 版本以前处理网络请求和数据操作都是单线程，减少了上下文切换，性能对于一些中小项目是完全足够的。网络请求处理使用的 IO 多路复用，数据处理因为是在内存中操作，CPU 资源并不会限制 <code>Redis</code>  性能。6.0 版本对网络处理使用了多线程，保留数据操作使用单线程，同时页保证不会出现并发问题。</li>\n<li>基于内存操作，不必多说，比 <code>MySQL</code>  要请求磁盘快多了。</li>\n<li>高效的数据结构， <code>Redis</code>  底层使用了很多高效的数据结构，比如 <code>SDS</code> 、压缩列表、跳表、哈希表等。</li>\n<li>自定义协议：使用了高性能的自定义 <code>Redis</code>  协议 RESP 和协议分析器。</li>\n</ul>\n<p>这些因素加起来使得 <code>Redis</code>  能够达到一秒十万级别的处理。</p>\n<p>参考链接：</p>\n<ul>\n<li><span class=\"exturl\" data-url=\"aHR0cHM6Ly9tcC53ZWl4aW4ucXEuY29tL3MvS3R6dmF3RG5RUXdoZmpuQ29YcGNNUQ==\">https://mp.weixin.qq.com/s/KtzvawDnQQwhfjnCoXpcMQ</span></li>\n<li><span class=\"exturl\" data-url=\"aHR0cHM6Ly9tcC53ZWl4aW4ucXEuY29tL3MvbXNjS0luV05BdWhDYmcxODNVbTlfZw==\">https://mp.weixin.qq.com/s/mscKInWNAuhCbg183Um9_g</span></li>\n</ul>\n<h1 id=\"数据类型篇\"><a class=\"anchor\" href=\"#数据类型篇\">#</a> 数据类型篇</h1>\n<p><code>String、List、Hash、Set、Zset、Stream、Hyperloglog、GEO、BitMap、BloomFilter</code></p>\n<blockquote>\n<p><code>redisObject</code>  是什么，为什么需要这个对象？</p>\n</blockquote>\n<p><code>redis</code>  是键值数据库，这意味着会对键有大量的操作，一些命令只适用于特定的数据类型，如 <code>zadd</code>  只适用于 <code>zset</code>  而不适合 <code>string</code> ，但是又有一些命令适用于所有 <code>key</code> ，如 <code>ttl</code> 、 <code>del</code>  等，所以这些命令要正确执行， <code>key</code>  就需要带有类型信息，使得程序可以检查 <code>key</code>  类型，选择合适的处理方式。</p>\n<p>为此 <code>redis</code>  构建了自己的类型系统，包括显示多态，类型判断，对象分配销毁和共享。 <code>redisObject</code>  的属性有 <code>type、encoding、lru、refcount、ptr</code></p>\n<figure class=\"highlight c\"><figcaption data-lang=\"c\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token keyword\">typedef</span> <span class=\"token keyword\">struct</span> <span class=\"token class-name\">redisObject</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre></pre></td></tr><tr><td data-num=\"3\"></td><td><pre>    <span class=\"token comment\">// 类型，判断数据类型</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre>    <span class=\"token keyword\">unsigned</span> type<span class=\"token operator\">:</span><span class=\"token number\">4</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre></pre></td></tr><tr><td data-num=\"6\"></td><td><pre>    <span class=\"token comment\">// 编码方式，判断数据结构</span></pre></td></tr><tr><td data-num=\"7\"></td><td><pre>    <span class=\"token keyword\">unsigned</span> encoding<span class=\"token operator\">:</span><span class=\"token number\">4</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"8\"></td><td><pre></pre></td></tr><tr><td data-num=\"9\"></td><td><pre>    <span class=\"token comment\">// LRU - 24 位，记录最末一次访问时间（相对于 lru_clock）; 或者 LFU（最少使用的数据：8 位频率，16 位访问时间）</span></pre></td></tr><tr><td data-num=\"10\"></td><td><pre>    <span class=\"token keyword\">unsigned</span> lru<span class=\"token operator\">:</span>LRU_BITS<span class=\"token punctuation\">;</span> <span class=\"token comment\">// LRU_BITS: 24</span></pre></td></tr><tr><td data-num=\"11\"></td><td><pre></pre></td></tr><tr><td data-num=\"12\"></td><td><pre>    <span class=\"token comment\">// 引用计数</span></pre></td></tr><tr><td data-num=\"13\"></td><td><pre>    <span class=\"token keyword\">int</span> refcount<span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"14\"></td><td><pre></pre></td></tr><tr><td data-num=\"15\"></td><td><pre>    <span class=\"token comment\">// 指向底层数据结构实例</span></pre></td></tr><tr><td data-num=\"16\"></td><td><pre>    <span class=\"token keyword\">void</span> <span class=\"token operator\">*</span>ptr<span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"17\"></td><td><pre></pre></td></tr><tr><td data-num=\"18\"></td><td><pre><span class=\"token punctuation\">&#125;</span> robj<span class=\"token punctuation\">;</span></pre></td></tr></table></figure><p>当执行一个命令时，假设是 <code>zadd</code> ，就会先将 <code>key</code>  从字典中找到对应的 <code>redisObject</code> ，如果为 <code>null</code>  就说明 <code>key</code>  不存在，然后继续检查 <code>type</code>  是否正确，最后根据 <code>encoding</code>  判断底层的数据结构到底是什么来调用多态函数。</p>\n<p>参考链接：</p>\n<ul>\n<li>\n<p><span class=\"exturl\" data-url=\"aHR0cHM6Ly9wZGFpLnRlY2gvbWQvZGIvbm9zcWwtcmVkaXMvZGItcmVkaXMteC1yZWRpcy1vYmplY3QuaHRtbA==\">https://pdai.tech/md/db/nosql-redis/db-redis-x-redis-object.html</span></p>\n</li>\n<li>\n<p><span class=\"exturl\" data-url=\"aHR0cHM6Ly93d3cubW9kYi5wcm8vZGIvNzI5NDc=\">https://www.modb.pro/db/72947</span></p>\n</li>\n</ul>\n<blockquote>\n<p>Redis 的 <code>SDS</code>  是什么？</p>\n</blockquote>\n<p>脑图：回忆一下当初学 C 语言时字符串的缺陷， <code>SDS</code>  就是为了克服这些缺陷。</p>\n<p><code>SDS</code>  是 <code>Redis</code>  自定义的简单动态字符串，也是 <code>Redis</code>  最基本的数据结构之一。设计 <code>SDS</code>  是因为 c 语言的字符串问题太多，性能太低了。主要问题是：</p>\n<ul>\n<li>获取长度时间复杂度为 <code>O(N)</code> ， <code>SDS</code>  内部维护了当前字符串长度 <code>len</code> ， <code>O(1)</code>  复杂度</li>\n<li>操作不方便，容易溢出，类似 <code>strcat</code>  这种函数，拼接两个字符串，都会默认前一个字符串剩余空间足够，所以很不方便。 <code>SDS</code>  维护了当前分配空间大小 <code>alloc</code> ，可以检测剩余空间。</li>\n<li>以 <code>\\0</code>  结束，需要指定编码格式。这种性质使得字符串只能存储文本文件， <code>SDS</code>  使用了字节数组，使得可以存储任何可转为字节的数据。</li>\n</ul>\n<p><code>SDS</code>  还可以动态扩容，并且会还会多分配一些未使用空间，减少分配次数。具体是，当操作触发扩容机制，会先算出需要扩容到多少才够，这个值保存在 <code>newLen</code>  中，然后真正扩容还会多分配一些空间：</p>\n<ul>\n<li>如果 <code>newLen &lt; 1MB</code> ，那么 <code>newLen *= 2</code>  再进行扩容</li>\n<li>如果 <code>newLen &gt;= 1MB</code> ，那么 <code>newLen += 1MB</code>  再进行扩容</li>\n</ul>\n<p><code>SDS</code>  设计了不同类型的结构体，区别在于 <code>len</code>  和 <code>alloc</code>  的大小不同，通过为不同大小字符串灵活分配，可以节省内存。</p>\n<figure class=\"highlight c\"><figcaption data-lang=\"c\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token keyword\">struct</span> <span class=\"token keyword\">__attribute__</span> <span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>__packed__<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> <span class=\"token class-name\">sdshdr16</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre>    <span class=\"token class-name\">uint16_t</span> len<span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre>    <span class=\"token class-name\">uint16_t</span> alloc<span class=\"token punctuation\">;</span> </pre></td></tr><tr><td data-num=\"4\"></td><td><pre>    <span class=\"token keyword\">unsigned</span> <span class=\"token keyword\">char</span> flags<span class=\"token punctuation\">;</span> \t<span class=\"token comment\">// SDS 类型</span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre>    <span class=\"token keyword\">char</span> buf<span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"6\"></td><td><pre><span class=\"token punctuation\">&#125;</span><span class=\"token punctuation\">;</span></pre></td></tr></table></figure><p>最后， <code>SDS</code>  还使用了编译优化 <code>__attribute__ ((__packed__))</code> ，告诉编译器取消结构在编译中的对齐优化，而是实际使用多少就分配多少。比如一个结构体有 1 个 <code>int</code>  和 1 个 <code>char</code> ，正常的优化对齐会使 <code>char</code>  对齐 <code>int</code> ，也就是 <code>char</code>  也会占 3 个字节。其实这 3 个字节就浪费了。 <code>SDS</code>  的编译优化就可以使 <code>char</code>  只分配一个字节。</p>\n<p><code>SDS</code>  与 <code>redisObject</code>  的关系根据字符串存储的值的不同而有所不同：</p>\n<ul>\n<li>\n<p>字符串对象保存的整数值，并且该整数可以用 <code>long</code>  表示，那么就会把 <code>redisObject.ptr</code>  从 <code>void*</code>  改为 <code>long</code> ，并设置 <code>encoding=int</code></p>\n</li>\n<li>\n<p>字符串对象保存的字符串，则使用 <code>SDS</code>  保存字符串， <code>ptr</code>  指向 <code>SDS</code>  地址，实际数据放在 <code>SDS</code>  中的 <code>buf</code>  中。如果字符串字节长度 <code>&lt;=X</code> ，则 <code>encoding=embstr</code>  反之则 <code>encoding=raw</code> 。（x 在个版本中定义不同，2.+ 是 32，3.0-4.0 是 39，5.0 是 44）</p>\n</li>\n</ul>\n<p><code>embstr</code>  会一次性分配 <code>redisObject</code>  和 <code>SDS</code>  的空间，有利于内存连续更好利用 CPU 缓存，减少内存分配次数，内存释放次数。而 <code>raw</code>  就需要分配两次。 <code>embstr</code>  是只读的，如果要对 <code>embstr</code>  操作，就会先升级为 <code>raw</code>  再执行修改命令。</p>\n<p>参考链接：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9tcC53ZWl4aW4ucXEuY29tL3MvcXB0RTE3MnNsZ182VGwxeXV6ZGJmdw==\">https://mp.weixin.qq.com/s/qptE172slg_6Tl1yuzdbfw</span></p>\n<blockquote>\n<p><code>redis</code>  的 <code>BitMap</code>  是什么？</p>\n</blockquote>\n<p>位图，本质就是比特数组，用于存储一些只有两个状态的数据，占用空间小，实际应用：打卡，判断用户登录态（5000 万用户只需要 6MB 空间）</p>\n<blockquote>\n<p>刚才你提到了 <code>HyperLoglog</code> ，知道怎么原理吗？</p>\n</blockquote>\n<p><code>HyperLoglog</code>  是一个基于基数统计（集合中不重复元素的个数）的数据结构，其实底层算法很早之前就有人提出了，但是 <code>Redis</code>  第一次使用数据结构将其实现。说到应用，我们可以假设一个场景，有个业务需求需要每个网页都统计访问量，同一 IP 多次访问只算做一次访问。如果是在业务端实现，最先想到的就是对每一个网页加一个 <code>Set</code> ，最后需要统计量时直接获取集合大小即可，如果访问量很大，上百万、千万什么的，就非常消耗内存，不可能为了这么小的业务需求付出这么大的内存，并且这种业务是可以容忍一定误差的，所以就可以使用 <code>Redis</code>  里的 <code>HyperLoglog</code> 。</p>\n<p>至于原理，涉及到统计概率中的伯努利实验，以及后来者引入的桶和加权平均等修正，我还没来得及深入了解。仅仅只知道这确实可以统计去重元素个数，但是存在一点误差，如果可以容忍误差，那么性能是很高的。</p>\n<blockquote>\n<p><code>GEO</code>  是什么，有什么用？</p>\n</blockquote>\n<p><code>GEO</code>  并没有设计新的数据结构，而是使用了 <code>Sorted Set</code> ，使用 <code>GeoHash</code>  编码方法实现了经纬度到元素权重分数的转换，关键就是【对二维地图做区间划分】和【对区间进行编码】，经纬度落到某个区间，就用这个区间的编码值表示 <code>Sorted Set</code>  元素的权重分数。</p>\n<p>实际应用如<strong>滴滴叫车</strong>：主要使用 <code>GEOADD</code>  和 <code>GEORADIUS</code>  两个命令。使用 <code>GEO</code>  集合保存所有车辆的经纬度，当用户想寻找自己附近的车，LBS（Location Based Services）应用就可以以用户的经纬度为中心指定公里内的车辆信息找到并返回。</p>\n<blockquote>\n<p><code>Redis</code>  的压缩列表了解过吗？</p>\n</blockquote>\n<p>压缩列表是 <code>Redis</code>  的基础数据结构，如果 <code>list</code>  或者 <code>hash</code>  的节点较少，且保存的项都是一些小整数或者短字符串，通常会使用压缩列表来作为列表键的底层实现。</p>\n<p>压缩列表的本质是数组，不用链表是因为链表的节点之间内存不连续，无法高效利用 <code>CPU</code>  缓存，命中率很低。而压缩列表是内存连续的，命中率高。</p>\n<p>压缩列表前面几个字段是列表的一些信息，比如列表占用字节数，列表尾部节点的偏移量，节点数量，压缩列表结束点。而每个节点的构成为：前一个节点的长度，自身数据类型和节点长度，数据。</p>\n<p>为了尽可能节省内存，和 <code>MySQL</code>  记录中 <code>varchar</code>  一样，使用了不同字节来记录数据长度。前一个节点长度在 256 之内，使用 1 个字节，反之使用 5 个字节。但是这种机制会导致<strong>连锁更新</strong>问题，比如首节点插入长度大于 256 的数据，而下一个节点之前记录长度使用的 1 字节，此时就需要扩容，扩容后可能自身也超过了 256 字节，它的下一个节点也要扩容，如此往复，直到最后一个节点扩容完成。</p>\n<p>参考链接：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9tcC53ZWl4aW4ucXEuY29tL3MvcXB0RTE3MnNsZ182VGwxeXV6ZGJmdw==\">https://mp.weixin.qq.com/s/qptE172slg_6Tl1yuzdbfw</span></p>\n<blockquote>\n<p><code>Redis</code>  的哈希表了解过吗？</p>\n</blockquote>\n<p>哈希表是 <code>Redis</code>  的基础数据结构，数据类型 <code>hash</code>  如果节点很多或者项是大的整数、长字符串，就会使用哈希表。哈希表底层实现使用的是数组，链式增长解决哈希冲突。当负载因子 <code>&gt;= 1</code>  时，如果没有执行 <code>rdb</code>  或 <code>aof</code>  就会 <code>rehash</code> 。当负载因子 <code>&gt;=5</code>  时，不论有没有 <code>rdb\\aof</code>  都会 <code>rehash</code> 。</p>\n<p><code>rehash</code>  使用的是渐进式 <code>rehash</code> ，假设原哈希表 1 扩容后为哈希表 2，那么在 <code>rehash</code>  的过程中，每次有请求增删改查哈希表 1，就会把当前索引的节点转移到哈希表 2，使得整个 <code>rehash</code>  过程分配到各个请求上，避免一次性 <code>rehash</code>  的耗时操作。</p>\n<p>如果有一个查询请求，在哈希表 1 查不到，就会去哈希表 2 查询。在渐进式 <code>rehash</code>  进行期间，哈希元素的操作都是在两个哈希表进行的。</p>\n<p>参考链接：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9tcC53ZWl4aW4ucXEuY29tL3MvcXB0RTE3MnNsZ182VGwxeXV6ZGJmdw==\">https://mp.weixin.qq.com/s/qptE172slg_6Tl1yuzdbfw</span></p>\n<blockquote>\n<p><code>Redis</code>  的整数集合是什么？</p>\n</blockquote>\n<p>整数集合是实现 <code>set</code>  的数据结构之一，底层其实就是整数数组</p>\n<figure class=\"highlight c\"><figcaption data-lang=\"c\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token keyword\">typedef</span> <span class=\"token keyword\">struct</span> <span class=\"token class-name\">intset</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre>    <span class=\"token comment\">// 编码方式</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre>    <span class=\"token class-name\">uint32_t</span> encoding<span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre>    <span class=\"token comment\">// 集合包含的元素数量</span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre>    <span class=\"token class-name\">uint32_t</span> length<span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"6\"></td><td><pre>    <span class=\"token comment\">// 保存元素的数组</span></pre></td></tr><tr><td data-num=\"7\"></td><td><pre>    <span class=\"token class-name\">int8_t</span> contents<span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"8\"></td><td><pre><span class=\"token punctuation\">&#125;</span> intset<span class=\"token punctuation\">;</span></pre></td></tr></table></figure><p>但是数组的元素类型由 <code>encoding</code>  控制，从 <code>8bit</code> 、 <code>16bit</code> 、 <code>32bit</code> 、 <code>64bit</code>  增长。不一来就用 <code>64bit</code>  也是想尽量节省空间。在插入新元素时，会维护有序性和唯一性。如果插入的整数所占用的字节超过了数组 1 个元素的字节，就要先升级再插入</p>\n<blockquote>\n<p><code>Redis</code>  的跳表了解过吗？</p>\n</blockquote>\n<p>跳表算是一种很优雅的实现，相较于普通链表，查询效率提升，相较于二叉树，省去了平衡、树退化的问题。比如一个链表节点为： <code>1 2 3 4 5 </code> 那么从中挑出一半 <code>1 3 5</code>  形成新的链表，并且将新的链表接到原来的链表上面，如此直到最上面的节点只有 1 个。这种数据结构使得查询效率为 <code>O(logn)</code> 。但是当插入新节点时，需要调整上面的节点，严重时时间复杂度还是 <code>O(n)</code> 。</p>\n<p>所以 <code>Redis</code>  也是优化了跳表，在每次插入节点时就通过随机数决定其层数（随机数 r&lt;=0.25 就加一，继续生成，&gt;0.25 就停止），然后提前加入到对应的层数。这样虽然不是严格的 <code>log2N</code> ，也许要存储的节点会变多，也可能变小，但总的效率依然维持在一个很高的水平。</p>\n<p>在 <code>Redis</code>  常用的数据类型中， <code>zset</code>  就是通过跳表实现的。</p>\n<p>至于为什么 <code>zset</code>  使用了跳表而不是平衡树，原因：</p>\n<ul>\n<li>平衡树实现复杂，还要考虑插入删除后树的平衡调整</li>\n<li>在<strong>范围查找</strong>时，平衡树比较难实现，而跳表只需要找到最小值然后遍历即可。</li>\n</ul>\n<p>参考链接：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9tcC53ZWl4aW4ucXEuY29tL3MvTk9zWGRyTXJXd3E0TlRtMTgwYTZ2dw==\">https://mp.weixin.qq.com/s/NOsXdrMrWwq4NTm180a6vw</span></p>\n<blockquote>\n<p><code>redis</code>  的 <code>quicklist</code>  了解过吗？</p>\n</blockquote>\n<p><code>3.0</code>  版本之前，列表都是使用 <code>list</code>  和压缩列表（ <code>ziplist</code> ）实现的， <code>3.2</code>  之后就是只使用 <code>quicklist</code>  实现了。 <code>quicklist</code>  其实就是 <code>list+ziplist</code> 。</p>\n<blockquote>\n<p><code>redis</code>  的 <code>listpack</code>  了解过吗？</p>\n</blockquote>\n<p><code>listpack</code>  也是一种压缩列表的实现，之前提到的 <code>quicklist</code>  并没有解决连锁更新的问题，就是因为节点记录了前一个节点的长度，为了能后从后向前遍历，所以 <code>listpack</code>  不再记录前一个节点的长度而是记录当前节点的长度。其实这也能从后向前遍历，是由 <code>lpDecodeBacklen</code>  函数实现，它会从当前列表项起始位置的指针开始，向左逐个字节解析，得到前一项的  <code>entry-len</code>  值。</p>\n<h1 id=\"持久化篇\"><a class=\"anchor\" href=\"#持久化篇\">#</a> 持久化篇</h1>\n<blockquote>\n<p><code>Redis</code>  如何保证数据持久化？</p>\n</blockquote>\n<p><code>Redis</code>  为了保证数据高可用，引入了持久化机制， 在早期版本，还有 VM，后来版本不推荐了。现在一般都是使用 <code>RDB</code> 、 <code>AOF</code>  或者混合持久化。</p>\n<ul>\n<li>\n<p><code>RDB</code>  通过对内存数据拍摄快照来持久化数据，触发机制是在一定时间内发生一定次数的修改操作。当然也可以使用 <code>save/bgsave</code>  主动拍摄快照，前者会阻塞线程，后者才会 <code>fork</code>  一个子线程进行快照拍摄。因为采用了压缩算法，实际占用空间很小。异步存储为了保证数据一致性，借助了操作系统的 <code>Copy on Write</code>  机制，主线程修改哪个页，就会先将这个页复制出来，再在复制页进行修改。等快照拍摄结束，再将复制的页合并到原始内存中。（<strong>也有博客说复制页是用于 rdb，主进程还是在原地址修改</strong>）。</p>\n</li>\n<li>\n<p><code>AOF</code>  通过存储执行的命令到磁盘中保证数据的持久性，可以配置多种存储方式，比如执行一条命令就存储一条，或者每秒存储一次，或者看系统心情，什么时候有空什么时候就将缓冲区的命令存进去。 <code>AOF</code>  机制执行久了，就会导致文件保存了很多无效的命令，所以需要重写 <code>AOF</code>  文件 —— <code>bgrewriteaof</code> ，过程为：子线程遍历 <code>Redis</code>  内存生成一系列指令，然后将这些指令序列化到临时文件中，过程中的增量命令会同时写到<strong> aof 缓冲区</strong>和<strong> aof 重写缓冲区</strong>，会追加到临时文件中，最后替换 <code>AOF</code>  文件。这里需要重点说一下，我们将数据写入到文件中时，其实是先写入到内核缓冲区，再到磁盘缓冲区，最后到磁盘，最后一个阶段我们是无法介入的，但是可以调用 <code>fsync()</code>  强制将数据刷新到磁盘缓冲区。 <code>redis</code>  默认是每秒调用一次。（有参数控制何时重写，比如文件大小超过多少，增量达到多少）</p>\n</li>\n<li>\n<p><code>混合持久化</code> ：4.0 版本后还出现了混合持久化（混合持久化工作在 <strong>AOF 日志重写过程</strong>。），该机制必须打开 <code>AOF</code> ，（就是重写 aof 文件）隔一段时间拍摄快照，生成 <code>rdb</code>  数据，两次快照之间的记录使用 <code>AOF</code>  日志来记录，并追加到 <code>rdb</code>  数据后面。恢复数据时，先回复 <code>rbd</code>  数据，再执行 <code>AOF</code>  日志。这种机制既解决了 <code>rdb</code>  快照摄时突然断电导致整个快照丢失（因为还在临时文件中），也解决了 <code>AOF</code>  文件太大，不断重写的性能消耗。</p>\n</li>\n</ul>\n<p>参考链接：</p>\n<ul>\n<li><span class=\"exturl\" data-url=\"aHR0cHM6Ly9wZGFpLnRlY2gvbWQvZGIvbm9zcWwtcmVkaXMvZGItcmVkaXMteC1yZGItYW9mLmh0bWwjcmRiJUU1JTkyJThDYW9mJUU2JUI3JUI3JUU1JTkwJTg4JUU2JTk2JUI5JUU1JUJDJThGLTQtMCVFNyU4OSU4OCVFNiU5QyVBQw==\">Redis 进阶 - 持久化</span></li>\n<li><span class=\"exturl\" data-url=\"aHR0cHM6Ly9tcC53ZWl4aW4ucXEuY29tL3MvT19xRGNvNi1EYXN1M1JvbVdJS19JZw==\">https://mp.weixin.qq.com/s/O_qDco6-Dasu3RomWIK_Ig</span></li>\n</ul>\n<blockquote>\n<p><code>Redis</code>  的大 <code>key</code>  对持久化有什么影响？</p>\n</blockquote>\n<p>首先大 <code>key</code>  不是指 <code>key</code>  的长度很大或者字面量很大，而是指对应的 <code>value</code>  占用内存很大。</p>\n<p>大 <code>key</code>  在 <code>aof</code>  机制写入命令时，如果是 <code>Always</code>  机制，那么调用 <code>fsync()</code>  函数，将数据从内核缓冲区写入磁盘时，必须等待写完函数才会返回，如果是大 <code>key</code> ，数据量很大，自然就会导致长时间阻塞。如果是 <code>Everysec</code>  机制，因为是创建异步任务调用 <code>fsync</code> ，所以不会影响主线程。</p>\n<p>同时，如果 <code>redis</code>  存储了很多大 <code>key</code> ，一方面会使 <code>aof</code>  文件频繁重写，另一方面会导致主线程对应的页表越大， <code>rdb</code>  异步快照和 <code>aof</code>  重写都会 <code>fork</code>  一个子线程，就需要为子线程复制一份页表，页表越大，复制过程就越长，主线程阻塞在 <code>fork</code>  调用就越久。</p>\n<h1 id=\"高可用篇\"><a class=\"anchor\" href=\"#高可用篇\">#</a> 高可用篇</h1>\n<blockquote>\n<p><code>Redis</code>  如何保证数据一致性？</p>\n</blockquote>\n<p>首先，很难保证缓存和数据库 100% 数据一致，因为我们引入 <code>Redis</code>  本身是为了性能，花很大代价完全保证数据一致性，有时性能反而还会下降，只能说尽量吧。</p>\n<p>考虑到并发，面对更新请求，我了解到的解决方案就是：先更新数据库再删除缓存（也有问题，但是相对概率很小）。如果是更新数据库 + 更新缓存，并发问题很大，哪怕是先删除缓存再更新数据库，也存在并发问题，因为从数据库拿数据到缓存中是两步：从数据库读取，写到缓存中。只要不是原子操作，在并发环境就可能导致数据不一致。</p>\n<p>再考虑到删除缓存操作可能会失败，现在的解决方案一般是使用消息队列或者订阅数据库变更日志再操作缓存（canal）。</p>\n<p>参考链接：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9tcC53ZWl4aW4ucXEuY29tL3MvRDRJazZsVEFfeVNCT3lEM3dhTmoxdw==\">https://mp.weixin.qq.com/s/D4Ik6lTA_ySBOyD3waNj1w</span></p>\n<blockquote>\n<p><code>Redis</code>  内存淘汰是怎么一回事？</p>\n</blockquote>\n<p>首先 <code>Redis</code>  对于过期了的 <code>key</code>  采用两种策略：惰性删除和定期删除。所以当内存耗尽时， <code>Redis</code>  存在过期 / 没过期两种键，所以删除策略也有不同，有 8 种：直接返回错误 / 删除 LRU、LFU 最早的过期（所有）key / 随机删除过期（所有）key</p>\n<p>传统的 <code>LRU</code>  存在存储、误删的缺点，所以 <code>Redis</code>  配置文件定义了一个属性，默认为 5，会取出 5 个的 <code>key</code> ，按照 <code>LRU</code>  算法删除对应 <code>key</code> 。</p>\n<p>至于 <code>LFU</code> ， <code>Redis</code>  也是采用了一些随机算法的策略，因为在 <code>RedisObject</code>  中有个 <code>lru</code>  属性，前 <code>24bit</code>  用于记录 <code>LRU</code> ，后 <code>8bit</code>  记录 <code>LFU</code>  的访问热度。 <code>8bit</code>  最多表示 255，所以不能单纯的访问一次就自增，而是通过比较两个参数：</p>\n<ol>\n<li>从 0~1 随机生成一个随机数 <code>R</code></li>\n<li>配置中有一个 <code>factory</code>  参数，用于计算 <code>P = 1 / (差值*factory+1)</code> 。这里的差表示当前热度减去初始值。</li>\n<li>如果 <code>P&gt;R</code> ，热度 + 1，反之 + 0。</li>\n</ol>\n<p>可以看出，热度越高，那么上升的概率越小。</p>\n<p>参考链接：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9tcC53ZWl4aW4ucXEuY29tL3MvLWNhTVRyT1hRdS1vME80NGU2STlkUQ==\">https://mp.weixin.qq.com/s/-caMTrOXQu-o0O44e6I9dQ</span></p>\n<blockquote>\n<p><code>Redis</code>  主从复制原理是什么？</p>\n</blockquote>\n<p>主节点和从节点的数据交互方式分为全量复制和增量复制。</p>\n<ul>\n<li>全量复制：在从节点与主节点建立连接（tcp 长连接）时，从节点先发送 <code>sync</code> ，主节点会执行 bgsave 生成 rdb 文件再发送，使得从节点加载并初始化数据。在生成 <code>rdb</code>  时新来的写命令请求会放在一个缓冲区，等 <code>rdb</code>  传输完了就会传输这个缓冲区数据到 <code>salve</code>  中。这个缓冲区就是 <code>replication buffer</code> 。</li>\n<li>增量复制：如果从节点（ <code>client</code> ）和主节点不断开连接，那么就可以一直通过 <code>replication buffer</code> （如果满了就会断开连接，删除缓存，重连）传输数据，但是如果连接不小心断开了， <code>replication buffer</code>  就会被释放。此时就需要<strong>增量复制</strong>。在建立主从连接时，双方会维护一个 <code>offset</code> ，在主节点将写操作记录到 <code>replication buffer</code>  时，还会记录到 <code>repl_backlog_buffer</code>  环形缓冲区，从节点维护的 <code>offset</code>  就是同步数据的偏移量。主节点维护的 <code>master_repl_offset</code>  就是环形缓冲区的当前数据的偏移，当从节点重新连接，发现 <code>master</code>  的 <code>offset</code>  没有覆盖自己维护的 <code>offset</code> ，就可以进行增量复制，如果覆盖了，就走全量复制。（ <code>abs(m_offset-s_offset) &lt; len</code> ）</li>\n</ul>\n<p>参考链接：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9qdWVqaW4uY24vcG9zdC82OTgxNzQ0NjMxMDAwMDcyMjA1\">https://juejin.cn/post/6981744631000072205</span></p>\n<blockquote>\n<p><code>Redis</code>  集群中如何判断某个节点是否正常工作？</p>\n</blockquote>\n<p>一般是一半以上的主节点 <code>ping</code>  一个节点都超时时，就会认为该节点挂了。</p>\n<ul>\n<li>主节点会默认每 <code>10s</code> （可以通过参数 <code>repl-ping-slave-period</code>  控制）向从节点发送 <code>ping</code>  命令，从而判断从节点存活性和连接状态。</li>\n<li>从节点每 1 秒向主节点发送 <code>replconf ack</code>  命令，给主节点报告自己的复制偏移量。一方面检测网络状态，另一方面检测数据复制情况。</li>\n</ul>\n<blockquote>\n<p><code>Redis</code>  的脑裂现象了解过吗？</p>\n</blockquote>\n<p>在 <code>Redis</code>  集群中，如果主节点 A 因为网络问题和集群失联了，但是客户端还在不断往主节点 A 写入数据。哨兵发现后选举出新的主节点 B，然后 A 网络恢复后降级为从节点 A，此时需要主从同步，从节点 A 需要先清空数据再获取主节点 B 的 <code>rdb</code>  文件。这就导致之前客户端写的数据没了，这其实也是一种数据不一致问题。</p>\n<p>解决方案为：如果主节点的连接的从节点少于 N 个（ <code>min-slaves-to-write</code> ）或者主从同步的延迟高于 x（ <code>min-slaves-max-lag</code> ），就禁止客户端写入数据。</p>\n<blockquote>\n<p><code>Redis</code>  的哨兵机制了解过吗？</p>\n</blockquote>\n<p>哨兵机制可以检测集群中主节点存活情况，如果主节点挂了，可以选举一个从节点为新的主节点，并把新的主节点的信息通知给客户端和其他从节点。所以哨兵主要负责：监控、选主、通知。</p>\n<ul>\n<li>\n<p><strong>监控</strong>：哨兵每隔 1 秒就会向节点发送 <code>ping</code>  命令，如果节点回复超时（ <code>down-after-milliseconds</code>  配置选项），就判定该节点<strong>主观下线</strong>。对于主节点而言，还有<strong>客观下线</strong>，就是主节点没有挂，因为服务器压力或者网络问题导致回复超时，如果哨兵集群超过一半都回复超时就会判定为<strong>客观下线</strong>。如果主节点并没有挂，那么超过一半的哨兵都回复超时的概率也比较低，这都是为了减少误判的概率。</p>\n</li>\n<li>\n<p><strong>选主</strong>：当哨兵发现主节点挂了，就会问其他哨兵是否觉得主节点挂了，超过一半那么该节点就认为其<strong>客观下线</strong>，然后该哨兵就会成为候选者，向其他哨兵请求投票，如果票数超过一半并且票数超过 <code>quorum</code> ，就会作为 <code>leader</code>  进行主从故障转移，选举出新的主节点。选举主节点的过程：</p>\n<ul>\n<li>从节点中选一个升级为主节点（从节点的网络状态，优先级，复制进度，ID 选出最合适的）</li>\n<li>让其他从节点修改复制目标，复制新的主节点</li>\n<li>将新的主节点的 IP 和信息通过发布者 / 订阅者机制通知给客户端</li>\n<li>继续监控旧主节点，一旦恢复上线，将其降为从节点</li>\n</ul>\n</li>\n<li>\n<p><strong>通知</strong>：客户端通过发布者 / 订阅者机制知道新选举出来的主节点。主从节点切换完后会在 <code>+switch-master</code>  频道发布新主节点的 IP 地址和信息。</p>\n</li>\n</ul>\n<blockquote>\n<p>哨兵集群之间是如何通信的？</p>\n</blockquote>\n<p>在搭建主从集群的哨兵集群时，每个哨兵只需要指定主节点名字、 <code>IP</code> 、端口以及 <code>quorum</code> ，哨兵之间就是通过 <code>Redis</code>  的发布者 / 订阅者机制通信的。主节点有一个 <code>_sentinel_:hello</code> ，各个哨兵把自己的 IP 和信息发布到 <code>_sentinel_:hello</code>  频道，其他哨兵就可以获取信息进行通信。</p>\n",
            "tags": [
                "八股文"
            ]
        },
        {
            "id": "https://cecilia.cool/2023/03/05/%E5%85%AB%E8%82%A1%E6%96%87/Java%E9%9B%86%E5%90%88/",
            "url": "https://cecilia.cool/2023/03/05/%E5%85%AB%E8%82%A1%E6%96%87/Java%E9%9B%86%E5%90%88/",
            "title": "Java集合",
            "date_published": "2023-03-05T02:02:30.000Z",
            "content_html": "<p>本文涉及到 Java 集合部分，也包括线程安全的集合，还是以面试题的形式记录。</p>\n<blockquote>\n<p>1. <code>ArrayList</code>  和 <code>LinkedList</code>  遍历谁更快？</p>\n</blockquote>\n<p><code>ArrayList</code>  更快， <code>ArrayList</code>  的优势就是内存连续，如果遍历 <code>LinkedList</code> ，随机 IO 可能更多一些</p>\n<blockquote>\n<p>2. 谈谈你对 <code>ArrayList</code>  的理解</p>\n</blockquote>\n<p>从我学习程度来看， <code>ArrayList</code>  需要注意的是：</p>\n<ul>\n<li>线程不安全，如果要保证安全，可以使用 <code>Vector</code>  或者使用 <code>Collections.synchronizedList(xx)</code> ，前者在每个操作方法上都加了 <code>synchronized</code>  关键字，后者内部会维护一个排他锁，每次对集合操作时，都需要先竞争锁。</li>\n<li>迭代器的 <code>fail-fast</code>  机制：这其实是每个实现了 <code>iterator</code>  集合都需要注意的地方，每次对数组进行增删，都会使得 <code>modCount++</code> ，而迭代器的 <code>next</code>  函数会检测 <code>modCount</code>  是否和最开始的自己保存的 <code>modCount</code>  相同，也就是检测在迭代的过程中数组是否发生了改变，<strong>因为这可能使得迭代过程中漏了某些元素或者重复遍历某些元素</strong>。</li>\n<li><code>ArrayList</code>  内部是使用 <code>Object</code>  数组 -- <code>elementData</code> ，但是该变量没有被 <code>private</code>  修饰，代码注释写的是方便内部类更快的访问该属性，如果被 <code>private</code>  修饰，那么同样的代码反编译后的字节码文件更复杂一些。</li>\n<li>扩容时，是 1.5 倍增长，而 <code>Vector</code>  扩容默认两倍扩容。</li>\n</ul>\n<blockquote>\n<p>3. <code>HashMap</code>  了解吗？1.7 版本和 1.8 版本都什么区别</p>\n</blockquote>\n<p><code>JDK1.7</code>  的 <code>HashMap</code>  使用的是数组桶加链表，如果链表过长，那么时间复杂度会退化为 O (n)，而 1.8 使用的引入了红黑树，当链表节点为 8 时，就转为红黑树。至于为什么是 8，估计也是个统计概率值。</p>\n<p><code>1.7</code>  节点插入时使用的是头插法，而 1.8 使用的是尾插法，头插在并发环境下容易出现环导致死循环，具体形成原因我并没有仔细研究，因为我认为 <code>HashMap</code>  本身就是线程不安全的，无论是头插还是尾插，在并发环境下都不能使用 <code>HashMap</code> ，曾经也有人向社区报告 <code>bug</code>  说 1.7 的 <code>HashMap</code>  尾插会在并发环境下出现死循环，但是社区并没有管，而是回复 “ <code>HashMap</code>  本来就不是给你在并发环境用的，想要安全请使用 <code>ConcurrentHashMap</code> ”。</p>\n<blockquote>\n<p>4. <code>HashMap</code>  扩容机制了解过吗，为什么容量大小必须是 2 的指数</p>\n</blockquote>\n<p><code>HashMap</code>  的容量大小默认是 16，阿里巴巴开发规范插件提示在初始化 <code>HashMap</code>  时尽量指定容量大小（预计存储个数 / 负载因子 + 1），因为没有指定可能会导致多次扩容，这个涉及到重建 <code>Hash</code>  表，链表分拆的操作，比较耗时。如果构造函数传入的容量不是 2 的指数，那么会将容量设置为既是 2 的指数，又是大于传入参数的最小数。</p>\n<p>至于为什么是 2 的指数，涉及 <code>hash</code>  公式，也就是 <code>index = hash &amp; (len - 1)</code> ，这个公式其实就相当于用长度取模 <code>index = hash % len</code> ，只不过位运算快很多。扩容机制为两倍扩容，公式为 <code>newCap = 2 * oldCap</code> ，我们假设下标为 <code>i</code>  的那一部分，这个部分的链表的节点都满足 <code>hash / len = y ... i</code> ，也就是说，我们假设商为 <code>y</code> ，那么当商为奇数时（ <code>hash &amp; oldCap == 0</code> ），扩容后再使用 <code>hash</code>  公式得到的结果就是 <code>i+n</code> ，如果商为偶数的话，扩容后再 <code>hash</code>  的结果还是 <code>i</code> 。这就使得每次扩容，都将现有的链表拆为两部分，一部分留在当前下标，另一部分转移到扩容后的 <code>i+n</code>  处，这种机制使用扩容时逻辑简单，操作迅速，还使得数组中节点分布均匀，不会出现那种节点都分布在前半部分或者后半部分。</p>\n<blockquote>\n<p>5. 为什么重写 <code>equals</code>  方法的同时建议重写 <code>hashCode方法</code> ，能用 <code>HashMap</code>  举个例子吗？</p>\n</blockquote>\n<p>我们假设一个类为 <code>People</code> ，只有一个属性 <code>name</code> ，那么现实情况下，两个对象相等，要么他们内存地址相同，要么 <code>equals</code>  比较后相同，这里也就是 <code>name</code>  相同。如果不重写 <code>hashCode</code>  方法，也就是使用 <code>Object</code>  内置的方法。现在有两个内存地址不同的 <code>People</code>  对象， <code>name</code>  相同使得 <code>equals</code>  相同，但是 <code>hashCode</code>  却不相同，那么当这两个对象作为 <code>Key</code>  加入到 <code>HashMap</code>  时，我们希望的是后加入的对象会覆盖前面加入的，因为两个对象是相同的，但是因为 <code>hashCode</code>  不同，他们两个甚至连映射出来的数组下标都不同，是无法覆盖的。这也就导致 <code>HashMap</code>  中有两个相同的 <code>key</code> ，这明显不符合哈希表的定义。</p>\n<p>再来看 <code>HashMap</code>  的源码，为了实现覆盖操作，首先就要使得 <code>equals</code>  相同的对象 <code>hashCode</code>  也相同才一定能映射到同样的下标，然后顺着链表向下查找，如果 <code>HashCode</code>  相同，同时 <code>equals</code>  也相同就会覆盖，反之如果遍历了整个链表都没有这样的节点，就算做新增节点，尾插到链表中。</p>\n<blockquote>\n<p>6. 说说 <code>HashMap</code>  与 <code>HashTable</code>  的区别</p>\n</blockquote>\n<ul>\n<li>并发： <code>HashTable</code>  用于并发环境，通过在方法上加入 <code>synchronized</code>  关键字保证线程安全，但是并行度太低了，所以大多数都使用 <code>ConcurrentHashMap</code> 。而 <code>HashMap</code>  是线程不安全的。</li>\n<li>存储： <code>HashTable</code>  键值对不允许存储 <code>null</code> ，而 <code>HashMap</code>  却可以， <code>HashTable</code>  源码中会先判断 <code>value</code>  是不是 <code>null</code> ，从而抛出空指针异常，并且会直接调用 <code>key</code>  的 <code>hashCode</code>  方法，计算比较粗暴，而 <code>HashMap</code>  统一指定 <code>key==null</code>  时 <code>hashCode</code>  为 0，至于 <code>value</code>  是否为 <code>null</code>  完全不重要，因为插入删除过程都不会涉及到 <code>value</code> ，只会比较 <code>key</code> 。</li>\n<li>迭代： <code>HashTable</code>  有两个迭代器， <code>Enumeration</code>  使用的是安全失败机制（ <code>fail-safe</code> ），而 <code>Iterator</code>  是快速失败机制。 <code>HashMap</code>  使用的是快速失败机制。</li>\n</ul>\n<blockquote>\n<p>7. 刚才提到 <code>ConcurrentHashMap</code> ，你知道什么？</p>\n</blockquote>\n<p>参考链接：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9tcC53ZWl4aW4ucXEuY29tL3MvTXk0UF9CQlhEbkFHWDFnaDYzMFpLdw==\">https://mp.weixin.qq.com/s/My4P_BBXDnAGX1gh630ZKw</span></p>\n<p>这个也要分 <code>1.7</code>  和 <code>1.8</code>  两个版本：</p>\n<ul>\n<li><code>1.7</code>  使用的是数组桶 + 链表，分段，段数决定并行度。通俗来讲， <code>1.7</code>  版本维护了一个数组，数组中每个元素都是一个 <code>HashMap</code> ，上锁就是对每个段上锁。所以并行度并不高</li>\n<li><code>1.8</code>  使用的是数组桶 + 链表（升级红黑树），同时锁的粒度更小了，使用 <code>CAS+synchronized</code>  来实现并发安全。维护的数组和 <code>HashMap</code>  的数组是一致的，只不过每次上锁都是对要修改的下标单个元素进行上锁。由于 <code>Synchronized</code>  的性能采用锁升级的方式优化后， <code>ConcurrentHashMap</code>  的性能也随之上升。</li>\n</ul>\n<p>至于 <code>CAS</code>  操作，是对数组元素进行修改，也就是<strong>链表的表头</strong>。</p>\n<p>只要上了锁，保证了线程安全，其他的都和 <code>HashMap</code>  没有太多区别，也就有些细节不同，比如 <code>HashMap</code>  再加入一个键值对就要扩容了，它会先插入再扩容，而 <code>ConcurrenHashMap</code>  是先扩容再插入。</p>\n<blockquote>\n<p>8. 这么了解 <code>ConcurrentHashMap</code> ，你实际用过吗，或者你看源码有什么地方用过吗？</p>\n</blockquote>\n<p>我倒是还没有做过并发项目，所以也就是看别人用过，这里举一个比较偏的现实例子。在日志框架中，日志门面 <code>slf4j-api</code>  自带了一个实现，叫做 <code>slf4j-simple</code> ，它在实现 <code>LoggerFactory</code>  时就用到了 <code>ConcurrenMap</code> ，键值对是 <code>&lt;String,Logger&gt;</code> ，这里的 <code>String</code>  对应的就是 <code>name</code> 。当外界传给 <code>LoggerFactory</code>  一个 <code>name</code>  时，就会从 <code>ConcurrentHashMap</code>  中找，如果有就返回，没有就新建一个，缓存起来再返回。</p>\n<figure class=\"highlight java\"><figcaption data-lang=\"java\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token keyword\">public</span> <span class=\"token keyword\">class</span> <span class=\"token class-name\">SimpleLoggerFactory</span> <span class=\"token keyword\">implements</span> <span class=\"token class-name\">ILoggerFactory</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre>    <span class=\"token class-name\">ConcurrentMap</span><span class=\"token generics\"><span class=\"token punctuation\">&lt;</span><span class=\"token class-name\">String</span><span class=\"token punctuation\">,</span> <span class=\"token class-name\">Logger</span><span class=\"token punctuation\">></span></span> loggerMap <span class=\"token operator\">=</span> <span class=\"token keyword\">new</span> <span class=\"token class-name\">ConcurrentHashMap</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre></pre></td></tr><tr><td data-num=\"4\"></td><td><pre>    <span class=\"token keyword\">public</span> <span class=\"token class-name\">SimpleLoggerFactory</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre>        <span class=\"token class-name\">SimpleLogger</span><span class=\"token punctuation\">.</span><span class=\"token function\">lazyInit</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"6\"></td><td><pre>    <span class=\"token punctuation\">&#125;</span></pre></td></tr><tr><td data-num=\"7\"></td><td><pre></pre></td></tr><tr><td data-num=\"8\"></td><td><pre>    <span class=\"token keyword\">public</span> <span class=\"token class-name\">Logger</span> <span class=\"token function\">getLogger</span><span class=\"token punctuation\">(</span><span class=\"token class-name\">String</span> name<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"9\"></td><td><pre>        <span class=\"token class-name\">Logger</span> simpleLogger <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span><span class=\"token class-name\">Logger</span><span class=\"token punctuation\">)</span><span class=\"token keyword\">this</span><span class=\"token punctuation\">.</span>loggerMap<span class=\"token punctuation\">.</span><span class=\"token function\">get</span><span class=\"token punctuation\">(</span>name<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"10\"></td><td><pre>        <span class=\"token keyword\">if</span> <span class=\"token punctuation\">(</span>simpleLogger <span class=\"token operator\">!=</span> <span class=\"token keyword\">null</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"11\"></td><td><pre>            <span class=\"token keyword\">return</span> simpleLogger<span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"12\"></td><td><pre>        <span class=\"token punctuation\">&#125;</span> <span class=\"token keyword\">else</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"13\"></td><td><pre>            <span class=\"token class-name\">Logger</span> newInstance <span class=\"token operator\">=</span> <span class=\"token keyword\">new</span> <span class=\"token class-name\">SimpleLogger</span><span class=\"token punctuation\">(</span>name<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"14\"></td><td><pre>            <span class=\"token class-name\">Logger</span> oldInstance <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span><span class=\"token class-name\">Logger</span><span class=\"token punctuation\">)</span><span class=\"token keyword\">this</span><span class=\"token punctuation\">.</span>loggerMap<span class=\"token punctuation\">.</span><span class=\"token function\">putIfAbsent</span><span class=\"token punctuation\">(</span>name<span class=\"token punctuation\">,</span> newInstance<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"15\"></td><td><pre>            <span class=\"token keyword\">return</span> <span class=\"token punctuation\">(</span><span class=\"token class-name\">Logger</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>oldInstance <span class=\"token operator\">==</span> <span class=\"token keyword\">null</span> <span class=\"token operator\">?</span> newInstance <span class=\"token operator\">:</span> oldInstance<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"16\"></td><td><pre>        <span class=\"token punctuation\">&#125;</span></pre></td></tr><tr><td data-num=\"17\"></td><td><pre>    <span class=\"token punctuation\">&#125;</span></pre></td></tr><tr><td data-num=\"18\"></td><td><pre></pre></td></tr><tr><td data-num=\"19\"></td><td><pre>    <span class=\"token keyword\">void</span> <span class=\"token function\">reset</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"20\"></td><td><pre>        <span class=\"token keyword\">this</span><span class=\"token punctuation\">.</span>loggerMap<span class=\"token punctuation\">.</span><span class=\"token function\">clear</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"21\"></td><td><pre>    <span class=\"token punctuation\">&#125;</span></pre></td></tr><tr><td data-num=\"22\"></td><td><pre><span class=\"token punctuation\">&#125;</span></pre></td></tr></table></figure><p>其实日志中使用 <code>ConcurrentHashMap</code>  很正常，因为日志打印本身就经常处于并发环境中，有些甚至会专门分配线程去处理日志。有时类专门有一个 <code>Logger</code> ，其对应键值就是类的全类限定名，多个线程可能都会请求这个 <code>logger</code> ，自然就需要线程安全的容器来缓存。</p>\n",
            "tags": [
                "八股文"
            ]
        },
        {
            "id": "https://cecilia.cool/2023/02/27/%E5%85%AB%E8%82%A1%E6%96%87/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/",
            "url": "https://cecilia.cool/2023/02/27/%E5%85%AB%E8%82%A1%E6%96%87/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/",
            "title": "计算机网络",
            "date_published": "2023-02-27T09:09:36.000Z",
            "content_html": "<p>本文积累了作者在准备面试时学习的<strong>计算机网络</strong>的知识与问题，作为科班出身，408 这些科目的重要性不必多说，能直接检验出你作为科班选手的水准。</p>\n<p>本 tag 的文章的问题都是本人先学习后，凭记忆写下的，相当于二次复习（复盘）了，这样有助于加深印象。</p>\n<blockquote>\n<p>1. 键入网址后依次发生了什么？</p>\n</blockquote>\n<p>参考链接：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9tcC53ZWl4aW4ucXEuY29tL3MvSTZCTHdiSXBmR0VKbnhqRGNQWGMxQQ==\">https://mp.weixin.qq.com/s/I6BLwbIpfGEJnxjDcPXc1A</span></p>\n<ul>\n<li>浏览器解析 URL 生成 Http 请求。</li>\n<li><strong>发送 Http 请求前，会将域名解析为 IP 地址</strong>，会先查询浏览器缓存、系统缓存、本机 <code>hosts</code>  文件，如果没有，就会发送请求到本地域名服务器，通过本地域名服务器分别访问根域名服务器、顶级域名服务器、权威域名服务器，最终拿到域名映射的 IP 地址，并且缓存起来。</li>\n<li><strong>建立 TCP 连接，三次握手</strong>。三次握手是为了双方确定对方具有发送和接收数据的能力，所以两次握手不行</li>\n<li><strong>增加 TCP 头部</strong>。如果 Http 数据太长，超过了 MSS（网络层数据大小），就需要将其切分再每个加上 TCP 头部。TCP 头部包含了源端口，目的端口，校验和，序号等信息，最后交给 IP 模块处理。</li>\n<li><strong>加上 IP 头部</strong>。IP 头部包含了源地址和目的地址，如果主机有多个网卡，会根据路由表规则来选择网卡，其实就是目的地址与网卡的掩码做<strong>与运算</strong>从而来选择，如果都不匹配，就会走默认网卡 0.0.0.0。</li>\n<li><strong>加上 MAC 头部</strong>。通过 ARP 协议来获取目的地址的 MAC 地址，如果 ARP 缓存有，就直接用，如果没有，就在以太网中广播目的地址从而得到响应拿到对应的 MAC 地址。</li>\n<li><strong>将给网卡，将包转为电信号，通过网线发送出去</strong>。</li>\n<li><strong>交换机拿到包，通过 MAC 表将数据从对应端口发送出去</strong>，如果表中没有对应映射，就对局域网所有主机发送。</li>\n<li><strong>路由器拿到包并根据 IP 地址进行转发</strong>。MAC 头部作用就是将包送到路由器，然后 MAC 头部就会被丢弃。路由器根据路由表决定下一跳（下一跳的 IP）。通过 ARP 协议拿到下一跳的 MAC 地址，重新发送。整体传输过程只有 MAC 地址在不断变化，因为包需要不断在以太网中传输。</li>\n<li>服务器收到请求，回复 ACK 和 Http 响应。浏览器得到响应，再请求 html 中的 js，css 资源。浏览器再解析渲染，呈现网页。</li>\n</ul>\n<h1 id=\"http协议\"><a class=\"anchor\" href=\"#http协议\">#</a> Http 协议</h1>\n<blockquote>\n<p>1. 谈谈 http 协议</p>\n</blockquote>\n<p>参考链接：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9tcC53ZWl4aW4ucXEuY29tL3MvQUsxUGI5cngwcTVIZjhkcTZITk9odw==\">https://mp.weixin.qq.com/s/AK1Pb9rx0q5Hf8dq6HNOhw</span></p>\n<p>http 协议是超文本传输协议，一开始用于传输 html、css、js 等资源文件，后来也可以传输图片、视频、音频等。http1.0 是无状态的，每个 http 请求都必须重新建立 TCP 连接，这导致开销较大。http1.1 通过 <code>Cookie</code>  来管理状态，同时实现了持久化连接。</p>\n<p>http 请求由：请求行、消息头、数据组成。请求行包括请求方法、<strong>URL</strong>、协议版本。</p>\n<p>请求方法包括 GET（表单）、POST（实体）、DELETE（删除文件）、PUT（文件）、TRACE 等组成。</p>\n<p>状态码含义分别是：</p>\n<ul>\n<li>1xx：表示请求正在处理</li>\n<li>2xx：表示请求成功处理</li>\n<li>3xx：表示重定向</li>\n<li>4xx：表示客户端错误，请求不合法</li>\n<li>5xx：表示服务器错误，不能处理合法请求</li>\n</ul>\n<blockquote>\n<p>2. <code>GET</code>  与 <code>POST</code>  区别</p>\n</blockquote>\n<ul>\n<li><code>GET</code>  的语义是从服务器获取指定的资源，请求的参数写在 <code>URL</code>  中，浏览器一般对 <code>URL</code>  的长度有一定的限制。 <code>GET</code>  方法是 ** 安全、幂等（多次请求结果不变）** 的。</li>\n<li><code>POST</code>  的语义是根据报文 <code>body</code> <strong> 对指定的资源做出修改</strong>。 <code>POST</code>  请求携带的数据一般是写在报文 <code>body</code>  中。 <code>POST</code>  方法不是安全、幂等的。</li>\n</ul>\n<p>但是实际开发中，很多人并没有遵循 <code>RFC</code>  语义来实现 <code>GET</code>  和 <code>POST</code> 。</p>\n<blockquote>\n<p>3. <code>Http</code>  缓存技术</p>\n</blockquote>\n<p><code>http</code>  的缓存技术分为<strong>强制缓存</strong>和<strong>协商缓存</strong>：</p>\n<ul>\n<li><strong>强制缓存</strong>：浏览器判断缓存没有过期，就直接使用缓存，是通过响应头部的两个字段实现的。 <code>Cache_Controller</code>  是相对时间； <code>Expire</code>  是一个绝对时间，如果两个字段同时存在于头部，那么 <code>Cache_Controller</code>  的优先级更高。以 <code>Cache_Controller</code>  为例，浏览器第一次访问资源，服务器返回资源的同时在头部加上 <code>Cache_Contorller</code> 。</li>\n<li><strong>协商缓存</strong>：有时响应返回的状态码为 <code>304</code> ，其实就是服务端告知客户端是否可以使用缓存，这就是协商缓存。</li>\n</ul>\n<blockquote>\n<p>4. <code>Http</code>  各版本之间的特性</p>\n</blockquote>\n<ul>\n<li><code>http1.0</code> ：短连接， <code>TCP</code>  连接只处理一个请求，处理完就关闭连接。引入了 <code>POST</code>  和 <code>HEAD</code> 。</li>\n<li><code>http1.1</code> ：通过 <code>Cookie</code>  维护长连接，同时引入了 <code>PUT</code> 、 <code>DELETE</code> 、 <code>PATCH</code>  等动词。<strong>使用管道可以同时发送多个请求</strong>，减少整体响应时间（管道解决了<strong>请求</strong>的队头阻塞，但是没有解决响应的队头阻塞，而且管道这个功能默认其实是不开启的，而且浏览器基本没有支持）。</li>\n<li><code>http2.0</code> ：针对 <code>http</code>  报文做出优化，如压缩首部，采用二进制报文格式，对计算机更友好。因为 <code>http1.1</code>  默认不开启管道，浏览器也不支持， <code>http2.0</code>  引入了 <code>Stream</code>  概念，多个 <code>Stream</code>  可以复用一个 <code>TCP</code>  连接。不同的 <code>http</code>  请求可以使用唯一的 <code>Stream ID</code>  来区分，接收端通过 <code>Stream ID</code>  组装 <code>http</code>  消息。</li>\n<li><code>http3.0</code> ：使用了基于 <code>UDP</code>  实现的 <code>QUIC</code>  协议， <code>http2.0</code>  的缺点在于如果某一个 <code>Stream</code>  在 <code>tcp</code>  层有数据丢失，那么所有排在该 <code>Stream</code>  后面的数据都要等待重传，应用层无法获取。 <code>http3.0</code>  的每一个 <code>Stream</code>  并不互相依赖，某一个 <code>Stream</code>  数据丢失之后阻塞自己这个 <code>Stream</code> 。其他特性还有更快的建立连接， <code>QUIC</code>  包含了 <code>TLS</code> ，只需要三次握手即可建立连接与密钥协商，而 <code>TCP</code>  三次握手之后还需要额外的 <code>TSL</code>  握手。由于 <code>QUIC</code>  只是维护连接 <code>ID</code> ，所以哪怕连接的主机 IP 变了（wifi 变流量），只要连接 ID 和上下文信息还在就不需要重新建立连接（ <code>TCP</code>  的唯一标识是四元组，这种情况需要断开原来的连接，建立新的连接）。</li>\n</ul>\n<blockquote>\n<p>5. 说说如何优化 <code>http1.1</code>  协议？</p>\n</blockquote>\n<p>优化 <code>http</code>  协议的思路：</p>\n<ul>\n<li>减少 <code>http</code>  请求次数：\n<ul>\n<li>使用缓存，将请求和响应的数据保存到本地磁盘，并设置过期时间。</li>\n<li>减少重定向次数，一般客户端和浏览器之间还会设置代理服务器，如果有资源的位置变了需要重定向，重定向的工作交给代理服务器做就可以减少 <code>http</code>  请求。</li>\n<li>合并请求和懒加载</li>\n</ul>\n</li>\n<li>减少 <code>http</code>  响应数据的大小：对数据使用无损压缩（谷歌的 br 压缩）或者有损压缩（谷歌的 <code>WebP</code>  格式）。</li>\n</ul>\n<blockquote>\n<p>3. 谈谈 https 协议</p>\n</blockquote>\n<p>参考链接：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9tcC53ZWl4aW4ucXEuY29tL3MvMjFKYVh3ZGZTakl0ajVTZ093aGFwZw==\">https://mp.weixin.qq.com/s/21JaXwdfSjItj5SgOwhapg</span></p>\n<p>数据传输使用的仍然是 http 协议，只不过使用了 SSL 对数据进行了加密，保证了数据传输的安全。</p>\n<p>其过程为：</p>\n<ul>\n<li>客户端发送 https 请求，服务端响应<strong> CA 证书和公钥</strong>。（需要注意，是<strong>证书里面附带了公钥</strong>）</li>\n<li>客户端校验 CA 证书合法性，生成随机密钥 key，并使用<strong>公钥</strong>对 key 加密，再发送给服务端。</li>\n<li>服务端收到后，使用私钥对可 key 解密，拿到真正的 key，双方之后的数据传输就用该 key 进行<strong>对称加密传输</strong>。</li>\n</ul>\n<p>上述过程，如果没有 CA 证书，那么中间人攻击可以这样：在第一步将公钥换成自己的公钥 1，这样在第二步对客户端产生的 key 使用自己的私钥 1 解密从而拿到 key。最后再将 key 使用最开始服务端发送的公钥进行加密发送给服务端，这样 MITM（中间人攻击）照样可以完成。</p>\n<p>所以需要第三方的公信认证，CA 机构有一对公钥 / 私钥，公钥是对外界公开的，而私钥必须严格保密。当服务端将<strong> CA 证书 + 服务端公钥</strong>发送给客户端时， 会先将证书的数据（包括服务端公钥）进行哈希，得到哈希值<strong> H</strong>，再用私钥将 H 加密，最后客户端（浏览器）拿到证书后，会使用系统 / 浏览器内置的 CA 公钥对 H 进行解密得到 H，再自己通过证书指定的哈希算法对数据进行哈希得到<strong> H'</strong>，比较<strong> H==H'</strong>。</p>\n<p>私钥和公钥可以互相加密解密，非不是私钥只能解密。上述过程，假设有中间人换了证书中的服务端公钥，也会因为不知道私钥而无法加密哈希值导致客户端会检测出来。所以 CA 机构应该严格保密私钥，如果泄露，就会失去公信力。</p>\n<p>如果整个 https 通信全部用非对称加密确实可以，双方各拿一对公钥 / 私钥，然后交换公钥进行通信。至于为什么本质还是要使用对称加密，是因为非对称加密太耗时间了，仅用于传输 key 即可。</p>\n<h1 id=\"icmp\"><a class=\"anchor\" href=\"#icmp\">#</a> ICMP</h1>\n<blockquote>\n<p>4. 用过 ping 吗，说说原理</p>\n</blockquote>\n<p>参考链接：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9tcC53ZWl4aW4ucXEuY29tL3MvM0tGMEl4THVtOEVPdGNGMFpOSWlQQQ==\">https://mp.weixin.qq.com/s/3KF0IxLum8EOtcF0ZNIiPA</span></p>\n<p><code>ping</code>  实现原理依托于 <code>ICMP</code>  协议。该协议是互联网控制报文协议，用于报告网络错误、传送报文运输情况等。 <code>ICMP</code>  报文被封装到 <code>IP</code>  数据包里面。ping 使用的两种 <code>ICMP</code>  数据包是<strong>回送请求</strong>和<strong>回送响应</strong>。</p>\n<p>假设主机 A 对主机 B 进行了 ping 操作，那么主机 A 会封装 ** <code>ICMP</code>  回送请求 **，此时会记录请求产生的时间，并将其封装到 IP 数据包中，再加上 MAC 头部，最后发送出去。没有缓存目的 MAC 地址，先通过 ARP 协议获取。</p>\n<p>主机 B 收到报文后，逐步拆除 MAC 和 IP 头部，经过地址检验后，将有用的信息提取交给 ICMP 协议，再发送 ** <code>ICMP</code>  回送响应 **。主机 A 收到回送响应后，用当前时间减去 <code>ICMP</code>  数据包发送时间，就可以得到 <code>RTT</code> 。</p>\n<p>同时， <code>ICMP</code>  还维护了一个 <code>TTL</code> ，每次数据包经过一个路由器，就会 - 1，直到为 0 被丢弃，TTL 就可以检测出两个主机之间经过多少跳。</p>\n<p><code>tracert</code>  也是借助 <code>ICMP</code>  协议实现的。</p>\n<h1 id=\"tcpip\"><a class=\"anchor\" href=\"#tcpip\">#</a> TCP/IP</h1>\n<blockquote>\n<p>5. 聊聊 TCP 三次握手和四次挥手</p>\n</blockquote>\n<p>参考链接：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9tcC53ZWl4aW4ucXEuY29tL3MvclgzQV9GQTE5bjRwSTlIaWNJRXNYZw==\">https://mp.weixin.qq.com/s/rX3A_FA19n4pI9HicIEsXg</span></p>\n<p>TCP<strong> 三次握手</strong>：</p>\n<ul>\n<li>\n<p>确认双方都有发送和接收数据的能力。</p>\n</li>\n<li>\n<p>防止旧连接覆盖新连接。客户端知道自己此时应该建立哪个连接，但是网络传输过程复杂，很可能旧的连接 SYN 比新的 SYN 后到，到底建立哪个连接服务端是不知道的，所以必须有第三次握手，让客户端确认到底建立哪个连接。</p>\n</li>\n<li>\n<p>防止浪费资源。两次握手中，服务端不知道自己的 <code>ACK+SYN</code>  是否被客户端收到，这会导致重复发送 <code>SYN+ACK</code> ，建立很多个无用的连接。</p>\n</li>\n<li>\n<p>同步初始化序列号。同步序列号能够防止接收端接收的数据乱序。</p>\n</li>\n</ul>\n<p>总的来说，第三次握手是必要的，<strong>必须由客户端确认建立连接的各种状态信息的正确性</strong>。值得一提的是，第三次握手，发送方可以顺带发送数据。</p>\n<p>TCP<strong> 四次挥手</strong>：</p>\n<p>重点：主动放弃连接的一方会进入 <code>Time_Wait</code>  状态，在 Linux 中，会等待 2MSL（60 秒）。</p>\n<p>四次挥手的过程为（假设客户端主动断开连接）：</p>\n<ol>\n<li>客户端发送 <code>Fin</code>  表示自己断开连接，不再发送数据，但是可以接收数据，进入 <code>FIN_WAIT_1</code>  状态。</li>\n<li>服务端收到 <code>Fin</code> ，发送 <code>ACK</code> ，表示自己收到断开请求，需要处理剩下的数据，进入 <code>CLOSED_WAIT</code> 。</li>\n<li>服务端处理完数据，发送 <code>Fin</code> ，进入 <code>LAST_ACK</code>  状态。</li>\n<li>客户端收到 <code>Fin</code> ，发送 <code>ACK</code> ，进入 <code>TIME_WAIT</code>  状态，等待 <code>2MSL</code> ，最后进入 <code>CLOSED</code>  状态。</li>\n</ol>\n<p>正是因为服务端需要处理剩下数据，所以是四次挥手，同样，如果省去最后一次挥手，那么服务端就会一直处于 <code>LAST_ACK</code>  状态，当客户端想建立新的连接，发送 <code>SYN</code> ，服务端就会回复 <code>RST</code> ，建立连接的过程会终止。</p>\n<blockquote>\n<p>6. 为什么四次挥手中要有 <code>TIME_WAIT</code>  状态以及为什么要等 2MSL？</p>\n</blockquote>\n<p>参考链接：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9tcC53ZWl4aW4ucXEuY29tL3MvclgzQV9GQTE5bjRwSTlIaWNJRXNYZw==\">https://mp.weixin.qq.com/s/rX3A_FA19n4pI9HicIEsXg</span></p>\n<p>我们这里默认主动断开连接的是客户端。首先，主动断开连接的那一方才会进入 <code>TIME_WAIT</code> ，这个时间是 2MSL，在 Linux 中为 60s，而且这个时间是固定的，也就是在内核代码中写死了，无法修改。</p>\n<p><code>TIME_WAIT</code>  的出现能够保证被动断开连接方（服务端）可以正常的关闭，从 <code>LAST_ACK</code>  进入 <code>CLOSED</code>  状态。</p>\n<p>MSL 是数据包在网络传输中存活的最长时间， <code>TIME_WAIT</code>  设置为 2MSL，比较合理的解释为：如果服务端没有没有 ACK，超时重传 <code>FIN</code>  后再接收 <code>ACK</code>  的时间在 2MSL 之内。<strong>当客户端重新接收到 <code>FIN</code>  时，会重置 2MSL 时间</strong>。同时网络连接中的旧数据包在 2MSL 中能够被清理干净，如果客户端当前端口重新建立连接，不会有旧的数据传到当前端口，造成数据混乱。</p>\n<p><code>TIME_WAIT</code>  出现的原因为：</p>\n<ul>\n<li>保证服务端正常关闭</li>\n<li>防止旧的四元组数据包影响下一次连接传输。</li>\n</ul>\n<p>正因为主动断开会进入 <code>TIME_WAIT</code> ，此时既会白白占用端口，又会无法传输数据，经历时间还非常长，对于服务端来说是很大的负担，所以这个烂摊子尽量交给对方，尽量让对方断开连接。</p>\n<p>解决 <code>TIME_WAIT</code>  方法：</p>\n<ul>\n<li>使用 <code>tcp_rw_reuse</code> + <code>tcp_timestamp</code> ：这样可以使得处于 <code>TIME_WAIT</code>  套接字复用，因为开启了时间戳，新的连接不会接收时间戳过期的数据。</li>\n<li>其他方法不推荐使用。</li>\n</ul>\n<blockquote>\n<p>7. 知道 SYN 攻击吗，说说你知道的防御手段</p>\n</blockquote>\n<p>参考链接：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9pbmZvLnN1cHBvcnQuaHVhd2VpLmNvbS9pbmZvLWZpbmRlci9lbmN5Y2xvcGVkaWEvemgvU1lOK0Zsb29kLmh0bWw=\">https://info.support.huawei.com/info-finder/encyclopedia/zh/SYN+Flood.html</span></p>\n<p>SYN 攻击是 DDos 攻击的一种，通过程序不断发送 SYN 迅速占满服务端的 SYN 队列，使其崩溃的攻击手段。</p>\n<p>防御手段：</p>\n<ul>\n<li>\n<p>首包丢弃：大多数 SYN 攻击都是变源的，这使得在 SYN Flood 攻击中，每个 SYN 都是首包，Anti-DDos 系统可以丢弃收到的 SYN 首包，如果对方客户端是正常的，那么基于 TCP 超时机制，一定会重传，此时 SYN 就不是首包了，可以对其进行源认证。</p>\n</li>\n<li>\n<p>源认证：Anti-DDos 系统部署在网络入口，先代替服务端发送 SYN+ACK，如果收到了客户端的 ACK，就将其 IP 加入白名单，之后一段时间都不会代替服务端对该 IP 的 SYN 进行拦截。</p>\n</li>\n</ul>\n<p>源认证必须配合首包丢弃使用，不然性能瓶颈也只是从服务器转移到了 Anti-DDos 系统中。</p>\n<ul>\n<li>设置 TCP 参数也可以一定程度上防御 SYN 攻击，比如扩大半连接队列，开启 <code>syncookies</code> 。</li>\n</ul>\n<blockquote>\n<p>8.TCP 的半连接队列和全连接队列了解吗？如果队列满了怎么办？</p>\n</blockquote>\n<p>参考链接：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9tcC53ZWl4aW4ucXEuY29tL3MvdFJYbHExaEVycUtRTE1NTGN4b1h2Zw==\">https://mp.weixin.qq.com/s/tRXlq1hErqKQLMMLcxoXvg</span></p>\n<p>半连接队列是指 SYN 队列，服务端收到 SYN 请求，就会将其加入到 SYN 队列；全连接队列是指 Accept 队列，当服务端收到客户端的 <code>ACK</code>  就会将 SYN 队列对应节点放到 Accept 队列中。当队列满了，Linux 默认的操作是拒绝再接收 ACK。因为队列装不下了，但是有个问题就是，客户端发送了 ACK 就会进入 <code>ESTABLISHED</code>  状态，但是实际上服务端却没有接收。</p>\n<p>Linux 中变量 <code>tcp_abort_on_overflow</code>  为 0，就是丢掉客户端发送的数据，为 1 就会发送一个 <code>reset</code>  包给客户端。</p>\n<p><strong>所以全连接队列满了，一般解决方法就是扩大队列长度，Accept 队列长度由两个变量决定，结果式为 <code>len = min(backlog, somaxconn)</code> 。</strong></p>\n<p>半连接队列长度 <code>max_qlen_log</code>  取决于全连接队列长度 <code>len</code> 、变量 <code>max_syn_backlog</code> ： <code>max_qlen_log = 2 * min(len, max_syn_backlog)</code> 。</p>\n<p>半连接队列一般不会满，当队列中剩余长度达到某个特定值时（和 <code>max_syn_backlog</code>  有关，但是不同 Linux 版本计算方法可能不同），就不会再接收 <code>SYN</code>  了。其实当全连接队列满了，不论半连接队列如何，都不会再接收 <code>SYN</code>  了。</p>\n<p>半连接队列满了（假设遇到了 <code>SYN</code>  攻击），策略有三个：</p>\n<ul>\n<li>增大半连接队列长度，也就是增大那三个参数。</li>\n<li>打开 <code>syncookies</code> ，将该变量设置为 1 即可（0-- 关闭，1-- 队列满了打开 <code>syncookies</code> ，2-- 直接打开 <code>syncookies</code> ）。开启该功能后，不会再丢弃 <code>SYN</code>  包，而是服务器根据当前状态计算出一个值，放在 <code>SYN+ACK</code>  中发出，当客户端返回 <code>ACK</code>  报文时，取出该值校验合法性，建立连接。</li>\n<li>减少 <code>SYN+ACK</code>  重发次数，使得处于 <code>SYN_REVC</code>  状态的连接尽快断开。</li>\n</ul>\n<blockquote>\n<p>9. 谈谈 TCP 相关的参数</p>\n</blockquote>\n<p>参考链接：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9tcC53ZWl4aW4ucXEuY29tL3MveXRWN1JaU3lGWHl2UFdfbEtodjhodw==\">https://mp.weixin.qq.com/s/ytV7RZSyFXyvPW_lKhv8hw</span></p>\n<p>这里讲的 TCP 参数与 TCP 三次握手和四次挥手优化有关。</p>\n<p>三次握手优化角度：</p>\n<ul>\n<li>客户端（发送方）：客户端行为有发送 <code>SYN</code>  和 <code>ACK</code> ，以及重发 <code>SYN</code>  和 <code>ACK</code> 。\n<ul>\n<li><code>tcp_syn_retries</code>  参数：控制重传 <code>SYN</code>  次数，每次超时时间为上次 2 倍，初始为 1s。<strong>超过次数就会断开连接</strong>。</li>\n</ul>\n</li>\n<li>服务端（接收方）：服务端行为较复杂，涉及到半连接队列和全连接队列的大小以及拒绝策略\n<ul>\n<li>重发 <code>FIN+ACK</code>  次数：由 <code>tcp_synack_retires</code>  决定</li>\n<li>半连接队列：大小由 <code>tcp_max_syn</code> 、 <code>backlog</code> 、 <code>somaxconn</code>  共同决定。可以通过增大这三个参数来增大半连接队列。同时 <code>syncookies</code>  参数控制当半连接队列满了时，生成状态值校验来避免放到半连接队列中。</li>\n<li>全连接队列：大小由 <code>backlog</code>  和 <code>somaxconn</code>  共同决定。拒绝策略由 <code>tcp_abort_on_overflow</code>  决定，0 表示丢弃 <code>ACK</code> ，不让其进入全连接队列，一般用这个，还可以解决短暂的突发网络繁忙。1 表示发送 <code>RST</code>  包使其断开连接。</li>\n</ul>\n</li>\n<li>绕过三次握手：Linux 内核 3.1 版本后，出现了 <code>Fast Open</code>  机制，通过 <code>Cookie</code>  来绕过后面的三次握手。第一次正常三次握手，但是服务端可以在第二次握手时创建 <code>Cookie</code>  并发送给客户端。之后就可以重用该 <code>TCP</code>  连接，而不需要重复建立 TCP 连接。因为后续数据发送可以携带 <code>Cookie</code> ，服务端只需要验证 <code>Cookie</code>  即可。这种的缺点就是，如果重发，还需要重发 <code>Cookie</code> 。该机制使用 <code>tcp_fastopn</code> ：\n<ul>\n<li>0 ——  <code>close</code></li>\n<li>1 ——  <code>Client</code>  打开</li>\n<li>2 ——  <code>Server</code>  打开</li>\n<li>3 —— 双方都打开</li>\n</ul>\n</li>\n</ul>\n<p>四次挥手优化角度：</p>\n<ul>\n<li>主动断开方：会进入 <code>TIME_WAIT</code>  状态，接收发送 <code>FIN</code>  和 <code>ACK</code> 。\n<ul>\n<li><code>tcp_max_orphan</code>  参数：调用 <code>close</code>  函数后，连接就变成了<strong>孤儿连接</strong>，该参数限制了最大孤儿连接数量，超过直接发送 <code>RST</code>  包断开连接。</li>\n<li><code>FIN_WAIT1</code>  状态优化： <code>tcp_orphan_retries</code>  参数 —— 表示处于 <code>FIN_WAIT1</code>  状态的 <code>FIN</code>  重传次数，超过直接关掉连接。</li>\n<li><code>FIN_WAIT2</code>  状态优化： <code>tcp_fin_timeout</code>  参数：表示孤儿连接等待 <code>FIN</code>  的最长时间，默认 60s。</li>\n<li><code>TIME_WAIT</code>  状态优化：\n<ul>\n<li><code>tcp_max_tw_buckets</code>  参数：如果处于 <code>TIME_WAIT</code>  连接超过该参数，之后的连接不再进入该状态。</li>\n<li><code>tcp_tw_reuse</code>  参数：开启后可以复用处于 <code>TIME_WAIT</code>  状态的连接，需要配合时间戳使用。</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>被动断开方：\n<ul>\n<li>还是借助 <code>tcp_orphan_retires</code>  参数限定 <code>FIN</code>  重传次数。</li>\n</ul>\n</li>\n</ul>\n<p>小结：</p>\n<ul>\n<li>三次握手参数： <code>tcp_syn_retries</code> 、 <code>somaxconn</code> 、 <code>backlog</code> 、 <code>tcp_max_syn</code> 、 <code>syncookies</code> 、 <code>tcp_abort_on_overflow</code> 、 <code>tcp_fastopn</code> 。</li>\n<li>四次挥手参数： <code>tcp_max_orphan</code> 、 <code>tcp_orphan_retries</code> 、 <code>tcp_fin_timeout</code> 、 <code>tcp_max_tw_buckets</code> 、 <code>tcp_tw_reuse</code> 。</li>\n</ul>\n<blockquote>\n<p>10. 聊聊 TCP 的可靠传输机制，比如重传、拥塞、流量控制等</p>\n</blockquote>\n<ul>\n<li>\n<p><strong>重传机制</strong>： 接收方回复 ACK 用于提醒发送方应该发那个数据包，当出现数据包丢失，接收方需要重传，分为超时重传和快速重传</p>\n<ul>\n<li>\n<p>超时重传：接收方拿不到 3 这个数据包，就不发 3 的 ACK，发送方等待 3 这个 ACK 超时，再重传，一种是只重传 3（节省带宽，慢），另一种是 3，4，5（快，浪费带宽）等都重传。</p>\n</li>\n<li>\n<p>快速重传：发送方连续三次接收到同一个 ACK，则重传对应的数据报。</p>\n</li>\n</ul>\n<p>其实重传都面临一个选择：只重传这一个还是重传后边所有数据报。这就引出<strong> SACK</strong> 机制，接收方回复 SACK，SACK 会汇报收到的数据碎片，这个协议需要两边都支持。但是 SACK 并不能替代 ACK，<strong>接收方有权把已经报给发送端 SACK 里的数据给丢了</strong>。</p>\n<p><strong>SACK</strong> 有一个严重的问题，Linux 代码中，使用一个 <code>sk_buff</code>  的数据结构，简称 <code>SBK</code> ，用于存储<strong>发送、接收</strong>队列等，还有一个结构体为 <code>skb_cb</code>  用于控制缓存，记录各种<strong> TCP packet</strong> 的各种信息，如小报文的数量 <code>tcp_gso_segs</code> ，无符号两字节，最多表示 64K，SKB 会将小报文段分片累积成大报文段再发送，但是内部最多维护 17 个分片队列，每个队列最大 32KB，如果有恶意攻击者将 <code>mss</code>  设置为 8，则每个小报文段大小为 8B。<strong>SACK</strong> 机制会将许多 <code>SKB</code>  合并填满一个 <code>SKB</code> ，那么就可能出现： <code>17 * 32 * 1024 / 8 &gt; 64K</code>  导致 <code>tcp_gso_segs</code>  溢出，进入 <code>BUG_ON</code>  函数使得服务器拒绝远程连接。</p>\n</li>\n<li>\n<p><strong>滑动窗口</strong>：发送方和接收方都有窗口，接收方的滑动窗口可以使发送方根据接收方的接收能力来发送数据。确认机制为<strong>累计确认 / 累计应答</strong>，假设收到序列号为 100 的 ACK，说明 100 以前的数据都收到了。</p>\n<p>如果接收方的窗口为 0 了，也会将发送方的窗口设为 0，此时不再发送数据，直到接收方窗口恢复，此时发送一个通知消息给发送方，并等待数据。如果这个通知消息因为网络拥塞丢失了，就会导致：接收方一直等待数据，发送方一直等待通知的死锁状况。所以一旦发送方窗口被置为 0，就会每隔一段时间发送探测报文，询问接收方窗口大小。</p>\n<p><strong>Silly Window Syndrome</strong> 是一种现象，会对小的 window size 做出响应，为了避免对小的 window size 做出响应，直到有足够大的 window size 再响应，如果窗口太小，发送出去的数据甚至没有 <code>MSS</code>  高，就会先累积再发送。</p>\n</li>\n<li>\n<p><strong>拥塞处理</strong>：名词： <code>ssthresh</code>  是慢启动阈值， <code>cwnd</code>  为拥塞窗口大小。</p>\n<p>三个状态，分别是慢启动，拥塞避免和快速恢复。</p>\n<ul>\n<li><strong>慢启动</strong>： <code>cwnd</code> （拥塞窗口）一开始为 <code>1MSS</code> ，每过 1 个 <code>RTT</code>  就二倍上升（本质是每收到一个 <code>ACK</code>  就 <code>cwmd++</code> ），如果<strong>超时</strong>， <code>ssthresh=cwnd/2</code> ，并且 <code>cwnd=1</code>  重新慢启动。如果之后 <code>cwnd &gt;=  ssthresh</code>  就进入<strong>拥塞避免</strong>。如果触发快速重传，就进入<strong>快速恢复</strong>。</li>\n<li><strong>拥塞避免</strong>：每一个 <code>RTT</code>  就 <code>cwnd++</code> ，如果超时，设置 <code>ssthresh=cwnd/2, cwnd = 1</code> ，进入慢启动。如果连续三次收到同一个 <code>Ack</code> ，设置参数为 <code>ssthresh=cwnd/2, cwnd = ssthresh + 3</code> 。进入<strong>快速恢复</strong>。</li>\n<li><strong>快速恢复</strong>：如果超时，同样操作，进入慢启动；每次收到一个冗余 <code>ACK</code> ， <code>cwnd++</code> ，如果收到新 <code>ACK</code> ，进入<strong>拥塞避免</strong>。</li>\n</ul>\n<p><strong>这里的 <code>RTT</code>  是指一个窗口的数据全部发送出去，又全部收到 <code>ACK</code>  的时间，而不是某一个报文的往返时间</strong>。</p>\n</li>\n</ul>\n<blockquote>\n<p>11.DNS 劫持和 DNS 污染</p>\n</blockquote>\n<p><strong>DNS 劫持</strong>：劫持了 DNS 服务器，通过某些手段取得某域名的解析记录控制权，进而修改此域名的解析结果，导致对该域名的访问由原 IP 地址转入到修改后的指定 IP。</p>\n<p><strong>DNS 污染</strong>：通过对 UDP 端口 53 上的 DNS 查询进行入侵检测，一经发现与关键词相匹配的请求则立即伪装成目标域名的解析服务器（NS，Name Server）给查询者返回虚假结果。很难靠个人设置解决，使用 VPN 是一个方法</p>\n<blockquote>\n<p>12. 聊聊 IP 协议，它和 MAC 地址有什么区别，IPV4 和 IPV6 呢？</p>\n</blockquote>\n<p>IP 协议用于唯一标识网络设备，属于网络层协议，传输层将数据包传到网络层后，会为数据加上 IP 首部。 <code>MAC</code>  属于链路层，用于标识下一跳的网络设备的物理地址，数据从源主机到目的主机的过程中， <code>MAC</code>  首部每经过一个路由器都会变换，而 IP 地址不会变换。</p>\n<p><code>IPv4</code>  地址由 32 位组成，以前会根据前几位将其分为 <code>ABCDE</code>  类地址，但是分类地址的局限性太多，比如 C 类 IP 数量太少，而 A 类 IP 数量有太多，所以采用了无分类 IP 地址，通过子网掩码和 IP 地址做 ** <code>&amp;</code>  运算<strong>来确定</strong>网络号、子网号 **。在路由控制中，目的地址与路由表中的子网掩码运算并比较网络号，从而进行路由转发。</p>\n<p>IP 协议因为不能重组分片数据，所以分片会导致严重的性能损耗，一个分片丢失了，就要重发整个 IP 数据报，所以通过引入 <code>MSS</code>  将分片操作交给 <code>TCP</code>  处理。</p>\n<p>IPv6 相对于 IPv4 的改进：</p>\n<ul>\n<li>取消了首部校验和字段：因为数据链路层和传输层都会校验</li>\n<li>取消分片 / 重组相关字段，这种操作只允许源 / 目标主机。</li>\n<li>使用了 128 位，16 进制，极大扩充了 IP 数量</li>\n</ul>\n",
            "tags": [
                "八股文"
            ]
        }
    ]
}