{
    "version": "https://jsonfeed.org/version/1",
    "title": "慕青の迷途 • All posts by \"八股文\" tag",
    "description": "时雨病重症患者",
    "home_page_url": "https://cecilia.cool",
    "items": [
        {
            "id": "https://cecilia.cool/2024/03/07/%E5%85%AB%E8%82%A1%E6%96%87/%E9%9D%A2%E7%BB%8F/",
            "url": "https://cecilia.cool/2024/03/07/%E5%85%AB%E8%82%A1%E6%96%87/%E9%9D%A2%E7%BB%8F/",
            "title": "面经",
            "date_published": "2024-03-07T02:14:42.000Z",
            "content_html": "<p>本文是我在网上搜集、朋友面试、自己面试的总结，是一个持续更新的系列，所以应该会很长</p>\n<h1 id=\"敖丙-读者面试\"><a class=\"anchor\" href=\"#敖丙-读者面试\">#</a> 敖丙 - 读者面试</h1>\n<p>资料来源：<span class=\"exturl\" data-url=\"aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL0JWMUNFNDExRTdTRy8/dmRfc291cmNlPTVhY2Y1YTdiMjNkMjhlNzYzM2U1YTliMzgxYzU3YzQy\">https://www.bilibili.com/video/BV1CE411E7SG/?vd_source=5acf5a7b23d28e7633e5a9b381c57c42</span></p>\n<p>问了好多问题，因为不是真实面试场景，所以一遇到不会的问题就换技术栈，所以涉及比较广，我选择有代表性的问题、我也不会的问题写上去。</p>\n<blockquote>\n<p>Q1： <code>SpringBean</code>  注入过程</p>\n</blockquote>\n<blockquote>\n<p>Q2：聊聊 Java8 新特性，了解过更高版本的新特性吗？</p>\n</blockquote>\n<blockquote>\n<p>Q3：线上 CPU100% 如何排查？</p>\n</blockquote>\n<blockquote>\n<p>Q4：InnoDB 为什么使用 B + 树而不用二叉树或者 B 树？</p>\n</blockquote>\n<blockquote>\n<p>Q5：MyISAM 和 InnoDB 的区别</p>\n</blockquote>\n<blockquote>\n<p>Q6：一张表最多能建多少个索引？</p>\n</blockquote>\n<blockquote>\n<p>Q7：Redis 如何保证数据高可用？</p>\n</blockquote>\n<p><code>Redis</code>  为了保证数据高可用，引入了持久化机制， 在早期版本，还有 VM，后来版本不推荐了。现在一般都是使用 <code>RDB</code> 、 <code>AOF</code>  或者混合持久化。</p>\n<ul>\n<li>\n<p><code>RDB</code>  通过对内存数据拍摄快照来持久化数据，触发机制是在一定时间内发生一定次数的修改操作。当然也可以使用 <code>save/bgsave</code>  主动拍摄快照，前者会阻塞线程，后者才会 <code>fork</code>  一个子线程进行快照拍摄。因为采用了压缩算法，实际占用空间很小。异步存储为了保证数据一致性，借助了操作系统的 <code>Copy on Write</code>  机制，主线程修改哪个页，就会先将这个页复制出来，在复制页进行修改。等快照拍摄结束，再将复制的页合并到原始内存中。</p>\n</li>\n<li>\n<p><code>AOF</code>  通过存储执行的命令到磁盘中保证数据的持久性，可以配置多种存储方式，比如执行一条命令就存储一条，或者每秒存储一次，或者看系统心情，什么时候有空什么时候就将缓冲区的命令存进去。 <code>AOF</code>  机制执行久了，就会导致文件保存了很多无效的命令，所以需要重写 <code>AOF</code>  文件 —— <code>bgrewriteaof</code> ，过程为：子线程遍历 <code>Redis</code>  内存生成一系列指令，然后将这些指令序列化到临时文件中，过程中的增量命令会追加到临时文件中，最后替换 <code>AOF</code>  文件。这里需要重点说一下，我们将数据写入到文件中时，其实是先写入到内核缓冲区，再到磁盘缓冲区，最后到磁盘，最后一个阶段我们是无法介入的，但是可以调用 <code>fsync()</code>  强制将数据刷新到磁盘缓冲区。 <code>redis</code>  默认是每秒调用一次。（有参数控制何时重写，比如文件大小超过多少，增量达到多少）</p>\n</li>\n<li>\n<p><code>混合持久化</code> ：4.0 版本后还出现了混合持久化，该机制必须打开 <code>AOF</code> ，隔一段时间拍摄快照，生成 <code>rdb</code>  数据，两次快照之间的记录使用 <code>AOF</code>  日志来记录，并追加到 <code>rdb</code>  数据后面。恢复数据时，先回复 <code>rbd</code>  数据，再执行 <code>AOF</code>  日志。这种机制既解决了 <code>rdb</code>  快照摄时突然断电导致整个快照丢失（因为还在临时文件中），也解决了 <code>AOF</code>  文件太大，不断重写的性能消耗。</p>\n</li>\n</ul>\n<p>参考链接：</p>\n<ul>\n<li><span class=\"exturl\" data-url=\"aHR0cHM6Ly9wZGFpLnRlY2gvbWQvZGIvbm9zcWwtcmVkaXMvZGItcmVkaXMteC1yZGItYW9mLmh0bWwjcmRiJUU1JTkyJThDYW9mJUU2JUI3JUI3JUU1JTkwJTg4JUU2JTk2JUI5JUU1JUJDJThGLTQtMCVFNyU4OSU4OCVFNiU5QyVBQw==\">Redis 进阶 - 持久化</span></li>\n<li><span class=\"exturl\" data-url=\"aHR0cHM6Ly9tcC53ZWl4aW4ucXEuY29tL3MvT19xRGNvNi1EYXN1M1JvbVdJS19JZw==\">https://mp.weixin.qq.com/s/O_qDco6-Dasu3RomWIK_Ig</span></li>\n</ul>\n<blockquote>\n<p>Q8：Redis 雪崩、穿透和击穿是什么？</p>\n</blockquote>\n<ul>\n<li><strong>缓存雪崩</strong>：是因为大量的 <code>Key</code>  同时过期，然后又来了海量的请求，导致全部打到后端数据库，造成数据库压力甚至宕机。在主从复制中，如果主节点和从节点时间相差很大，在主节点没有过期的 <code>Key</code>  可能从节点已经过期了，如果主节点宕机了恰好选举出这个从节点作为新的主节点，那么它可能就会面临一大批同时过期的 <code>Key</code> ，可能就会导致缓存雪崩。上述例子提示我们应该保证主从节点时间一致。同时，<strong>应对缓存雪崩可以在过期时间上加一个随机值</strong>。</li>\n<li><strong>缓存穿透</strong>：通俗来说就是大量<strong>不存在数据的请求</strong>直接打到后端数据库中，击穿了缓存。假设黑客攻击，请求的全是一些非法数据，比如 <code>id = -1、age = -1</code>  什么的，后端数据库都不存在，那么缓存更不可能存在，所以请求相当于无视缓存直接请求到后端数据库。应对方法可以<strong>对每个不存在数据的请求在缓存中加上 <code>unknown</code> ，这样下次相同的非法请求打过来缓存就可以拦截。<strong>但是这也有问题，毕竟非法、各不相同的请求有无数个，所以可以在接口层增加校验，不合法的参数直接 <code>return</code> 。 <code>Redis</code>  自带的</strong>布隆过滤器</strong>也能够很好的防止缓存穿透，它的原理就是利用高效的数据结构和算法快速判断这个 <code>key</code>  是否在数据库中。</li>\n<li><strong>缓存击穿</strong>：某个热点数据，很多请求都在访问它，但是如果该数据突然过期了，还是会导致很多请求打到后端数据库，所以可以<strong>设置热点数据不过期或者加上互斥锁</strong>。</li>\n</ul>\n<p>参考链接：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9tcC53ZWl4aW4ucXEuY29tL3Mva256LWotbThiVGc1R25LYzdvZVpMZw==\">https://mp.weixin.qq.com/s/knz-j-m8bTg5GnKc7oeZLg</span></p>\n<blockquote>\n<p>Q9：看你项目是涉及金融场景，是如何保证<strong>幂等</strong>的？</p>\n</blockquote>\n<h1 id=\"朋友-腾讯云智研发\"><a class=\"anchor\" href=\"#朋友-腾讯云智研发\">#</a> 朋友 - 腾讯云智研发</h1>\n<p>该部分面经是我朋友面的腾讯云智研发，他在面完之后只有一个体会：计网、os 这些真的很能考研一个科班是否基础扎实。因为他被问了大量的计网知识，面试过程也是疯狂道歉。希望大家能够巩固基础，下 <code>offer</code>  雨。</p>\n<blockquote>\n<p>Q1：TCP 与 UDP 区别</p>\n</blockquote>\n<p>TCP 是一对一建立连接的可靠传输控制协议，可以保证数据被全部有序的接收，同时还有流量控制、拥塞避免的功能。</p>\n<p>UDP 是一对多、一对一，多对一，多对多的不可靠无连接用户数据报协议，对应用层传过来的报文不合并也不拆分，只是添加 UDP 首部（TCP 会根据 MSS 拆分）。它是尽最大努力发送数据，但是不能保证数据一定会到达接收方，数据在传输过程中丢了也就丢了，不会重传，也不会根据网络拥塞情况做出优化让步。但是因为速度快，可以用于直播这种实时应用。</p>\n<blockquote>\n<p>Q2：GET/POST 区别</p>\n</blockquote>\n<p><code>GET</code>  主要用于传输表单，会在 <code>URL</code>  上携带参数，存在一定风险； <code>POST</code>  主要用于传输实体对象，不会修改 <code>URL</code> 。</p>\n<blockquote>\n<p>Q3：HTTP/HTTPS 区别</p>\n</blockquote>\n<p><code>http</code>  是明文传输，在数据传输的过程中容易被中间人窃取甚至替换，是不安全的。但是 <code>https</code>  是安全的，通过非对称加密传输客户端生成的<strong>随机密钥</strong>，再用该密钥进行对称加密传输数据。</p>\n<p>详细过程参考（我在脑海里走了一遍加密流程）：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9tcC53ZWl4aW4ucXEuY29tL3MvMjFKYVh3ZGZTakl0ajVTZ093aGFwZw==\">https://mp.weixin.qq.com/s/21JaXwdfSjItj5SgOwhapg</span></p>\n<blockquote>\n<p>Q4：SESSION/COOKIE 区别</p>\n</blockquote>\n<blockquote>\n<p>Q5：TOKEN 是啥？</p>\n</blockquote>\n<blockquote>\n<p>Q6：如何实现多个设备同时登陆 ？</p>\n</blockquote>\n<blockquote>\n<p>Q7：如何实现多个设备无法同时登陆？</p>\n</blockquote>\n<blockquote>\n<p>Q8：Redis 的 hash 如何存储？</p>\n</blockquote>\n<blockquote>\n<p>Q9：Redis 的 list</p>\n</blockquote>\n<blockquote>\n<p>Q10：Redis 持久化</p>\n</blockquote>\n<p><code>rdb</code> 、 <code>aof</code> 、混合持久化，在第一篇面试已经写了</p>\n<blockquote>\n<p>Q11：MySQL 常用数据库引擎有哪些？</p>\n</blockquote>\n<blockquote>\n<p>Q12：InnoDB 主键索引实现方式？</p>\n</blockquote>\n<blockquote>\n<p>Q13：B + 树和散列表有什么区别？</p>\n</blockquote>\n<blockquote>\n<p>Q14：什么时候会索引失效？</p>\n</blockquote>\n<blockquote>\n<p>Q15：事务有哪些特性？</p>\n</blockquote>\n<p><code>ACID</code> ：原子性、持久性、隔离性、一致性</p>\n<blockquote>\n<p>Q16：间隙锁</p>\n</blockquote>\n<blockquote>\n<p>Q17：事务有哪些隔离级别？</p>\n</blockquote>\n<blockquote>\n<p>Q18：MVCC</p>\n</blockquote>\n<p>笔试：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9sZWV0Y29kZS5jbi9wcm9ibGVtcy9kZXNpZ24tY2lyY3VsYXItcXVldWUv\">设计循环队列</span></p>\n",
            "tags": [
                "八股文"
            ]
        },
        {
            "id": "https://cecilia.cool/2023/03/11/%E5%85%AB%E8%82%A1%E6%96%87/MySQL/",
            "url": "https://cecilia.cool/2023/03/11/%E5%85%AB%E8%82%A1%E6%96%87/MySQL/",
            "title": "MySQL",
            "date_published": "2023-03-11T12:10:31.000Z",
            "content_html": "<p>本文参考网上各种博客以及《MySQL 是怎样运行的》一书，特别声明的是，我并没有看过任何关于 MySQL 的源码，我所见所学，皆是站在前人的基础。</p>\n<blockquote>\n<p>一条 <code>SQL</code>  查询语句在 <code>MySQL</code>  中如何执行的？</p>\n</blockquote>\n<p>算是一道经典面试题了，相当于计网的一条 <code>URL</code>  从输入到界面呈现发生了什么类似。</p>\n<ol>\n<li>先检查语句是否有权限，在 8.0 版本以前，有权限会先检查缓存，但是这并不是一个很好的特性，所以在 8.0 版本后删除了该操作。</li>\n<li>分析器进行词法分析，提取关键字，判断语法。</li>\n<li>优化器生成执行方案，最后交给执行器进行权限校验后执行，返回执行结果。</li>\n</ol>\n<blockquote>\n<p>一般如何对 <code>SQL</code>  优化？</p>\n</blockquote>\n<p>这种问题把你能想到的都答上去，但是尽量要有条理。</p>\n<ol>\n<li>设计表方面：尽量减少字符串使用，多用数字类型；使用字符串尽量用 <code>varchar</code>  节省空间。当索引列的重复数据占大多数时，建议删掉索引，因为使用这种索引并不会节省太多时间，相反有时回表太多导致性能下降。</li>\n<li><code>SQL</code>  语句方面：少使用 <code>select *</code> ，尽量避免用 <code>or</code>  连接条件。</li>\n<li>索引方面：对要查询的条件的列， <code>order by</code>  的字段建立索引，不要建立过多的索引，尽量使用组合索引。</li>\n</ol>\n<blockquote>\n<p>怎么看执行计划，里面的字段是什么意思？</p>\n</blockquote>\n<p>用 <code>explain</code>  命令，后面跟查询语句： <code>explain select name from student;</code> 。里面的字段，记不清，但是看到了我知道它代表什么意思。</p>\n<blockquote>\n<p>为什么不用 B 树而是用 B + 树？</p>\n</blockquote>\n<p>B + 树是 B 树的一种优化，B + 树非叶子节点存储的不是数据记录，而是目录记录。因为不存储数据，所以可以存储更多的目录记录，对应的叶子节点就可以存储更多的数据记录，使得整棵树层数更小，一般来说，一棵树都不会超过 4 层，所以查找起来非常快。</p>\n<p>个人还有个浅显的理解，在增删记录时，B + 树一般修改的只有非叶子节点，以删除一条记录为例，如果删除的记录是该页中键值大小在中间的记录，那么直接删了就行，并不会影响上面的目录记录。但是如果是 B 树，删除的记录在往往会影响下面节点的记录的位置。</p>\n<blockquote>\n<p>什么是最左前缀原则和最左匹配原则？</p>\n</blockquote>\n<ul>\n<li>最左前缀：创建多列索引（组合索引），应该把 <code>where</code>  使用最频繁的列放在索引的最左边。</li>\n<li>最左匹配：创建的组合索引 <code>(a,b,c)</code>  相当于创建了索引 <code>(a)、(a,b)、(a,b,c)</code>  三个索引。</li>\n</ul>\n<blockquote>\n<p>索引下推是什么？</p>\n</blockquote>\n<p><code>MySQL5.6</code>  默认开启的索引下推，在联合索引中，比如是 <code>(a,b)</code>  两个字段，再加上主键。假设有一条 <code>sql</code>  语句的过滤条件涉及到 a、b 两个，那么索引下推开启后，存储引擎先找到所有符合关于 a 条件的数据，再根据索引中已经存在的 b 进行过滤。找到符合条件的数据，最后在回表查询。</p>\n<p>参考链接：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9tcC53ZWl4aW4ucXEuY29tL3MvODdxc3JqLV9oRzU0dXhjT2xGcjM1UQ==\">https://mp.weixin.qq.com/s/87qsrj-_hG54uxcOlFr35Q</span></p>\n<blockquote>\n<p><code>MySQL</code>  调优你一般从哪些方面入手？</p>\n</blockquote>\n<ul>\n<li>查询 <code>explain</code>  执行计划</li>\n<li>索引的建立一定要合适，尽量使索引满足最左前缀原则，查询过程中尽量触发索引覆盖和索引下推。</li>\n<li>在读多写少的业务场景中，建立普通索引使用 <code>change buffer</code>  减少 IO 耗时。</li>\n<li>如果遇到很长的字符串，而这些字符串又有一些共性，可以考虑建立前缀索引。</li>\n</ul>\n<blockquote>\n<p>事物的隔离级别有哪些？</p>\n</blockquote>\n<p><code>SQL</code>  标准定义的隔离级别有：</p>\n<ul>\n<li>读未提交：可以读到其他未提交事务修改的数据，也会导致脏读、不可重复读、幻读。</li>\n<li>读提交：可以读到其他事务提交后的数据，会导致不可重复读、幻读。</li>\n<li>可重复读：保证一个事务中同一个数据的一致性，会导致幻读</li>\n<li>串行化：性能最差，最安全。</li>\n</ul>\n<blockquote>\n<p>看你简历写了熟悉 <code>MVCC</code> ，能说说是什么原理吗？</p>\n</blockquote>\n<p>解释 <code>MVCC</code>  首先要回到事务隔离级别以及如何实现这种隔离级别：</p>\n<ul>\n<li><strong>读未提交</strong>是事务可以读到其他事务修改但未提交的数据，会造成脏读，这种隔离级别实现直接读最新的数据即可。</li>\n<li><strong>读提交</strong>是事务可以读到其他事务修改并提交后的数据，<strong>可重复读</strong>是连其他事务修改了的数据都不能读，这两者就是通过 <code>MVCC</code>  实现的。</li>\n<li><strong>串行化</strong>直接加锁即可，性能最低，但最安全。</li>\n</ul>\n<p>所以平时业务需要根据对数据一致性的不同要求设置不同的事务隔离。提到 <code>MVCC</code> ，是基于 <code>undo</code>  日志形成的版本链和 <code>ReadView</code>  实现的。</p>\n<ul>\n<li><strong>版本链</strong>：每条记录都有一个隐藏属性 <code>roll_pointer</code>  可以指向上一个记录的版本，这个版本实质上就是 <code>undo</code>  日志，而 <code>undo</code>  日志本身也是记录，也有 <code>roll_pointer</code> ，从而就形成了版本链。还需要提到的就是，记录还有一个隐藏属性 <code>trx_id</code> ，表示该记录版本的事务 <code>id</code> 。</li>\n<li><strong>ReadView</strong>：产生的时机待会再说，器中包括了几个重要的属性\n<ul>\n<li><code>m_ids</code> ：生成 <code>ReadView</code>  时，还在活跃的所有事务的 <code>id</code> ，活跃就是指还没有提交的事务。</li>\n<li><code>min_trx_id</code> ：生成 <code>ReadView</code>  时，还在活跃的最小的事务 <code>id</code> 。</li>\n<li><code>max_trx_id</code> ：生成 <code>ReadView</code>  时，下一个事务应该被分配的 <code>id</code> 。注意它并不是 <code>m_ids</code>  中的最大值。</li>\n<li><code>creator_trx_id</code> ：创建该 <code>ReadView</code>  的事务的 <code>id</code> 。</li>\n</ul>\n</li>\n</ul>\n<p>我们先看一下 <code>MVCC</code>  如何实现<strong>读提交</strong>的隔离性，这个级别的要求就是事务可以读到已提交的事务的修改。我们假设所有的事务都在对记录 A（此时 A 的 <code>trx_id</code>  为 80，该事务已经提交）进行修改和读取。当事务 100 修改了两次记录 A，那么关于记录 A 的版本链 <code>100-&gt;100-&gt;80</code> 。然后事务 90 要对记录 A 进行查询，此时就会响应 <code>select</code>  生成 <code>ReadView</code> ，那么这个 <code>ReadView</code>  的 <code>m_ids</code>  就包括了 100，然后先读到第一个 100，发现 100 已经在 <code>m_ids</code>  中，表示这个事务还没有提交，就不能读到它的修改的数据，就向下查找，直到找到 80 这个版本，发现已经提交了，可以读。</p>\n<p>后来假设事务 100 已经提交了，那么事务 90 再执行查询语句，又会新生成一个 <code>ReadView</code> ，此时 <code>m_ids</code>  就不会包含 100 了，也就可以读到 100 的记录版本了。</p>\n<p>再看一下如何实现<strong>可重复读</strong>的隔离级别，它要保证整个事务的数据都是一致的，其实对应的策略就是只生成一个 <code>ReadView</code> ，而不是每次查询都生成一个 <code>ReadView</code> 。</p>\n<p>参考：《MySQL 是怎样运行的》第 21 章，作者 —— 小孩子 4919</p>\n",
            "tags": [
                "八股文"
            ]
        },
        {
            "id": "https://cecilia.cool/2023/03/08/%E5%85%AB%E8%82%A1%E6%96%87/Redis/",
            "url": "https://cecilia.cool/2023/03/08/%E5%85%AB%E8%82%A1%E6%96%87/Redis/",
            "title": "Redis",
            "date_published": "2023-03-08T09:27:22.000Z",
            "content_html": "<p>本文积累了我在（准备）面试的关于 <code>Redis</code>  的问题，当然我不喜欢背八股文，所以就会强迫自己去系统性学习， <code>Redis</code>  之前我开了一个 <code>tag</code>  的，但是没更完就要准备面试了，Java 太卷了！！</p>\n<blockquote>\n<p><code>Redis</code>  为什么这么快？</p>\n</blockquote>\n<p>有几个方面的原因：</p>\n<ul>\n<li>单线程执行，在 6.0 版本以前处理网络请求和数据操作都是单线程，减少了上下文切换，性能对于一些中小项目是完全足够的。网络请求处理使用的 IO 多路复用，数据处理因为是在内存中操作，CPU 资源并不会限制 <code>Redis</code>  性能。6.0 版本对网络处理使用了多线程，保留数据操作使用单线程，同时页保证不会出现并发问题。</li>\n<li>基于内存操作，不必多说，比 <code>MySQL</code>  要请求磁盘快多了。</li>\n<li>高效的数据结构， <code>Redis</code>  底层使用了很多高效的数据结构，比如 <code>SDS</code> 、压缩列表、跳表、哈希表等。</li>\n<li>自定义协议：使用了高性能的自定义 <code>Redis</code>  协议 RESP 和协议分析器。</li>\n</ul>\n<p>这些因素加起来使得 <code>Redis</code>  能够达到一秒十万级别的处理。</p>\n<p>参考链接：</p>\n<ul>\n<li><span class=\"exturl\" data-url=\"aHR0cHM6Ly9tcC53ZWl4aW4ucXEuY29tL3MvS3R6dmF3RG5RUXdoZmpuQ29YcGNNUQ==\">https://mp.weixin.qq.com/s/KtzvawDnQQwhfjnCoXpcMQ</span></li>\n<li><span class=\"exturl\" data-url=\"aHR0cHM6Ly9tcC53ZWl4aW4ucXEuY29tL3MvbXNjS0luV05BdWhDYmcxODNVbTlfZw==\">https://mp.weixin.qq.com/s/mscKInWNAuhCbg183Um9_g</span></li>\n</ul>\n<blockquote>\n<p>刚才你提到了 <code>HyperLoglog</code> ，知道怎么原理吗？</p>\n</blockquote>\n<p><code>HyperLoglog</code>  是一个基于基数统计的数据结构，其实底层算法很早之前就有人提出了，但是 <code>Redis</code>  第一次使用数据结构将其实现。说到应用，我们可以假设一个场景，有个业务需求需要每个网页都统计访问量，同一 IP 多次访问只算做一次访问。如果是在业务端实现，最先想到的就是对每一个网页加一个 <code>Set</code> ，最后需要统计量时直接获取集合大小即可，如果访问量很大，上百万、千万什么的，就非常消耗内存，不可能为了这么小的业务需求付出这么大的内存，并且这种业务是可以容忍一定误差的，所以就可以使用 <code>Redis</code>  里的 <code>HyperLoglog</code> 。</p>\n<p>至于原理，涉及到统计概率中的伯努利实验，以及后来者引入的桶和加权平均等修正，我还没来得及深入了解。仅仅只知道这确实可以统计去重元素个数，但是存在一点误差，如果可以容忍误差，那么性能是很高的。</p>\n<blockquote>\n<p><code>Redis</code>  如何保证数据高可用的？（变相问你持久化）</p>\n</blockquote>\n<p><code>Redis</code>  为了保证数据高可用，引入了持久化机制， 在早期版本，还有 VM，后来版本不推荐了。现在一般都是使用 <code>RDB</code> 、 <code>AOF</code>  或者混合持久化。</p>\n<ul>\n<li>\n<p><code>RDB</code>  通过对内存数据拍摄快照来持久化数据，触发机制是在一定时间内发生一定次数的修改操作。当然也可以使用 <code>save/bgsave</code>  主动拍摄快照，前者会阻塞线程，后者才会 <code>fork</code>  一个子线程进行快照拍摄。因为采用了压缩算法，实际占用空间很小。异步存储为了保证数据一致性，借助了操作系统的 <code>Copy on Write</code>  机制，主线程修改哪个页，就会先将这个页复制出来，在复制页进行修改。等快照拍摄结束，再将复制的页合并到原始内存中。</p>\n</li>\n<li>\n<p><code>AOF</code>  通过存储执行的命令到磁盘中保证数据的持久性，可以配置多种存储方式，比如执行一条命令就存储一条，或者每秒存储一次，或者看系统心情，什么时候有空什么时候就将缓冲区的命令存进去。 <code>AOF</code>  机制执行久了，就会导致文件保存了很多无效的命令，所以需要重写 <code>AOF</code>  文件 —— <code>bgrewriteaof</code> ，过程为：子线程遍历 <code>Redis</code>  内存生成一系列指令，然后将这些指令序列化到临时文件中，过程中的增量命令会追加到临时文件中，最后替换 <code>AOF</code>  文件。这里需要重点说一下，我们将数据写入到文件中时，其实是先写入到内核缓冲区，再到磁盘缓冲区，最后到磁盘，最后一个阶段我们是无法介入的，但是可以调用 <code>fsync()</code>  强制将数据刷新到磁盘缓冲区。 <code>redis</code>  默认是每秒调用一次。（有参数控制何时重写，比如文件大小超过多少，增量达到多少）</p>\n</li>\n<li>\n<p><code>混合持久化</code> ：4.0 版本后还出现了混合持久化，该机制必须打开 <code>AOF</code> ，隔一段时间拍摄快照，生成 <code>rdb</code>  数据，两次快照之间的记录使用 <code>AOF</code>  日志来记录，并追加到 <code>rdb</code>  数据后面。恢复数据时，先回复 <code>rbd</code>  数据，再执行 <code>AOF</code>  日志。这种机制既解决了 <code>rdb</code>  快照摄时突然断电导致整个快照丢失（因为还在临时文件中），也解决了 <code>AOF</code>  文件太大，不断重写的性能消耗。</p>\n</li>\n</ul>\n<p>参考链接：</p>\n<ul>\n<li><span class=\"exturl\" data-url=\"aHR0cHM6Ly9wZGFpLnRlY2gvbWQvZGIvbm9zcWwtcmVkaXMvZGItcmVkaXMteC1yZGItYW9mLmh0bWwjcmRiJUU1JTkyJThDYW9mJUU2JUI3JUI3JUU1JTkwJTg4JUU2JTk2JUI5JUU1JUJDJThGLTQtMCVFNyU4OSU4OCVFNiU5QyVBQw==\">Redis 进阶 - 持久化</span></li>\n<li><span class=\"exturl\" data-url=\"aHR0cHM6Ly9tcC53ZWl4aW4ucXEuY29tL3MvT19xRGNvNi1EYXN1M1JvbVdJS19JZw==\">https://mp.weixin.qq.com/s/O_qDco6-Dasu3RomWIK_Ig</span></li>\n</ul>\n<blockquote>\n<p><code>Redis</code>  如何保证数据一致性？</p>\n</blockquote>\n<p>首先，很难保证缓存和数据库 100% 数据一致，因为我们引入 <code>Redis</code>  本身是为了性能，花很大代价完全保证数据一致性，有时性能反而还会下降，只能说尽量吧。</p>\n<p>考虑到并发，面对更新请求，我了解到的解决方案就是：先更新数据库再删除缓存（也有问题，但是相对概率很小）。如果是更新数据库 + 更新缓存，并发问题很大，哪怕是先删除缓存再更新数据库，也存在并发问题，因为从数据库拿数据到缓存中是两步：从数据库读取，写到缓存中。只要不是原子操作，在并发环境就可能导致数据不一致。</p>\n<p>再考虑到删除缓存操作可能会失败，现在的解决方案一般是使用消息队列或者订阅数据库变更日志再操作缓存（canal）。</p>\n<p>参考链接：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9tcC53ZWl4aW4ucXEuY29tL3MvRDRJazZsVEFfeVNCT3lEM3dhTmoxdw==\">https://mp.weixin.qq.com/s/D4Ik6lTA_ySBOyD3waNj1w</span></p>\n<blockquote>\n<p>Redis 的 <code>SDS</code>  是什么？</p>\n</blockquote>\n<p>脑图：回忆一下当初学 C 语言时字符串的缺陷， <code>SDS</code>  就是为了克服这些缺陷。</p>\n<p><code>SDS</code>  是 <code>Redis</code>  自定义的简单动态字符串，也是 <code>Redis</code>  最基本的数据结构之一。设计 <code>SDS</code>  是因为 c 语言的字符串问题太多，性能太低了。主要问题是：</p>\n<ul>\n<li>获取长度时间复杂度为 <code>O(N)</code> ， <code>SDS</code>  内部维护了当前字符串长度 <code>len</code> ， <code>O(1)</code>  复杂度</li>\n<li>操作不方便，容易溢出，类似 <code>strcat</code>  这种函数，拼接两个字符串，都会默认前一个字符串剩余空间足够，所以很不方便。 <code>SDS</code>  维护了当前分配空间大小 <code>alloc</code> ，可以检测剩余空间。</li>\n<li>以 <code>\\0</code>  结束，需要指定编码格式。这种性质使得字符串只能存储文本文件， <code>SDS</code>  使用了字节数组，使得可以存储任何可转为字节的数据。</li>\n</ul>\n<p><code>SDS</code>  还可以动态扩容，并且会还会多分配一些未使用空间，减少分配次数。 <code>SDS</code>  设计了不同类型的结构体，区别在于 <code>len</code>  和 <code>alloc</code>  的大小不同，通过为不同大小字符串灵活分配，可以节省内存。</p>\n<figure class=\"highlight c\"><figcaption data-lang=\"c\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token keyword\">struct</span> <span class=\"token keyword\">__attribute__</span> <span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>__packed__<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> <span class=\"token class-name\">sdshdr16</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre>    <span class=\"token class-name\">uint16_t</span> len<span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre>    <span class=\"token class-name\">uint16_t</span> alloc<span class=\"token punctuation\">;</span> </pre></td></tr><tr><td data-num=\"4\"></td><td><pre>    <span class=\"token keyword\">unsigned</span> <span class=\"token keyword\">char</span> flags<span class=\"token punctuation\">;</span> \t<span class=\"token comment\">// SDS 类型</span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre>    <span class=\"token keyword\">char</span> buf<span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"6\"></td><td><pre><span class=\"token punctuation\">&#125;</span><span class=\"token punctuation\">;</span></pre></td></tr></table></figure><p>最后， <code>SDS</code>  还使用了编译优化 <code>__attribute__ ((__packed__))</code> ，告诉编译器取消结构在编译中的对齐优化，而是实际使用多少就分配多少。比如一个结构体有 1 个 <code>int</code>  和 1 个 <code>char</code> ，正常的优化对齐会使 <code>char</code>  对齐 <code>int</code> ，也就是 <code>char</code>  也会占 3 个字节。其实这 3 个字节就浪费了。 <code>SDS</code>  的编译优化就可以使 <code>char</code>  只分配一个字节。</p>\n<p>参考链接：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9tcC53ZWl4aW4ucXEuY29tL3MvcXB0RTE3MnNsZ182VGwxeXV6ZGJmdw==\">https://mp.weixin.qq.com/s/qptE172slg_6Tl1yuzdbfw</span></p>\n<blockquote>\n<p><code>Redis</code>  的压缩列表了解过吗？</p>\n</blockquote>\n<p>压缩列表是 <code>Redis</code>  的基础数据结构，如果 <code>list</code>  或者 <code>hash</code>  的节点较少，且保存的项都是一些小整数或者短字符串，通常会使用压缩列表来作为列表键的底层实现。</p>\n<p>压缩列表的本质是数组，不用链表是因为链表的节点之间内存不连续，无法高效利用 <code>CPU</code>  缓存，命中率很低。而压缩列表是内存连续的，命中率高。</p>\n<p>压缩列表前面几个字段是列表的一些信息，比如列表占用字节数，列表尾部节点的偏移量，节点数量，压缩列表结束点。而每个节点的构成为：前一个节点的长度，自身数据类型和节点长度，数据。</p>\n<p>为了尽可能节省内存，和 <code>MySQL</code>  记录中 <code>varchar</code>  一样，使用了不同字节来记录数据长度。前一个节点长度在 256 之内，使用 1 个字节，反之使用 5 个字节。但是这种机制会导致<strong>连锁更新</strong>问题，比如首节点插入长度大于 256 的数据，而下一个节点之前记录长度使用的 1 字节，此时就需要扩容，扩容后可能自身也超过了 256 字节，它的下一个节点也要扩容，如此往复，直到最后一个节点扩容完成。</p>\n<p>参考链接：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9tcC53ZWl4aW4ucXEuY29tL3MvcXB0RTE3MnNsZ182VGwxeXV6ZGJmdw==\">https://mp.weixin.qq.com/s/qptE172slg_6Tl1yuzdbfw</span></p>\n<blockquote>\n<p><code>Redis</code>  的哈希表了解过吗？</p>\n</blockquote>\n<p>哈希表是 <code>Redis</code>  的基础数据结构，数据类型 <code>hash</code>  如果节点很多或者项是大的整数、长字符串，就会使用哈希表。哈希表底层实现使用的是数组，链式增长解决哈希冲突。当负载因子 <code>&gt;= 1</code>  时，如果没有执行 <code>rdb</code>  或 <code>aof</code>  就会 <code>rehash</code> 。当负载因子 <code>&gt;=5</code>  时，不论有没有 <code>rdb\\aof</code>  都会 <code>rehash</code> 。</p>\n<p><code>rehash</code>  使用的是渐进式 <code>rehash</code> ，假设原哈希表 1 扩容后为哈希表 2，那么在 <code>rehash</code>  的过程中，每次有请求增删改查哈希表 1，就会把当前索引的节点转移到哈希表 2，使得整个 <code>rehash</code>  过程分配到各个请求上，避免一次性 <code>rehash</code>  的耗时操作。</p>\n<p>如果有一个查询请求，在哈希表 1 查不到，就会去哈希表 2 查询。在渐进式 <code>rehash</code>  进行期间，哈希元素的操作都是在两个哈希表进行的。</p>\n<p>参考链接：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9tcC53ZWl4aW4ucXEuY29tL3MvcXB0RTE3MnNsZ182VGwxeXV6ZGJmdw==\">https://mp.weixin.qq.com/s/qptE172slg_6Tl1yuzdbfw</span></p>\n<blockquote>\n<p><code>Redis</code>  的跳表了解过吗？</p>\n</blockquote>\n<p>跳表算是一种很优雅的实现，相较于普通链表，查询效率提升，相较于二叉树，省去了平衡、树退化的问题。比如一个链表节点为： <code>1 2 3 4 5 </code> 那么从中挑出一半 <code>1 3 5</code>  形成新的链表，并且将新的链表接到原来的链表上面，如此直到最上面的节点只有 1 个。这种数据结构使得查询效率为 <code>O(logn)</code> 。但是当插入新节点时，需要调整上面的节点，严重时时间复杂度还是 <code>O(n)</code> 。</p>\n<p>所以 <code>Redis</code>  也是优化了跳表，在每次插入节点时就通过随机数决定其层数，然后提前加入到对应的层数。这样虽然不是严格的 <code>log2N</code> ，也许要存储的节点会变多，也可能变小，但总的效率依然维持在一个很高的水平。</p>\n<p>在 <code>Redis</code>  常用的数据类型中， <code>zset</code>  就是通过跳表实现的。</p>\n<p>参考链接：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9tcC53ZWl4aW4ucXEuY29tL3MvTk9zWGRyTXJXd3E0TlRtMTgwYTZ2dw==\">https://mp.weixin.qq.com/s/NOsXdrMrWwq4NTm180a6vw</span></p>\n<blockquote>\n<p><code>Redis</code>  内存淘汰是怎么一回事？</p>\n</blockquote>\n<p>首先 <code>Redis</code>  对于过期了的 <code>key</code>  采用两种策略：惰性删除和定期删除。所以当内存耗尽时， <code>Redis</code>  存在过期 / 没过期两种键，所以删除策略也有不同，有 8 种：直接返回错误 / 删除 LRU、LFU 最早的过期（所有）key / 随机删除过期（所有）key</p>\n<p>传统的 <code>LRU</code>  存在存储、误删的缺点，所以 <code>Redis</code>  配置文件定义了一个属性，默认为 5，会取出 5 个过期的 <code>key</code> ，按照 <code>LRU</code>  算法删除对应 <code>key</code> 。</p>\n<p>至于 <code>LFU</code> ， <code>Redis</code>  也是采用了一些随机算法的策略，因为在 <code>RedisObject</code>  中有个 <code>lru</code>  属性，前 <code>24bit</code>  用于记录 <code>LRU</code> ，后 <code>8bit</code>  记录 <code>LFU</code>  的访问热度。 <code>8bit</code>  最多表示 255，所以不能单纯的访问一次就自增，而是通过比较两个参数：</p>\n<ol>\n<li>从 0~1 随机生成一个随机数 <code>R</code></li>\n<li>配置中有一个 <code>factory</code>  参数，用于计算 <code>P = 1 / (差值*factory+1)</code> 。这里的差表示当前热度减去初始值。</li>\n<li>如果 <code>P&gt;R</code> ，热度 + 1，反之 + 0。</li>\n</ol>\n<p>可以看出，热度越高，那么上升的概率越小。</p>\n<p>参考链接：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9tcC53ZWl4aW4ucXEuY29tL3MvLWNhTVRyT1hRdS1vME80NGU2STlkUQ==\">https://mp.weixin.qq.com/s/-caMTrOXQu-o0O44e6I9dQ</span></p>\n",
            "tags": [
                "八股文"
            ]
        },
        {
            "id": "https://cecilia.cool/2023/03/05/%E5%85%AB%E8%82%A1%E6%96%87/Java%E9%9B%86%E5%90%88/",
            "url": "https://cecilia.cool/2023/03/05/%E5%85%AB%E8%82%A1%E6%96%87/Java%E9%9B%86%E5%90%88/",
            "title": "Java集合",
            "date_published": "2023-03-05T02:02:30.000Z",
            "content_html": "<p>本文涉及到 Java 集合部分，也包括线程安全的集合，还是以面试题的形式记录。</p>\n<blockquote>\n<p>1. <code>ArrayList</code>  和 <code>LinkedList</code>  遍历谁更快？</p>\n</blockquote>\n<p><code>ArrayList</code>  更快， <code>ArrayList</code>  的优势就是内存连续，如果遍历 <code>LinkedList</code> ，随机 IO 可能更多一些</p>\n<blockquote>\n<p>2. 谈谈你对 <code>ArrayList</code>  的理解</p>\n</blockquote>\n<p>从我学习程度来看， <code>ArrayList</code>  需要注意的是：</p>\n<ul>\n<li>线程不安全，如果要保证安全，可以使用 <code>Vector</code>  或者使用 <code>Collections.synchronizedList(xx)</code> ，前者在每个操作方法上都加了 <code>synchronized</code>  关键字，后者内部会维护一个排他锁，每次对集合操作时，都需要先竞争锁。</li>\n<li>迭代器的 <code>fail-fast</code>  机制：这其实是每个实现了 <code>iterator</code>  集合都需要注意的地方，每次对数组进行增删，都会使得 <code>modCount++</code> ，而迭代器的 <code>next</code>  函数会检测 <code>modCount</code>  是否和最开始的自己保存的 <code>modCount</code>  相同，也就是检测在迭代的过程中数组是否发生了改变，<strong>因为这可能使得迭代过程中漏了某些元素或者重复遍历某些元素</strong>。</li>\n<li><code>ArrayList</code>  内部是使用 <code>Object</code>  数组 -- <code>elementData</code> ，但是该变量没有被 <code>private</code>  修饰，代码注释写的是方便内部类更快的访问该属性，如果被 <code>private</code>  修饰，那么同样的代码反编译后的字节码文件更复杂一些。</li>\n<li>扩容时，是 1.5 倍增长，而 <code>Vector</code>  扩容默认两倍扩容。</li>\n</ul>\n<blockquote>\n<p>3. <code>HashMap</code>  了解吗？1.7 版本和 1.8 版本都什么区别</p>\n</blockquote>\n<p><code>JDK1.7</code>  的 <code>HashMap</code>  使用的是数组桶加链表，如果链表过长，那么时间复杂度会退化为 O (n)，而 1.8 使用的引入了红黑树，当链表节点为 8 时，就转为红黑树。至于为什么是 8，估计也是个统计概率值。</p>\n<p><code>1.7</code>  节点插入时使用的是头插法，而 1.8 使用的是尾插法，头插在并发环境下容易出现环导致死循环，具体形成原因我并没有仔细研究，因为我认为 <code>HashMap</code>  本身就是线程不安全的，无论是头插还是尾插，在并发环境下都不能使用 <code>HashMap</code> ，曾经也有人向社区报告 <code>bug</code>  说 1.7 的 <code>HashMap</code>  尾插会在并发环境下出现死循环，但是社区并没有管，而是回复 “ <code>HashMap</code>  本来就不是给你在并发环境用的，想要安全请使用 <code>ConcurrentHashMap</code> ”。</p>\n<blockquote>\n<p>4. <code>HashMap</code>  扩容机制了解过吗，为什么容量大小必须是 2 的指数</p>\n</blockquote>\n<p><code>HashMap</code>  的容量大小默认是 16，阿里巴巴开发规范插件提示在初始化 <code>HashMap</code>  时尽量指定容量大小（预计存储个数 / 负载因子 + 1），因为没有指定可能会导致多次扩容，这个涉及到重建 <code>Hash</code>  表，链表分拆的操作，比较耗时。如果构造函数传入的容量不是 2 的指数，那么会将容量设置为既是 2 的指数，又是大于传入参数的最小数。</p>\n<p>至于为什么是 2 的指数，涉及 <code>hash</code>  公式，也就是 <code>index = hash &amp; (len - 1)</code> ，这个公式其实就相当于用长度取模 <code>index = hash % len</code> ，只不过位运算快很多。扩容机制为两倍扩容，公式为 <code>newCap = 2 * oldCap</code> ，我们假设下标为 <code>i</code>  的那一部分，这个部分的链表的节点都满足 <code>hash / len = y ... i</code> ，也就是说，我们假设商为 <code>y</code> ，那么当商为奇数时（ <code>hash &amp; oldCap == 0</code> ），扩容后再使用 <code>hash</code>  公式得到的结果就是 <code>i+n</code> ，如果商为偶数的话，扩容后再 <code>hash</code>  的结果还是 <code>i</code> 。这就使得每次扩容，都将现有的链表拆为两部分，一部分留在当前下标，另一部分转移到扩容后的 <code>i+n</code>  处，这种机制使用扩容时逻辑简单，操作迅速，还使得数组中节点分布均匀，不会出现那种节点都分布在前半部分或者后半部分。</p>\n<blockquote>\n<p>5. 为什么重写 <code>equals</code>  方法的同时建议重写 <code>hashCode方法</code> ，能用 <code>HashMap</code>  举个例子吗？</p>\n</blockquote>\n<p>我们假设一个类为 <code>People</code> ，只有一个属性 <code>name</code> ，那么现实情况下，两个对象相等，要么他们内存地址相同，要么 <code>equals</code>  比较后相同，这里也就是 <code>name</code>  相同。如果不重写 <code>hashCode</code>  方法，也就是使用 <code>Object</code>  内置的方法。现在有两个内存地址不同的 <code>People</code>  对象， <code>name</code>  相同使得 <code>equals</code>  相同，但是 <code>hashCode</code>  却不相同，那么当这两个对象作为 <code>Key</code>  加入到 <code>HashMap</code>  时，我们希望的是后加入的对象会覆盖前面加入的，因为两个对象是相同的，但是因为 <code>hashCode</code>  不同，他们两个甚至连映射出来的数组下标都不同，是无法覆盖的。这也就导致 <code>HashMap</code>  中有两个相同的 <code>key</code> ，这明显不符合哈希表的定义。</p>\n<p>再来看 <code>HashMap</code>  的源码，为了实现覆盖操作，首先就要使得 <code>equals</code>  相同的对象 <code>hashCode</code>  也相同才一定能映射到同样的下标，然后顺着链表向下查找，如果 <code>HashCode</code>  相同，同时 <code>equals</code>  也相同就会覆盖，反之如果遍历了整个链表都没有这样的节点，就算做新增节点，尾插到链表中。</p>\n<blockquote>\n<p>6. 说说 <code>HashMap</code>  与 <code>HashTable</code>  的区别</p>\n</blockquote>\n<ul>\n<li>并发： <code>HashTable</code>  用于并发环境，通过在方法上加入 <code>synchronized</code>  关键字保证线程安全，但是并行度太低了，所以大多数都使用 <code>ConcurrentHashMap</code> 。而 <code>HashMap</code>  是线程不安全的。</li>\n<li>存储： <code>HashTable</code>  键值对不允许存储 <code>null</code> ，而 <code>HashMap</code>  却可以， <code>HashTable</code>  源码中会先判断 <code>value</code>  是不是 <code>null</code> ，从而抛出空指针异常，并且会直接调用 <code>key</code>  的 <code>hashCode</code>  方法，计算比较粗暴，而 <code>HashMap</code>  统一指定 <code>key==null</code>  时 <code>hashCode</code>  为 0，至于 <code>value</code>  是否为 <code>null</code>  完全不重要，因为插入删除过程都不会涉及到 <code>value</code> ，只会比较 <code>key</code> 。</li>\n<li>迭代： <code>HashTable</code>  有两个迭代器， <code>Enumeration</code>  使用的是安全失败机制（ <code>fail-safe</code> ），而 <code>Iterator</code>  是快速失败机制。 <code>HashMap</code>  使用的是快速失败机制。</li>\n</ul>\n<blockquote>\n<p>7. 刚才提到 <code>ConcurrentHashMap</code> ，你知道什么？</p>\n</blockquote>\n<p>参考链接：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9tcC53ZWl4aW4ucXEuY29tL3MvTXk0UF9CQlhEbkFHWDFnaDYzMFpLdw==\">https://mp.weixin.qq.com/s/My4P_BBXDnAGX1gh630ZKw</span></p>\n<p>这个也要分 <code>1.7</code>  和 <code>1.8</code>  两个版本：</p>\n<ul>\n<li><code>1.7</code>  使用的是数组桶 + 链表，分段，段数决定并行度。通俗来讲， <code>1.7</code>  版本维护了一个数组，数组中每个元素都是一个 <code>HashMap</code> ，上锁就是对每个段上锁。所以并行度并不高</li>\n<li><code>1.8</code>  使用的是数组桶 + 链表（升级红黑树），同时锁的粒度更小了，使用 <code>CAS+synchronized</code>  来实现并发安全。维护的数组和 <code>HashMap</code>  的数组是一致的，只不过每次上锁都是对要修改的下标单个元素进行上锁。由于 <code>Synchronized</code>  的性能采用锁升级的方式优化后， <code>ConcurrentHashMap</code>  的性能也随之上升。</li>\n</ul>\n<p>至于 <code>CAS</code>  操作，是对数组元素进行修改，也就是<strong>链表的表头</strong>。</p>\n<p>只要上了锁，保证了线程安全，其他的都和 <code>HashMap</code>  没有太多区别，也就有些细节不同，比如 <code>HashMap</code>  再加入一个键值对就要扩容了，它会先插入再扩容，而 <code>ConcurrenHashMap</code>  是先扩容再插入。</p>\n<blockquote>\n<p>8. 这么了解 <code>ConcurrentHashMap</code> ，你实际用过吗，或者你看源码有什么地方用过吗？</p>\n</blockquote>\n<p>我倒是还没有做过并发项目，所以也就是看别人用过，这里举一个比较偏的现实例子。在日志框架中，日志门面 <code>slf4j-api</code>  自带了一个实现，叫做 <code>slf4j-simple</code> ，它在实现 <code>LoggerFactory</code>  时就用到了 <code>ConcurrenMap</code> ，键值对是 <code>&lt;String,Logger&gt;</code> ，这里的 <code>String</code>  对应的就是 <code>name</code> 。当外界传给 <code>LoggerFactory</code>  一个 <code>name</code>  时，就会从 <code>ConcurrentHashMap</code>  中找，如果有就返回，没有就新建一个，缓存起来再返回。</p>\n<figure class=\"highlight java\"><figcaption data-lang=\"java\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token keyword\">public</span> <span class=\"token keyword\">class</span> <span class=\"token class-name\">SimpleLoggerFactory</span> <span class=\"token keyword\">implements</span> <span class=\"token class-name\">ILoggerFactory</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre>    <span class=\"token class-name\">ConcurrentMap</span><span class=\"token generics\"><span class=\"token punctuation\">&lt;</span><span class=\"token class-name\">String</span><span class=\"token punctuation\">,</span> <span class=\"token class-name\">Logger</span><span class=\"token punctuation\">></span></span> loggerMap <span class=\"token operator\">=</span> <span class=\"token keyword\">new</span> <span class=\"token class-name\">ConcurrentHashMap</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre></pre></td></tr><tr><td data-num=\"4\"></td><td><pre>    <span class=\"token keyword\">public</span> <span class=\"token class-name\">SimpleLoggerFactory</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre>        <span class=\"token class-name\">SimpleLogger</span><span class=\"token punctuation\">.</span><span class=\"token function\">lazyInit</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"6\"></td><td><pre>    <span class=\"token punctuation\">&#125;</span></pre></td></tr><tr><td data-num=\"7\"></td><td><pre></pre></td></tr><tr><td data-num=\"8\"></td><td><pre>    <span class=\"token keyword\">public</span> <span class=\"token class-name\">Logger</span> <span class=\"token function\">getLogger</span><span class=\"token punctuation\">(</span><span class=\"token class-name\">String</span> name<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"9\"></td><td><pre>        <span class=\"token class-name\">Logger</span> simpleLogger <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span><span class=\"token class-name\">Logger</span><span class=\"token punctuation\">)</span><span class=\"token keyword\">this</span><span class=\"token punctuation\">.</span>loggerMap<span class=\"token punctuation\">.</span><span class=\"token function\">get</span><span class=\"token punctuation\">(</span>name<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"10\"></td><td><pre>        <span class=\"token keyword\">if</span> <span class=\"token punctuation\">(</span>simpleLogger <span class=\"token operator\">!=</span> <span class=\"token keyword\">null</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"11\"></td><td><pre>            <span class=\"token keyword\">return</span> simpleLogger<span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"12\"></td><td><pre>        <span class=\"token punctuation\">&#125;</span> <span class=\"token keyword\">else</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"13\"></td><td><pre>            <span class=\"token class-name\">Logger</span> newInstance <span class=\"token operator\">=</span> <span class=\"token keyword\">new</span> <span class=\"token class-name\">SimpleLogger</span><span class=\"token punctuation\">(</span>name<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"14\"></td><td><pre>            <span class=\"token class-name\">Logger</span> oldInstance <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span><span class=\"token class-name\">Logger</span><span class=\"token punctuation\">)</span><span class=\"token keyword\">this</span><span class=\"token punctuation\">.</span>loggerMap<span class=\"token punctuation\">.</span><span class=\"token function\">putIfAbsent</span><span class=\"token punctuation\">(</span>name<span class=\"token punctuation\">,</span> newInstance<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"15\"></td><td><pre>            <span class=\"token keyword\">return</span> <span class=\"token punctuation\">(</span><span class=\"token class-name\">Logger</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">(</span>oldInstance <span class=\"token operator\">==</span> <span class=\"token keyword\">null</span> <span class=\"token operator\">?</span> newInstance <span class=\"token operator\">:</span> oldInstance<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"16\"></td><td><pre>        <span class=\"token punctuation\">&#125;</span></pre></td></tr><tr><td data-num=\"17\"></td><td><pre>    <span class=\"token punctuation\">&#125;</span></pre></td></tr><tr><td data-num=\"18\"></td><td><pre></pre></td></tr><tr><td data-num=\"19\"></td><td><pre>    <span class=\"token keyword\">void</span> <span class=\"token function\">reset</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"20\"></td><td><pre>        <span class=\"token keyword\">this</span><span class=\"token punctuation\">.</span>loggerMap<span class=\"token punctuation\">.</span><span class=\"token function\">clear</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></pre></td></tr><tr><td data-num=\"21\"></td><td><pre>    <span class=\"token punctuation\">&#125;</span></pre></td></tr><tr><td data-num=\"22\"></td><td><pre><span class=\"token punctuation\">&#125;</span></pre></td></tr></table></figure><p>其实日志中使用 <code>ConcurrentHashMap</code>  很正常，因为日志打印本身就经常处于并发环境中，有些甚至会专门分配线程去处理日志。有时类专门有一个 <code>Logger</code> ，其对应键值就是类的全类限定名，多个线程可能都会请求这个 <code>logger</code> ，自然就需要线程安全的容器来缓存。</p>\n",
            "tags": [
                "八股文"
            ]
        },
        {
            "id": "https://cecilia.cool/2023/02/27/%E5%85%AB%E8%82%A1%E6%96%87/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/",
            "url": "https://cecilia.cool/2023/02/27/%E5%85%AB%E8%82%A1%E6%96%87/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/",
            "title": "计算机网络",
            "date_published": "2023-02-27T09:09:36.000Z",
            "content_html": "<p>本文积累了作者在准备面试时学习的<strong>计算机网络</strong>的知识与问题，作为科班出身，408 这些科目的重要性不必多说，能直接检验出你作为科班选手的水准。</p>\n<p>本 tag 的文章的问题都是本人先学习后，凭记忆写下的，相当于二次复习（复盘）了，这样有助于加深印象。</p>\n<blockquote>\n<p>1. 键入网址后依次发生了什么？</p>\n</blockquote>\n<p>参考链接：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9tcC53ZWl4aW4ucXEuY29tL3MvSTZCTHdiSXBmR0VKbnhqRGNQWGMxQQ==\">https://mp.weixin.qq.com/s/I6BLwbIpfGEJnxjDcPXc1A</span></p>\n<ul>\n<li>浏览器解析 URL 生成 Http 请求。</li>\n<li><strong>发送 Http 请求前，会将域名解析为 IP 地址</strong>，会先查询浏览器缓存、系统缓存、本机 <code>hosts</code>  文件，如果没有，就会发送请求到本地域名服务器，通过本地域名服务器分别访问根域名服务器、顶级域名服务器、权威域名服务器，最终拿到域名映射的 IP 地址，并且缓存起来。</li>\n<li><strong>建立 TCP 连接，三次握手</strong>。三次握手是为了双方确定对方具有发送和接收数据的能力，所以两次握手不行</li>\n<li><strong>增加 TCP 头部</strong>。如果 Http 数据太长，超过了 MSS（网络层数据大小），就需要将其切分再每个加上 TCP 头部。TCP 头部包含了源端口，目的端口，校验和，序号等信息，最后交给 IP 模块处理。</li>\n<li><strong>加上 IP 头部</strong>。IP 头部包含了源地址和目的地址，如果主机有多个网卡，会根据路由表规则来选择网卡，其实就是目的地址与网卡的掩码做<strong>与运算</strong>从而来选择，如果都不匹配，就会走默认网卡 0.0.0.0。</li>\n<li><strong>加上 MAC 头部</strong>。通过 ARP 协议来获取目的地址的 MAC 地址，如果 ARP 缓存有，就直接用，如果没有，就在以太网中广播目的地址从而得到响应拿到对应的 MAC 地址。</li>\n<li><strong>将给网卡，将包转为电信号，通过网线发送出去</strong>。</li>\n<li><strong>交换机拿到包，通过 MAC 表将数据从对应端口发送出去</strong>，如果表中没有对应映射，就对局域网所有主机发送。</li>\n<li><strong>路由器拿到包并根据 IP 地址进行转发</strong>。MAC 头部作用就是将包送到路由器，然后 MAC 头部就会被丢弃。路由器根据路由表决定下一跳（下一跳的 IP）。通过 ARP 协议拿到下一跳的 MAC 地址，重新发送。整体传输过程只有 MAC 地址在不断变化，因为包需要不断在以太网中传输。</li>\n<li>服务器收到请求，回复 ACK 和 Http 响应。浏览器得到响应，再请求 html 中的 js，css 资源。浏览器再解析渲染，呈现网页。</li>\n</ul>\n<blockquote>\n<p>2. 谈谈 http 协议</p>\n</blockquote>\n<p>参考链接：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9tcC53ZWl4aW4ucXEuY29tL3MvQUsxUGI5cngwcTVIZjhkcTZITk9odw==\">https://mp.weixin.qq.com/s/AK1Pb9rx0q5Hf8dq6HNOhw</span></p>\n<p>http 协议是超文本传输协议，一开始用于传输 html、css、js 等资源文件，后来也可以传输图片、视频、音频等。http1.0 是无状态的，每个 http 请求都必须重新建立 TCP 连接，这导致开销较大。http1.1 通过 <code>Cookie</code>  来管理状态，同时实现了持久化连接。</p>\n<p>http 请求由：请求行、消息头、数据组成。请求行包括请求方法、<strong>URI</strong>、协议版本。</p>\n<p>请求方法包括 GET（表单）、POST（实体）、DELETE（删除文件）、PUT（文件）、TRACE 等组成。</p>\n<p>状态码含义分别是：</p>\n<ul>\n<li>1xx：表示请求正在处理</li>\n<li>2xx：表示请求成功处理</li>\n<li>3xx：表示重定向</li>\n<li>4xx：表示客户端错误，请求不合法</li>\n<li>5xx：表示服务器错误，不能处理合法请求</li>\n</ul>\n<blockquote>\n<p>3. 谈谈 https 协议</p>\n</blockquote>\n<p>参考链接：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9tcC53ZWl4aW4ucXEuY29tL3MvMjFKYVh3ZGZTakl0ajVTZ093aGFwZw==\">https://mp.weixin.qq.com/s/21JaXwdfSjItj5SgOwhapg</span></p>\n<p>数据传输使用的仍然是 http 协议，只不过使用了 SSL 对数据进行了加密，保证了数据传输的安全。</p>\n<p>其过程为：</p>\n<ul>\n<li>客户端发送 https 请求，服务端响应<strong> CA 证书和公钥</strong>。（需要注意，是<strong>证书里面附带了公钥</strong>）</li>\n<li>客户端校验 CA 证书合法性，生成随机密钥 key，并使用<strong>公钥</strong>对 key 加密，再发送给服务端。</li>\n<li>服务端收到后，使用私钥对可 key 解密，拿到真正的 key，双方之后的数据传输就用该 key 进行<strong>对称加密传输</strong>。</li>\n</ul>\n<p>上述过程，如果没有 CA 证书，那么中间人攻击可以这样：在第一步将公钥换成自己的公钥 1，这样在第二步对客户端产生的 key 使用自己的私钥 1 解密从而拿到 key。最后再将 key 使用最开始服务端发送的公钥进行加密发送给服务端，这样 MITM（中间人攻击）照样可以完成。</p>\n<p>所以需要第三方的公信认证，CA 机构有一对公钥 / 私钥，公钥是对外界公开的，而私钥必须严格保密。当服务端将<strong> CA 证书 + 服务端公钥</strong>发送给客户端时， 会先将证书的数据（包括服务端公钥）进行哈希，得到哈希值<strong> H</strong>，再用私钥将 H 加密，最后客户端（浏览器）拿到证书后，会使用系统 / 浏览器内置的 CA 公钥对 H 进行解密得到 H，再自己通过证书指定的哈希算法对数据进行哈希得到<strong> H'</strong>，比较<strong> H==H'</strong>。</p>\n<p>私钥和公钥可以互相加密解密，非不是私钥只能解密。上述过程，假设有中间人换了证书中的服务端公钥，也会因为不知道私钥而无法加密哈希值导致客户端会检测出来。所以 CA 机构应该严格保密私钥，如果泄露，就会失去公信力。</p>\n<p>如果整个 https 通信全部用非对称加密确实可以，双方各拿一对公钥 / 私钥，然后交换公钥进行通信。至于为什么本质还是要使用对称加密，是因为非对称加密太耗时间了，仅用于传输 key 即可。</p>\n<blockquote>\n<p>4. 用过 ping 吗，说说原理</p>\n</blockquote>\n<p>参考链接：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9tcC53ZWl4aW4ucXEuY29tL3MvM0tGMEl4THVtOEVPdGNGMFpOSWlQQQ==\">https://mp.weixin.qq.com/s/3KF0IxLum8EOtcF0ZNIiPA</span></p>\n<p><code>ping</code>  实现原理依托于 <code>ICMP</code>  协议。该协议是互联网控制报文协议，用于报告网络错误、传送报文运输情况等。 <code>ICMP</code>  报文被封装到 <code>IP</code>  数据包里面。ping 使用的两种 <code>ICMP</code>  数据包是<strong>回送请求</strong>和<strong>回送响应</strong>。</p>\n<p>假设主机 A 对主机 B 进行了 ping 操作，那么主机 A 会封装 ** <code>ICMP</code>  回送请求 **，此时会记录请求产生的时间，并将其封装到 IP 数据包中，再加上 MAC 头部，最后发送出去。没有缓存目的 MAC 地址，先通过 ARP 协议获取。</p>\n<p>主机 B 收到报文后，逐步拆除 MAC 和 IP 头部，经过地址检验后，将有用的信息提取交给 ICMP 协议，再发送 ** <code>ICMP</code>  回送响应 **。主机 A 收到回送响应后，用当前时间减去 <code>ICMP</code>  数据包发送时间，就可以得到 <code>RTT</code> 。</p>\n<p>同时， <code>ICMP</code>  还维护了一个 <code>TTL</code> ，每次数据包经过一个路由器，就会 - 1，直到为 0 被丢弃，TTL 就可以检测出两个主机之间经过多少跳。</p>\n<p><code>tracert</code>  也是借助 <code>ICMP</code>  协议实现的。</p>\n<blockquote>\n<p>5. 聊聊 TCP 三次握手和四次挥手</p>\n</blockquote>\n<p>参考链接：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9tcC53ZWl4aW4ucXEuY29tL3MvclgzQV9GQTE5bjRwSTlIaWNJRXNYZw==\">https://mp.weixin.qq.com/s/rX3A_FA19n4pI9HicIEsXg</span></p>\n<p>TCP<strong> 三次握手</strong>：</p>\n<ul>\n<li>\n<p>确认双方都有发送和接收数据的能力。</p>\n</li>\n<li>\n<p>防止旧连接覆盖新连接。客户端知道自己此时应该建立哪个连接，但是网络传输过程复杂，很可能旧的连接 SYN 比新的 SYN 后到，到底建立哪个连接服务端是不知道的，所以必须有第三次握手，让客户端确认到底建立哪个连接。</p>\n</li>\n<li>\n<p>防止浪费资源。两次握手中，服务端不知道自己的 <code>ACK+SYN</code>  是否被客户端收到，这会导致重复发送 <code>SYN+ACK</code> ，建立很多个无用的连接。</p>\n</li>\n<li>\n<p>同步初始化序列号。同步序列号能够防止接收端接收的数据乱序。</p>\n</li>\n</ul>\n<p>总的来说，第三次握手是必要的，<strong>必须由客户端确认建立连接的各种状态信息的正确性</strong>。值得一提的是，第三次握手，发送方可以顺带发送数据。</p>\n<p>TCP<strong> 四次挥手</strong>：</p>\n<p>重点：主动放弃连接的一方会进入 <code>Time_Wait</code>  状态，在 Linux 中，会等待 2MSL（60 秒）。</p>\n<p>四次挥手的过程为（假设客户端主动断开连接）：</p>\n<ol>\n<li>客户端发送 <code>Fin</code>  表示自己断开连接，不再发送数据，但是可以接收数据，进入 <code>FIN_WAIT_1</code>  状态。</li>\n<li>服务端收到 <code>Fin</code> ，发送 <code>ACK</code> ，表示自己收到断开请求，需要处理剩下的数据，进入 <code>CLOSED_WAIT</code> 。</li>\n<li>服务端处理完数据，发送 <code>Fin</code> ，进入 <code>LAST_ACK</code>  状态。</li>\n<li>客户端收到 <code>Fin</code> ，发送 <code>ACK</code> ，进入 <code>TIME_WAIT</code>  状态，等待 <code>2MSL</code> ，最后进入 <code>CLOSED</code>  状态。</li>\n</ol>\n<p>正是因为服务端需要处理剩下数据，所以是四次挥手，同样，如果省去最后一次挥手，那么服务端就会一直处于 <code>LAST_ACK</code>  状态，当客户端想建立新的连接，发送 <code>SYN</code> ，服务端就会回复 <code>RST</code> ，建立连接的过程会终止。</p>\n<blockquote>\n<p>6. 为什么四次挥手中要有 <code>TIME_WAIT</code>  状态以及为什么要等 2MSL？</p>\n</blockquote>\n<p>参考链接：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9tcC53ZWl4aW4ucXEuY29tL3MvclgzQV9GQTE5bjRwSTlIaWNJRXNYZw==\">https://mp.weixin.qq.com/s/rX3A_FA19n4pI9HicIEsXg</span></p>\n<p>我们这里默认主动断开连接的是客户端。首先，主动断开连接的那一方才会进入 <code>TIME_WAIT</code> ，这个时间是 2MSL，在 Linux 中为 60s，而且这个时间是固定的，也就是在内核代码中写死了，无法修改。</p>\n<p><code>TIME_WAIT</code>  的出现能够保证被动断开连接方（服务端）可以正常的关闭，从 <code>LAST_ACK</code>  进入 <code>CLOSED</code>  状态。</p>\n<p>MSL 是数据包在网络传输中存活的最长时间， <code>TIME_WAIT</code>  设置为 2MSL，比较合理的解释为：如果服务端没有没有 ACK，超时重传 <code>FIN</code>  后再接收 <code>ACK</code>  的时间在 2MSL 之内。<strong>当客户端重新接收到 <code>FIN</code>  时，会重置 2MSL 时间</strong>。同时网络连接中的旧数据包在 2MSL 中能够被清理干净，如果客户端当前端口重新建立连接，不会有旧的数据传到当前端口，造成数据混乱。</p>\n<p><code>TIME_WAIT</code>  出现的原因为：</p>\n<ul>\n<li>保证服务端正常关闭</li>\n<li>防止旧的四元组数据包影响下一次连接传输。</li>\n</ul>\n<p>正因为主动断开会进入 <code>TIME_WAIT</code> ，此时既会白白占用端口，又会无法传输数据，经历时间还非常长，对于服务端来说是很大的负担，所以这个烂摊子尽量交给对方，尽量让对方断开连接。</p>\n<p>解决 <code>TIME_WAIT</code>  方法：</p>\n<ul>\n<li>使用 <code>tcp_rw_reuse</code> + <code>tcp_timestamp</code> ：这样可以使得处于 <code>TIME_WAIT</code>  套接字复用，因为开启了时间戳，新的连接不会接收时间戳过期的数据。</li>\n<li>其他方法不推荐使用。</li>\n</ul>\n<blockquote>\n<p>7. 知道 SYN 攻击吗，说说你知道的防御手段</p>\n</blockquote>\n<p>参考链接：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9pbmZvLnN1cHBvcnQuaHVhd2VpLmNvbS9pbmZvLWZpbmRlci9lbmN5Y2xvcGVkaWEvemgvU1lOK0Zsb29kLmh0bWw=\">https://info.support.huawei.com/info-finder/encyclopedia/zh/SYN+Flood.html</span></p>\n<p>SYN 攻击是 DDos 攻击的一种，通过程序不断发送 SYN 迅速占满服务端的 SYN 队列，使其崩溃的攻击手段。</p>\n<p>防御手段：</p>\n<ul>\n<li>\n<p>首包丢弃：大多数 SYN 攻击都是变源的，这使得在 SYN Flood 攻击中，每个 SYN 都是首包，Anti-DDos 系统可以丢弃收到的 SYN 首包，如果对方客户端是正常的，那么基于 TCP 超时机制，一定会重传，此时 SYN 就不是首包了，可以对其进行源认证。</p>\n</li>\n<li>\n<p>源认证：Anti-DDos 系统部署在网络入口，先代替服务端发送 SYN+ACK，如果收到了客户端的 ACK，就将其 IP 加入白名单，之后一段时间都不会代替服务端对该 IP 的 SYN 进行拦截。</p>\n</li>\n</ul>\n<p>源认证必须配合首包丢弃使用，不然性能瓶颈也只是从服务器转移到了 Anti-DDos 系统中。</p>\n<ul>\n<li>设置 TCP 参数也可以一定程度上防御 SYN 攻击，比如扩大半连接队列，开启 <code>syncookies</code> 。</li>\n</ul>\n<blockquote>\n<p>8.TCP 的半连接队列和全连接队列了解吗？如果队列满了怎么办？</p>\n</blockquote>\n<p>参考链接：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9tcC53ZWl4aW4ucXEuY29tL3MvdFJYbHExaEVycUtRTE1NTGN4b1h2Zw==\">https://mp.weixin.qq.com/s/tRXlq1hErqKQLMMLcxoXvg</span></p>\n<p>半连接队列是指 SYN 队列，服务端收到 SYN 请求，就会将其加入到 SYN 队列；全连接队列是指 Accept 队列，当服务端收到客户端的 <code>ACK</code>  就会将 SYN 队列对应节点放到 Accept 队列中。当队列满了，Linux 默认的操作是拒绝再接收 ACK。因为队列装不下了，但是有个问题就是，客户端发送了 ACK 就会进入 <code>ESTABLISHED</code>  状态，但是实际上服务端却没有接收。</p>\n<p>Linux 中变量 <code>tcp_abort_on_overflow</code>  为 0，就是丢掉客户端发送的数据，为 1 就会发送一个 <code>reset</code>  包给客户端。</p>\n<p><strong>所以全连接队列满了，一般解决方法就是扩大队列长度，Accept 队列长度由两个变量决定，结果式为 <code>len = min(backlog, somaxconn)</code> 。</strong></p>\n<p>半连接队列长度 <code>max_qlen_log</code>  取决于全连接队列长度 <code>len</code> 、变量 <code>max_syn_backlog</code> ： <code>max_qlen_log = 2 * min(len, max_syn_backlog)</code> 。</p>\n<p>半连接队列一般不会满，当队列中剩余长度达到某个特定值时（和 <code>max_syn_backlog</code>  有关，但是不同 Linux 版本计算方法可能不同），就不会再接收 <code>SYN</code>  了。其实当全连接队列满了，不论半连接队列如何，都不会再接收 <code>SYN</code>  了。</p>\n<p>半连接队列满了（假设遇到了 <code>SYN</code>  攻击），策略有三个：</p>\n<ul>\n<li>增大半连接队列长度，也就是增大那三个参数。</li>\n<li>打开 <code>syncookies</code> ，将该变量设置为 1 即可（0-- 关闭，1-- 队列满了打开 <code>syncookies</code> ，2-- 直接打开 <code>syncookies</code> ）。开启该功能后，不会再丢弃 <code>SYN</code>  包，而是服务器根据当前状态计算出一个值，放在 <code>SYN+ACK</code>  中发出，当客户端返回 <code>ACK</code>  报文时，取出该值校验合法性，建立连接。</li>\n<li>减少 <code>SYN+ACK</code>  重发次数，使得处于 <code>SYN_REVC</code>  状态的连接尽快断开。</li>\n</ul>\n<blockquote>\n<p>9. 谈谈 TCP 相关的参数</p>\n</blockquote>\n<p>参考链接：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9tcC53ZWl4aW4ucXEuY29tL3MveXRWN1JaU3lGWHl2UFdfbEtodjhodw==\">https://mp.weixin.qq.com/s/ytV7RZSyFXyvPW_lKhv8hw</span></p>\n<p>这里讲的 TCP 参数与 TCP 三次握手和四次挥手优化有关。</p>\n<p>三次握手优化角度：</p>\n<ul>\n<li>客户端（发送方）：客户端行为有发送 <code>SYN</code>  和 <code>ACK</code> ，以及重发 <code>SYN</code>  和 <code>ACK</code> 。\n<ul>\n<li><code>tcp_syn_retries</code>  参数：控制重传 <code>SYN</code>  次数，每次超时时间为上次 2 倍，初始为 1s。<strong>超过次数就会断开连接</strong>。</li>\n</ul>\n</li>\n<li>服务端（接收方）：服务端行为较复杂，涉及到半连接队列和全连接队列的大小以及拒绝策略\n<ul>\n<li>重发 <code>FIN+ACK</code>  次数：由 <code>tcp_synack_retires</code>  决定</li>\n<li>半连接队列：大小由 <code>tcp_max_syn</code> 、 <code>backlog</code> 、 <code>somaxconn</code>  共同决定。可以通过增大这三个参数来增大半连接队列。同时 <code>syncookies</code>  参数控制当半连接队列满了时，生成状态值校验来避免放到半连接队列中。</li>\n<li>全连接队列：大小由 <code>backlog</code>  和 <code>somaxconn</code>  共同决定。拒绝策略由 <code>tcp_abort_on_overflow</code>  决定，0 表示丢弃 <code>ACK</code> ，不让其进入全连接队列，一般用这个，还可以解决短暂的突发网络繁忙。1 表示发送 <code>RST</code>  包使其断开连接。</li>\n</ul>\n</li>\n<li>绕过三次握手：Linux 内核 3.1 版本后，出现了 <code>Fast Open</code>  机制，通过 <code>Cookie</code>  来绕过后面的三次握手。第一次正常三次握手，但是服务端可以在第二次握手时创建 <code>Cookie</code>  并发送给客户端。之后就可以重用该 <code>TCP</code>  连接，而不需要重复建立 TCP 连接。因为后续数据发送可以携带 <code>Cookie</code> ，服务端只需要验证 <code>Cookie</code>  即可。这种的缺点就是，如果重发，还需要重发 <code>Cookie</code> 。该机制使用 <code>tcp_fastopn</code> ：\n<ul>\n<li>0 ——  <code>close</code></li>\n<li>1 ——  <code>Client</code>  打开</li>\n<li>2 ——  <code>Server</code>  打开</li>\n<li>3 —— 双方都打开</li>\n</ul>\n</li>\n</ul>\n<p>四次挥手优化角度：</p>\n<ul>\n<li>主动断开方：会进入 <code>TIME_WAIT</code>  状态，接收发送 <code>FIN</code>  和 <code>ACK</code> 。\n<ul>\n<li><code>tcp_max_orphan</code>  参数：调用 <code>close</code>  函数后，连接就变成了<strong>孤儿连接</strong>，该参数限制了最大孤儿连接数量，超过直接发送 <code>RST</code>  包断开连接。</li>\n<li><code>FIN_WAIT1</code>  状态优化： <code>tcp_orphan_retries</code>  参数 —— 表示处于 <code>FIN_WAIT1</code>  状态的 <code>FIN</code>  重传次数，超过直接关掉连接。</li>\n<li><code>FIN_WAIT2</code>  状态优化： <code>tcp_fin_timeout</code>  参数：表示孤儿连接等待 <code>FIN</code>  的最长时间，默认 60s。</li>\n<li><code>TIME_WAIT</code>  状态优化：\n<ul>\n<li><code>tcp_max_tw_buckets</code>  参数：如果处于 <code>TIME_WAIT</code>  连接超过该参数，之后的连接不再进入该状态。</li>\n<li><code>tcp_tw_reuse</code>  参数：开启后可以复用处于 <code>TIME_WAIT</code>  状态的连接，需要配合时间戳使用。</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>被动断开方：\n<ul>\n<li>还是借助 <code>tcp_orphan_retires</code>  参数限定 <code>FIN</code>  重传次数。</li>\n</ul>\n</li>\n</ul>\n<p>小结：</p>\n<ul>\n<li>三次握手参数： <code>tcp_syn_retries</code> 、 <code>somaxconn</code> 、 <code>backlog</code> 、 <code>tcp_max_syn</code> 、 <code>syncookies</code> 、 <code>tcp_abort_on_overflow</code> 、 <code>tcp_fastopn</code> 。</li>\n<li>四次挥手参数： <code>tcp_max_orphan</code> 、 <code>tcp_orphan_retries</code> 、 <code>tcp_fin_timeout</code> 、 <code>tcp_max_tw_buckets</code> 、 <code>tcp_tw_reuse</code> 。</li>\n</ul>\n<blockquote>\n<p>10. 聊聊 TCP 的可靠传输机制，比如重传、拥塞、流量控制等</p>\n</blockquote>\n<ul>\n<li>\n<p><strong>重传机制</strong>： 接收方回复 ACK 用于提醒发送方应该发那个数据包，当出现数据包丢失，接收方需要重传，分为超时重传和快速重传</p>\n<ul>\n<li>\n<p>超时重传：接收方拿不到 3 这个数据包，就不发 3 的 ACK，发送方等待 3 这个 ACK 超时，再重传，一种是只重传 3（节省带宽，慢），另一种是 3，4，5（快，浪费带宽）等都重传。</p>\n</li>\n<li>\n<p>快速重传：发送方连续三次接收到同一个 ACK，则重传对应的数据报。</p>\n</li>\n</ul>\n<p>其实重传都面临一个选择：只重传这一个还是重传后边所有数据报。这就引出<strong> SACK</strong> 机制，接收方回复 SACK，SACK 会汇报收到的数据碎片，这个协议需要两边都支持。但是 SACK 并不能替代 ACK，<strong>接收方有权把已经报给发送端 SACK 里的数据给丢了</strong>。</p>\n<p><strong>SACK</strong> 有一个严重的问题，Linux 代码中，使用一个 <code>sk_buff</code>  的数据结构，简称 <code>SBK</code> ，用于存储<strong>发送、接收</strong>队列等，还有一个结构体为 <code>skb_cb</code>  用于控制缓存，记录各种<strong> TCP packet</strong> 的各种信息，如小报文的数量 <code>tcp_gso_segs</code> ，无符号两字节，最多表示 64K，SKB 会将小报文段分片累积成大报文段再发送，但是内部最多维护 17 个分片队列，每个队列最大 32KB，如果有恶意攻击者将 <code>mss</code>  设置为 8，则每个小报文段大小为 8B。<strong>SACK</strong> 机制会将许多 <code>SKB</code>  合并填满一个 <code>SKB</code> ，那么就可能出现： <code>17 * 32 * 1024 / 8 &gt; 64K</code>  导致 <code>tcp_gso_segs</code>  溢出，进入 <code>BUG_ON</code>  函数使得服务器拒绝远程连接。</p>\n</li>\n<li>\n<p><strong>滑动窗口</strong>：发送方和接收方都有窗口，接收方的滑动窗口可以使发送方根据接收方的接收能力来发送数据。确认机制为<strong>累计确认 / 累计应答</strong>，假设收到序列号为 100 的 ACK，说明 100 以前的数据都收到了。</p>\n<p>如果接收方的窗口为 0 了，也会将发送方的窗口设为 0，此时不再发送数据，直到接收方窗口恢复，此时发送一个通知消息给发送方，并等待数据。如果这个通知消息因为网络拥塞丢失了，就会导致：接收方一直等待数据，发送方一直等待通知的死锁状况。所以一旦发送方窗口被置为 0，就会每隔一段时间发送探测报文，询问接收方窗口大小。</p>\n<p><strong>Silly Window Syndrome</strong> 是一种现象，会对小的 window size 做出响应，为了避免对小的 window size 做出响应，直到有足够大的 window size 再响应，如果窗口太小，发送出去的数据甚至没有 <code>MSS</code>  高，就会先累积再发送。</p>\n</li>\n<li>\n<p><strong>拥塞处理</strong>：名词： <code>ssthresh</code>  是慢启动阈值， <code>cwnd</code>  为拥塞窗口大小。</p>\n<p>三个状态，分别是慢启动，拥塞避免和快速恢复。</p>\n<ul>\n<li><strong>慢启动</strong>： <code>cwnd</code> （拥塞窗口）一开始为 <code>1MSS</code> ，每收到 1 个 <code>ACK</code>  就二倍上升，如果<strong>超时</strong>， <code>ssthresh=cwnd/2</code> ，并且 <code>cwnd=1</code>  重新慢启动。如果之后 <code>cwnd &gt;=  ssthresh</code>  就进入<strong>拥塞避免</strong>。如果触发快速重传，就进入<strong>快速恢复</strong>。</li>\n<li><strong>拥塞避免</strong>：每一个 <code>RTT</code>  就 <code>cwnd++</code> ，如果超时，设置 <code>ssthresh=cwnd/2, cwnd = 1</code> ，进入慢启动。如果触发快速重传，进入<strong>快速恢复</strong>。</li>\n<li><strong>快速恢复</strong>：如果超时，同样操作，进入慢启动；每次收到一个冗余 <code>ACK</code> ， <code>cwnd++</code> ，如果收到新 <code>ACK</code> ，进入<strong>拥塞避免</strong>。</li>\n</ul>\n<p>进入<strong>快速恢复</strong>之前，设置参数为 <code>ssthresh=cwnd/2, cwnd = ssthresh + 3</code> 。</p>\n<p><strong>这里的 <code>RTT</code>  是指一个窗口的数据全部发送出去，又全部收到 <code>ACK</code>  的时间，而不是某一个报文的往返时间</strong>。</p>\n</li>\n</ul>\n<blockquote>\n<p>11.DNS 劫持和 DNS 污染</p>\n</blockquote>\n<p><strong>DNS 劫持</strong>：劫持了 DNS 服务器，通过某些手段取得某域名的解析记录控制权，进而修改此域名的解析结果，导致对该域名的访问由原 IP 地址转入到修改后的指定 IP。</p>\n<p><strong>DNS 污染</strong>：通过对 UDP 端口 53 上的 DNS 查询进行入侵检测，一经发现与关键词相匹配的请求则立即伪装成目标域名的解析服务器（NS，Name Server）给查询者返回虚假结果。很难靠个人设置解决，使用 VPN 是一个方法</p>\n<blockquote>\n<p>12. 聊聊 IP 协议，它和 MAC 地址有什么区别，IPV4 和 IPV6 呢？</p>\n</blockquote>\n<p>IP 协议用于唯一标识网络设备，属于网络层协议，传输层将数据包传到网络层后，会为数据加上 IP 首部。 <code>MAC</code>  属于链路层，用于标识下一跳的网络设备的物理地址，数据从源主机到目的主机的过程中， <code>MAC</code>  首部每经过一个路由器都会变换，而 IP 地址不会变换。</p>\n<p><code>IPv4</code>  地址由 32 位组成，以前会根据前几位将其分为 <code>ABCDE</code>  类地址，但是分类地址的局限性太多，比如 C 类 IP 数量太少，而 A 类 IP 数量有太多，所以采用了无分类 IP 地址，通过子网掩码和 IP 地址做 ** <code>&amp;</code>  运算<strong>来确定</strong>网络号、子网号 **。在路由控制中，目的地址与路由表中的子网掩码运算并比较网络号，从而进行路由转发。</p>\n<p>IP 协议因为不能重组分片数据，所以分片会导致严重的性能损耗，一个分片丢失了，就要重发整个 IP 数据报，所以通过引入 <code>MSS</code>  将分片操作交给 <code>TCP</code>  处理。</p>\n<p>IPv6 相对于 IPv4 的改进：</p>\n<ul>\n<li>取消了首部校验和字段：因为数据链路层和传输层都会校验</li>\n<li>取消分片 / 重组相关字段，这种操作只允许源 / 目标主机。</li>\n<li>使用了 128 位，16 进制，极大扩充了 IP 数量</li>\n</ul>\n",
            "tags": [
                "八股文"
            ]
        }
    ]
}